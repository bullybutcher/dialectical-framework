{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b618fd-e7cf-4402-9f50-775ade1d23e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.13.3, pytest-8.3.5, pluggy-1.6.0\n",
      "rootdir: /Users/justintan/dialectical-framework\n",
      "configfile: pyproject.toml\n",
      "plugins: anyio-4.9.0\n",
      "collected 6 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_analyst.py \u001b[32m.\u001b[0m\u001b[33ms\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                   [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_____________________________ test_thought_mapping _____________________________\u001b[0m\n",
      "\n",
      "self = <litellm.llms.azure.azure.AzureChatCompletion object at 0x10db8dd30>\n",
      "model = 'gpt-4-turbo'\n",
      "messages = [{'content': '<context>Putin started the war, Ukraine will not surrender and will finally win!</context>', 'role': 'us...on\\'t mention any special denotations such as \"T\", \"T1\", \"T+\", \"A-\", \"Ac\", \"Re\", etc.\\n</formatting>', 'role': 'user'}]\n",
      "model_response = ModelResponse(id='chatcmpl-0f0a4988-360d-4cde-864b-845245f55667', created=1748418118, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "api_key = '830bfa7d0fd34a5dae45e1a1d8f879de'\n",
      "api_base = 'https://openai-01-dialexityswedencentral.openai.azure.com/'\n",
      "api_version = '2024-10-21DIALEXITY_DEFAULT_MODEL=gpt-4', api_type = 'azure'\n",
      "azure_ad_token = None, azure_ad_token_provider = None, dynamic_params = False\n",
      "print_verbose = <function print_verbose at 0x10dbc3e20>, timeout = 600.0\n",
      "logging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ea301a0>\n",
      "optional_params = {'extra_body': {}, 'stream': False, 'tool_choice': 'required', 'tools': [{'function': {'description': 'Correctly forma..., 'properties': {'dialectical_components': {...}}, 'required': ['dialectical_components'], ...}}, 'type': 'function'}]}\n",
      "litellm_params = {'acompletion': False, 'aembedding': None, 'api_base': None, 'api_key': None, ...}\n",
      "logger_fn = None, acompletion = False, headers = None, client = None\n",
      "\n",
      "    def completion(  # noqa: PLR0915\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: list,\n",
      "        model_response: ModelResponse,\n",
      "        api_key: str,\n",
      "        api_base: str,\n",
      "        api_version: str,\n",
      "        api_type: str,\n",
      "        azure_ad_token: str,\n",
      "        azure_ad_token_provider: Callable,\n",
      "        dynamic_params: bool,\n",
      "        print_verbose: Callable,\n",
      "        timeout: Union[float, httpx.Timeout],\n",
      "        logging_obj: LiteLLMLoggingObj,\n",
      "        optional_params,\n",
      "        litellm_params,\n",
      "        logger_fn,\n",
      "        acompletion: bool = False,\n",
      "        headers: Optional[dict] = None,\n",
      "        client=None,\n",
      "    ):\n",
      "        if headers:\n",
      "            optional_params[\"extra_headers\"] = headers\n",
      "        try:\n",
      "            if model is None or messages is None:\n",
      "                raise AzureOpenAIError(\n",
      "                    status_code=422, message=\"Missing model or messages\"\n",
      "                )\n",
      "    \n",
      "            max_retries = optional_params.pop(\"max_retries\", None)\n",
      "            if max_retries is None:\n",
      "                max_retries = DEFAULT_MAX_RETRIES\n",
      "            json_mode: Optional[bool] = optional_params.pop(\"json_mode\", False)\n",
      "    \n",
      "            ### CHECK IF CLOUDFLARE AI GATEWAY ###\n",
      "            ### if so - set the model as part of the base url\n",
      "            if \"gateway.ai.cloudflare.com\" in api_base:\n",
      "                client = self._init_azure_client_for_cloudflare_ai_gateway(\n",
      "                    api_base=api_base,\n",
      "                    model=model,\n",
      "                    api_version=api_version,\n",
      "                    max_retries=max_retries,\n",
      "                    timeout=timeout,\n",
      "                    api_key=api_key,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    acompletion=acompletion,\n",
      "                    client=client,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "    \n",
      "                data = {\"model\": None, \"messages\": messages, **optional_params}\n",
      "            else:\n",
      "                data = litellm.AzureOpenAIConfig().transform_request(\n",
      "                    model=model,\n",
      "                    messages=messages,\n",
      "                    optional_params=optional_params,\n",
      "                    litellm_params=litellm_params,\n",
      "                    headers=headers or {},\n",
      "                )\n",
      "    \n",
      "            if acompletion is True:\n",
      "                if optional_params.get(\"stream\", False):\n",
      "                    return self.async_streaming(\n",
      "                        logging_obj=logging_obj,\n",
      "                        api_base=api_base,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        data=data,\n",
      "                        model=model,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        max_retries=max_retries,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "                else:\n",
      "                    return self.acompletion(\n",
      "                        api_base=api_base,\n",
      "                        data=data,\n",
      "                        model_response=model_response,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        model=model,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        logging_obj=logging_obj,\n",
      "                        max_retries=max_retries,\n",
      "                        convert_tool_call_to_json_mode=json_mode,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "            elif \"stream\" in optional_params and optional_params[\"stream\"] is True:\n",
      "                return self.streaming(\n",
      "                    logging_obj=logging_obj,\n",
      "                    api_base=api_base,\n",
      "                    dynamic_params=dynamic_params,\n",
      "                    data=data,\n",
      "                    model=model,\n",
      "                    api_key=api_key,\n",
      "                    api_version=api_version,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    timeout=timeout,\n",
      "                    client=client,\n",
      "                    max_retries=max_retries,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "            else:\n",
      "                ## LOGGING\n",
      "                logging_obj.pre_call(\n",
      "                    input=messages,\n",
      "                    api_key=api_key,\n",
      "                    additional_args={\n",
      "                        \"headers\": {\n",
      "                            \"api_key\": api_key,\n",
      "                            \"azure_ad_token\": azure_ad_token,\n",
      "                        },\n",
      "                        \"api_version\": api_version,\n",
      "                        \"api_base\": api_base,\n",
      "                        \"complete_input_dict\": data,\n",
      "                    },\n",
      "                )\n",
      "                if not isinstance(max_retries, int):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=422, message=\"max retries must be an int\"\n",
      "                    )\n",
      "                # init AzureOpenAI Client\n",
      "                azure_client = self.get_azure_openai_client(\n",
      "                    api_version=api_version,\n",
      "                    api_base=api_base,\n",
      "                    api_key=api_key,\n",
      "                    model=model,\n",
      "                    client=client,\n",
      "                    _is_async=False,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "                if not isinstance(azure_client, AzureOpenAI):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=500,\n",
      "                        message=\"azure_client is not an instance of AzureOpenAI\",\n",
      "                    )\n",
      "    \n",
      ">               headers, response = self.make_sync_azure_openai_chat_completion_request(\n",
      "                    azure_client=azure_client, data=data, timeout=timeout\n",
      "                )\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:328: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:148: in make_sync_azure_openai_chat_completion_request\n",
      "    raise e\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:140: in make_sync_azure_openai_chat_completion_request\n",
      "    raw_response = azure_client.chat.completions.with_raw_response.create(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_legacy_response.py\u001b[0m:364: in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_utils/_utils.py\u001b[0m:287: in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\u001b[0m:925: in create\n",
      "    return self._post(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_base_client.py\u001b[0m:1239: in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <openai.lib.azure.AzureOpenAI object at 0x10ea30590>\n",
      "cast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\n",
      "options = FinalRequestOptions(method='post', url='/deployments/gpt-4-turbo/chat/completions', params={}, headers={'X-Stainless-R...ctical Components'}}, 'required': ['dialectical_components'], 'type': 'object'}}, 'type': 'function'}]}, extra_json={})\n",
      "\n",
      "    def request(\n",
      "        self,\n",
      "        cast_to: Type[ResponseT],\n",
      "        options: FinalRequestOptions,\n",
      "        *,\n",
      "        stream: bool = False,\n",
      "        stream_cls: type[_StreamT] | None = None,\n",
      "    ) -> ResponseT | _StreamT:\n",
      "        cast_to = self._maybe_override_cast_to(cast_to, options)\n",
      "    \n",
      "        # create a copy of the options we were given so that if the\n",
      "        # options are mutated later & we then retry, the retries are\n",
      "        # given the original options\n",
      "        input_options = model_copy(options)\n",
      "        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n",
      "            # ensure the idempotency key is reused between requests\n",
      "            input_options.idempotency_key = self._idempotency_key()\n",
      "    \n",
      "        response: httpx.Response | None = None\n",
      "        max_retries = input_options.get_max_retries(self.max_retries)\n",
      "    \n",
      "        retries_taken = 0\n",
      "        for retries_taken in range(max_retries + 1):\n",
      "            options = model_copy(input_options)\n",
      "            options = self._prepare_options(options)\n",
      "    \n",
      "            remaining_retries = max_retries - retries_taken\n",
      "            request = self._build_request(options, retries_taken=retries_taken)\n",
      "            self._prepare_request(request)\n",
      "    \n",
      "            kwargs: HttpxSendArgs = {}\n",
      "            if self.custom_auth is not None:\n",
      "                kwargs[\"auth\"] = self.custom_auth\n",
      "    \n",
      "            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n",
      "    \n",
      "            response = None\n",
      "            try:\n",
      "                response = self._client.send(\n",
      "                    request,\n",
      "                    stream=stream or self._should_stream_response_body(request=request),\n",
      "                    **kwargs,\n",
      "                )\n",
      "            except httpx.TimeoutException as err:\n",
      "                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n",
      "    \n",
      "                if remaining_retries > 0:\n",
      "                    self._sleep_for_retry(\n",
      "                        retries_taken=retries_taken,\n",
      "                        max_retries=max_retries,\n",
      "                        options=input_options,\n",
      "                        response=None,\n",
      "                    )\n",
      "                    continue\n",
      "    \n",
      "                log.debug(\"Raising timeout error\")\n",
      "                raise APITimeoutError(request=request) from err\n",
      "            except Exception as err:\n",
      "                log.debug(\"Encountered Exception\", exc_info=True)\n",
      "    \n",
      "                if remaining_retries > 0:\n",
      "                    self._sleep_for_retry(\n",
      "                        retries_taken=retries_taken,\n",
      "                        max_retries=max_retries,\n",
      "                        options=input_options,\n",
      "                        response=None,\n",
      "                    )\n",
      "                    continue\n",
      "    \n",
      "                log.debug(\"Raising connection error\")\n",
      "                raise APIConnectionError(request=request) from err\n",
      "    \n",
      "            log.debug(\n",
      "                'HTTP Response: %s %s \"%i %s\" %s',\n",
      "                request.method,\n",
      "                request.url,\n",
      "                response.status_code,\n",
      "                response.reason_phrase,\n",
      "                response.headers,\n",
      "            )\n",
      "            log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n",
      "    \n",
      "            try:\n",
      "                response.raise_for_status()\n",
      "            except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n",
      "                log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n",
      "    \n",
      "                if remaining_retries > 0 and self._should_retry(err.response):\n",
      "                    err.response.close()\n",
      "                    self._sleep_for_retry(\n",
      "                        retries_taken=retries_taken,\n",
      "                        max_retries=max_retries,\n",
      "                        options=input_options,\n",
      "                        response=response,\n",
      "                    )\n",
      "                    continue\n",
      "    \n",
      "                # If the response is streamed then we need to explicitly read the response\n",
      "                # to completion before attempting to access the response text.\n",
      "                if not err.response.is_closed:\n",
      "                    err.response.read()\n",
      "    \n",
      "                log.debug(\"Re-raising status error\")\n",
      ">               raise self._make_status_error_from_response(err.response) from None\n",
      "\u001b[1m\u001b[31mE               openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_base_client.py\u001b[0m:1034: NotFoundError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "model = 'gpt-4-turbo'\n",
      "messages = [{'content': '<context>Putin started the war, Ukraine will not surrender and will finally win!</context>', 'role': 'us...on\\'t mention any special denotations such as \"T\", \"T1\", \"T+\", \"A-\", \"Ac\", \"Re\", etc.\\n</formatting>', 'role': 'user'}]\n",
      "timeout = 600.0, temperature = None, top_p = None, n = None, stream = False\n",
      "stream_options = None, stop = None, max_completion_tokens = None\n",
      "max_tokens = None, modalities = None, prediction = None, audio = None\n",
      "presence_penalty = None, frequency_penalty = None, logit_bias = None\n",
      "user = None, reasoning_effort = None, response_format = None, seed = None\n",
      "tools = [{'function': {'description': 'Correctly formatted and typed parameters extracted from the completion. Must include re...itle': 'Dialectical Components', 'type': 'array'}}, 'required': ['dialectical_components'], ...}}, 'type': 'function'}]\n",
      "tool_choice = 'required', logprobs = None, top_logprobs = None\n",
      "parallel_tool_calls = None, web_search_options = None, deployment_id = None\n",
      "extra_headers = None, functions = None, function_call = None, base_url = None\n",
      "api_version = '2024-10-21DIALEXITY_DEFAULT_MODEL=gpt-4'\n",
      "api_key = '830bfa7d0fd34a5dae45e1a1d8f879de', model_list = None, thinking = None\n",
      "kwargs = {'litellm_call_id': 'f71d263a-34bc-4266-aa7f-1684719d365f', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ea301a0>}\n",
      "args = {'api_key': None, 'api_version': None, 'audio': None, 'base_url': None, ...}\n",
      "api_base = 'https://openai-01-dialexityswedencentral.openai.azure.com/'\n",
      "mock_response = None, mock_tool_calls = None, mock_timeout = None\n",
      "force_timeout = 600, logger_fn = None\n",
      "\n",
      "    @client\n",
      "    def completion(  # type: ignore # noqa: PLR0915\n",
      "        model: str,\n",
      "        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n",
      "        messages: List = [],\n",
      "        timeout: Optional[Union[float, str, httpx.Timeout]] = None,\n",
      "        temperature: Optional[float] = None,\n",
      "        top_p: Optional[float] = None,\n",
      "        n: Optional[int] = None,\n",
      "        stream: Optional[bool] = None,\n",
      "        stream_options: Optional[dict] = None,\n",
      "        stop=None,\n",
      "        max_completion_tokens: Optional[int] = None,\n",
      "        max_tokens: Optional[int] = None,\n",
      "        modalities: Optional[List[ChatCompletionModality]] = None,\n",
      "        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n",
      "        audio: Optional[ChatCompletionAudioParam] = None,\n",
      "        presence_penalty: Optional[float] = None,\n",
      "        frequency_penalty: Optional[float] = None,\n",
      "        logit_bias: Optional[dict] = None,\n",
      "        user: Optional[str] = None,\n",
      "        # openai v1.0+ new params\n",
      "        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n",
      "        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n",
      "        seed: Optional[int] = None,\n",
      "        tools: Optional[List] = None,\n",
      "        tool_choice: Optional[Union[str, dict]] = None,\n",
      "        logprobs: Optional[bool] = None,\n",
      "        top_logprobs: Optional[int] = None,\n",
      "        parallel_tool_calls: Optional[bool] = None,\n",
      "        web_search_options: Optional[OpenAIWebSearchOptions] = None,\n",
      "        deployment_id=None,\n",
      "        extra_headers: Optional[dict] = None,\n",
      "        # soon to be deprecated params by OpenAI\n",
      "        functions: Optional[List] = None,\n",
      "        function_call: Optional[str] = None,\n",
      "        # set api_base, api_version, api_key\n",
      "        base_url: Optional[str] = None,\n",
      "        api_version: Optional[str] = None,\n",
      "        api_key: Optional[str] = None,\n",
      "        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n",
      "        # Optional liteLLM function params\n",
      "        thinking: Optional[AnthropicThinkingParam] = None,\n",
      "        **kwargs,\n",
      "    ) -> Union[ModelResponse, CustomStreamWrapper]:\n",
      "        \"\"\"\n",
      "        Perform a completion() using any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n",
      "        Parameters:\n",
      "            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n",
      "            messages (List): A list of message objects representing the conversation context (default is an empty list).\n",
      "    \n",
      "            OPTIONAL PARAMS\n",
      "            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n",
      "            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n",
      "            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n",
      "            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n",
      "            n (int, optional): The number of completions to generate (default is 1).\n",
      "            stream (bool, optional): If True, return a streaming response (default is False).\n",
      "            stream_options (dict, optional): A dictionary containing options for the streaming response. Only set this when you set stream: true.\n",
      "            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n",
      "            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n",
      "            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n",
      "            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request.. You can use `[\"text\", \"audio\"]`\n",
      "            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n",
      "            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n",
      "            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n",
      "            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n",
      "            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n",
      "            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n",
      "            logprobs (bool, optional): Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message\n",
      "            top_logprobs (int, optional): An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.\n",
      "            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n",
      "            api_base (str, optional): Base URL for the API (default is None).\n",
      "            api_version (str, optional): API version (default is None).\n",
      "            api_key (str, optional): API key (default is None).\n",
      "            model_list (list, optional): List of api base, version, keys\n",
      "            extra_headers (dict, optional): Additional headers to include in the request.\n",
      "    \n",
      "            LITELLM Specific Params\n",
      "            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n",
      "            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n",
      "            max_retries (int, optional): The number of retries to attempt (default is 0).\n",
      "        Returns:\n",
      "            ModelResponse: A response object containing the generated completion and associated metadata.\n",
      "    \n",
      "        Note:\n",
      "            - This function is used to perform completions() using the specified language model.\n",
      "            - It supports various optional parameters for customizing the completion behavior.\n",
      "            - If 'mock_response' is provided, a mock completion response is returned for testing or debugging.\n",
      "        \"\"\"\n",
      "        ### VALIDATE Request ###\n",
      "        if model is None:\n",
      "            raise ValueError(\"model param not passed in.\")\n",
      "        # validate messages\n",
      "        messages = validate_and_fix_openai_messages(messages=messages)\n",
      "        # validate tool_choice\n",
      "        tool_choice = validate_chat_completion_tool_choice(tool_choice=tool_choice)\n",
      "        ######### unpacking kwargs #####################\n",
      "        args = locals()\n",
      "        api_base = kwargs.get(\"api_base\", None)\n",
      "        mock_response = kwargs.get(\"mock_response\", None)\n",
      "        mock_tool_calls = kwargs.get(\"mock_tool_calls\", None)\n",
      "        mock_timeout = cast(Optional[bool], kwargs.get(\"mock_timeout\", None))\n",
      "        force_timeout = kwargs.get(\"force_timeout\", 600)  ## deprecated\n",
      "        logger_fn = kwargs.get(\"logger_fn\", None)\n",
      "        verbose = kwargs.get(\"verbose\", False)\n",
      "        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n",
      "        litellm_logging_obj = kwargs.get(\"litellm_logging_obj\", None)\n",
      "        id = kwargs.get(\"id\", None)\n",
      "        metadata = kwargs.get(\"metadata\", None)\n",
      "        model_info = kwargs.get(\"model_info\", None)\n",
      "        proxy_server_request = kwargs.get(\"proxy_server_request\", None)\n",
      "        fallbacks = kwargs.get(\"fallbacks\", None)\n",
      "        provider_specific_header = cast(\n",
      "            Optional[ProviderSpecificHeader], kwargs.get(\"provider_specific_header\", None)\n",
      "        )\n",
      "        headers = kwargs.get(\"headers\", None) or extra_headers\n",
      "    \n",
      "        ensure_alternating_roles: Optional[bool] = kwargs.get(\n",
      "            \"ensure_alternating_roles\", None\n",
      "        )\n",
      "        user_continue_message: Optional[ChatCompletionUserMessage] = kwargs.get(\n",
      "            \"user_continue_message\", None\n",
      "        )\n",
      "        assistant_continue_message: Optional[ChatCompletionAssistantMessage] = kwargs.get(\n",
      "            \"assistant_continue_message\", None\n",
      "        )\n",
      "        if headers is None:\n",
      "            headers = {}\n",
      "        if extra_headers is not None:\n",
      "            headers.update(extra_headers)\n",
      "        num_retries = kwargs.get(\n",
      "            \"num_retries\", None\n",
      "        )  ## alt. param for 'max_retries'. Use this to pass retries w/ instructor.\n",
      "        max_retries = kwargs.get(\"max_retries\", None)\n",
      "        cooldown_time = kwargs.get(\"cooldown_time\", None)\n",
      "        context_window_fallback_dict = kwargs.get(\"context_window_fallback_dict\", None)\n",
      "        organization = kwargs.get(\"organization\", None)\n",
      "        ### VERIFY SSL ###\n",
      "        ssl_verify = kwargs.get(\"ssl_verify\", None)\n",
      "        ### CUSTOM MODEL COST ###\n",
      "        input_cost_per_token = kwargs.get(\"input_cost_per_token\", None)\n",
      "        output_cost_per_token = kwargs.get(\"output_cost_per_token\", None)\n",
      "        input_cost_per_second = kwargs.get(\"input_cost_per_second\", None)\n",
      "        output_cost_per_second = kwargs.get(\"output_cost_per_second\", None)\n",
      "        ### CUSTOM PROMPT TEMPLATE ###\n",
      "        initial_prompt_value = kwargs.get(\"initial_prompt_value\", None)\n",
      "        roles = kwargs.get(\"roles\", None)\n",
      "        final_prompt_value = kwargs.get(\"final_prompt_value\", None)\n",
      "        bos_token = kwargs.get(\"bos_token\", None)\n",
      "        eos_token = kwargs.get(\"eos_token\", None)\n",
      "        preset_cache_key = kwargs.get(\"preset_cache_key\", None)\n",
      "        hf_model_name = kwargs.get(\"hf_model_name\", None)\n",
      "        supports_system_message = kwargs.get(\"supports_system_message\", None)\n",
      "        base_model = kwargs.get(\"base_model\", None)\n",
      "        ### DISABLE FLAGS ###\n",
      "        disable_add_transform_inline_image_block = kwargs.get(\n",
      "            \"disable_add_transform_inline_image_block\", None\n",
      "        )\n",
      "        ### TEXT COMPLETION CALLS ###\n",
      "        text_completion = kwargs.get(\"text_completion\", False)\n",
      "        atext_completion = kwargs.get(\"atext_completion\", False)\n",
      "        ### ASYNC CALLS ###\n",
      "        acompletion = kwargs.get(\"acompletion\", False)\n",
      "        client = kwargs.get(\"client\", None)\n",
      "        ### Admin Controls ###\n",
      "        no_log = kwargs.get(\"no-log\", False)\n",
      "        ### PROMPT MANAGEMENT ###\n",
      "        prompt_id = cast(Optional[str], kwargs.get(\"prompt_id\", None))\n",
      "        prompt_variables = cast(Optional[dict], kwargs.get(\"prompt_variables\", None))\n",
      "        ### COPY MESSAGES ### - related issue https://github.com/BerriAI/litellm/discussions/4489\n",
      "        messages = get_completion_messages(\n",
      "            messages=messages,\n",
      "            ensure_alternating_roles=ensure_alternating_roles or False,\n",
      "            user_continue_message=user_continue_message,\n",
      "            assistant_continue_message=assistant_continue_message,\n",
      "        )\n",
      "        ######## end of unpacking kwargs ###########\n",
      "        standard_openai_params = get_standard_openai_params(params=args)\n",
      "        non_default_params = get_non_default_completion_params(kwargs=kwargs)\n",
      "        litellm_params = {}  # used to prevent unbound var errors\n",
      "        ## PROMPT MANAGEMENT HOOKS ##\n",
      "        if isinstance(litellm_logging_obj, LiteLLMLoggingObj) and (\n",
      "            litellm_logging_obj.should_run_prompt_management_hooks(\n",
      "                prompt_id=prompt_id, non_default_params=non_default_params\n",
      "            )\n",
      "        ):\n",
      "            (\n",
      "                model,\n",
      "                messages,\n",
      "                optional_params,\n",
      "            ) = litellm_logging_obj.get_chat_completion_prompt(\n",
      "                model=model,\n",
      "                messages=messages,\n",
      "                non_default_params=non_default_params,\n",
      "                prompt_id=prompt_id,\n",
      "                prompt_variables=prompt_variables,\n",
      "                prompt_label=kwargs.get(\"prompt_label\", None),\n",
      "            )\n",
      "    \n",
      "        try:\n",
      "            if base_url is not None:\n",
      "                api_base = base_url\n",
      "            if num_retries is not None:\n",
      "                max_retries = num_retries\n",
      "            logging = litellm_logging_obj\n",
      "            fallbacks = fallbacks or litellm.model_fallbacks\n",
      "            if fallbacks is not None:\n",
      "                return completion_with_fallbacks(**args)\n",
      "            if model_list is not None:\n",
      "                deployments = [\n",
      "                    m[\"litellm_params\"] for m in model_list if m[\"model_name\"] == model\n",
      "                ]\n",
      "                return litellm.batch_completion_models(deployments=deployments, **args)\n",
      "            if litellm.model_alias_map and model in litellm.model_alias_map:\n",
      "                model = litellm.model_alias_map[\n",
      "                    model\n",
      "                ]  # update the model to the actual value if an alias has been passed in\n",
      "            model_response = ModelResponse()\n",
      "            setattr(model_response, \"usage\", litellm.Usage())\n",
      "            if (\n",
      "                kwargs.get(\"azure\", False) is True\n",
      "            ):  # don't remove flag check, to remain backwards compatible for repos like Codium\n",
      "                custom_llm_provider = \"azure\"\n",
      "            if deployment_id is not None:  # azure llms\n",
      "                model = deployment_id\n",
      "                custom_llm_provider = \"azure\"\n",
      "            model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n",
      "                model=model,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "                api_base=api_base,\n",
      "                api_key=api_key,\n",
      "            )\n",
      "    \n",
      "            if (\n",
      "                provider_specific_header is not None\n",
      "                and provider_specific_header[\"custom_llm_provider\"] == custom_llm_provider\n",
      "            ):\n",
      "                headers.update(provider_specific_header[\"extra_headers\"])\n",
      "    \n",
      "            if model_response is not None and hasattr(model_response, \"_hidden_params\"):\n",
      "                model_response._hidden_params[\"custom_llm_provider\"] = custom_llm_provider\n",
      "                model_response._hidden_params[\"region_name\"] = kwargs.get(\n",
      "                    \"aws_region_name\", None\n",
      "                )  # support region-based pricing for bedrock\n",
      "    \n",
      "            ### TIMEOUT LOGIC ###\n",
      "            timeout = timeout or kwargs.get(\"request_timeout\", 600) or 600\n",
      "            # set timeout for 10 minutes by default\n",
      "            if isinstance(timeout, httpx.Timeout) and not supports_httpx_timeout(\n",
      "                custom_llm_provider\n",
      "            ):\n",
      "                timeout = timeout.read or 600  # default 10 min timeout\n",
      "            elif not isinstance(timeout, httpx.Timeout):\n",
      "                timeout = float(timeout)  # type: ignore\n",
      "    \n",
      "            ### REGISTER CUSTOM MODEL PRICING -- IF GIVEN ###\n",
      "            if input_cost_per_token is not None and output_cost_per_token is not None:\n",
      "                litellm.register_model(\n",
      "                    {\n",
      "                        f\"{custom_llm_provider}/{model}\": {\n",
      "                            \"input_cost_per_token\": input_cost_per_token,\n",
      "                            \"output_cost_per_token\": output_cost_per_token,\n",
      "                            \"litellm_provider\": custom_llm_provider,\n",
      "                        }\n",
      "                    }\n",
      "                )\n",
      "            elif (\n",
      "                input_cost_per_second is not None\n",
      "            ):  # time based pricing just needs cost in place\n",
      "                output_cost_per_second = output_cost_per_second\n",
      "                litellm.register_model(\n",
      "                    {\n",
      "                        f\"{custom_llm_provider}/{model}\": {\n",
      "                            \"input_cost_per_second\": input_cost_per_second,\n",
      "                            \"output_cost_per_second\": output_cost_per_second,\n",
      "                            \"litellm_provider\": custom_llm_provider,\n",
      "                        }\n",
      "                    }\n",
      "                )\n",
      "            ### BUILD CUSTOM PROMPT TEMPLATE -- IF GIVEN ###\n",
      "            custom_prompt_dict = {}  # type: ignore\n",
      "            if (\n",
      "                initial_prompt_value\n",
      "                or roles\n",
      "                or final_prompt_value\n",
      "                or bos_token\n",
      "                or eos_token\n",
      "            ):\n",
      "                custom_prompt_dict = {model: {}}\n",
      "                if initial_prompt_value:\n",
      "                    custom_prompt_dict[model][\"initial_prompt_value\"] = initial_prompt_value\n",
      "                if roles:\n",
      "                    custom_prompt_dict[model][\"roles\"] = roles\n",
      "                if final_prompt_value:\n",
      "                    custom_prompt_dict[model][\"final_prompt_value\"] = final_prompt_value\n",
      "                if bos_token:\n",
      "                    custom_prompt_dict[model][\"bos_token\"] = bos_token\n",
      "                if eos_token:\n",
      "                    custom_prompt_dict[model][\"eos_token\"] = eos_token\n",
      "    \n",
      "            if kwargs.get(\"model_file_id_mapping\"):\n",
      "                messages = update_messages_with_model_file_ids(\n",
      "                    messages=messages,\n",
      "                    model_id=kwargs.get(\"model_info\", {}).get(\"id\", None),\n",
      "                    model_file_id_mapping=cast(\n",
      "                        Dict[str, Dict[str, str]], kwargs.get(\"model_file_id_mapping\")\n",
      "                    ),\n",
      "                )\n",
      "    \n",
      "            provider_config: Optional[BaseConfig] = None\n",
      "            if custom_llm_provider is not None and custom_llm_provider in [\n",
      "                provider.value for provider in LlmProviders\n",
      "            ]:\n",
      "                provider_config = ProviderConfigManager.get_provider_chat_config(\n",
      "                    model=model, provider=LlmProviders(custom_llm_provider)\n",
      "                )\n",
      "    \n",
      "            if provider_config is not None:\n",
      "                messages = provider_config.translate_developer_role_to_system_role(\n",
      "                    messages=messages\n",
      "                )\n",
      "    \n",
      "            if (\n",
      "                supports_system_message is not None\n",
      "                and isinstance(supports_system_message, bool)\n",
      "                and supports_system_message is False\n",
      "            ):\n",
      "                messages = map_system_message_pt(messages=messages)\n",
      "    \n",
      "            if dynamic_api_key is not None:\n",
      "                api_key = dynamic_api_key\n",
      "            # check if user passed in any of the OpenAI optional params\n",
      "            optional_param_args = {\n",
      "                \"functions\": functions,\n",
      "                \"function_call\": function_call,\n",
      "                \"temperature\": temperature,\n",
      "                \"top_p\": top_p,\n",
      "                \"n\": n,\n",
      "                \"stream\": stream,\n",
      "                \"stream_options\": stream_options,\n",
      "                \"stop\": stop,\n",
      "                \"max_tokens\": max_tokens,\n",
      "                \"max_completion_tokens\": max_completion_tokens,\n",
      "                \"modalities\": modalities,\n",
      "                \"prediction\": prediction,\n",
      "                \"audio\": audio,\n",
      "                \"presence_penalty\": presence_penalty,\n",
      "                \"frequency_penalty\": frequency_penalty,\n",
      "                \"logit_bias\": logit_bias,\n",
      "                \"user\": user,\n",
      "                # params to identify the model\n",
      "                \"model\": model,\n",
      "                \"custom_llm_provider\": custom_llm_provider,\n",
      "                \"response_format\": response_format,\n",
      "                \"seed\": seed,\n",
      "                \"tools\": tools,\n",
      "                \"tool_choice\": tool_choice,\n",
      "                \"max_retries\": max_retries,\n",
      "                \"logprobs\": logprobs,\n",
      "                \"top_logprobs\": top_logprobs,\n",
      "                \"api_version\": api_version,\n",
      "                \"parallel_tool_calls\": parallel_tool_calls,\n",
      "                \"messages\": messages,\n",
      "                \"reasoning_effort\": reasoning_effort,\n",
      "                \"thinking\": thinking,\n",
      "                \"web_search_options\": web_search_options,\n",
      "                \"allowed_openai_params\": kwargs.get(\"allowed_openai_params\"),\n",
      "            }\n",
      "            optional_params = get_optional_params(\n",
      "                **optional_param_args, **non_default_params\n",
      "            )\n",
      "            processed_non_default_params = pre_process_non_default_params(\n",
      "                model=model,\n",
      "                passed_params=optional_param_args,\n",
      "                special_params=non_default_params,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "                additional_drop_params=kwargs.get(\"additional_drop_params\"),\n",
      "            )\n",
      "            processed_non_default_params = add_provider_specific_params_to_optional_params(\n",
      "                optional_params=processed_non_default_params,\n",
      "                passed_params=non_default_params,\n",
      "            )\n",
      "    \n",
      "            if litellm.add_function_to_prompt and optional_params.get(\n",
      "                \"functions_unsupported_model\", None\n",
      "            ):  # if user opts to add it to prompt, when API doesn't support function calling\n",
      "                functions_unsupported_model = optional_params.pop(\n",
      "                    \"functions_unsupported_model\"\n",
      "                )\n",
      "                messages = function_call_prompt(\n",
      "                    messages=messages, functions=functions_unsupported_model\n",
      "                )\n",
      "    \n",
      "            # For logging - save the values of the litellm-specific params passed in\n",
      "            litellm_params = get_litellm_params(\n",
      "                acompletion=acompletion,\n",
      "                api_key=api_key,\n",
      "                force_timeout=force_timeout,\n",
      "                logger_fn=logger_fn,\n",
      "                verbose=verbose,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "                api_base=api_base,\n",
      "                litellm_call_id=kwargs.get(\"litellm_call_id\", None),\n",
      "                model_alias_map=litellm.model_alias_map,\n",
      "                completion_call_id=id,\n",
      "                metadata=metadata,\n",
      "                model_info=model_info,\n",
      "                proxy_server_request=proxy_server_request,\n",
      "                preset_cache_key=preset_cache_key,\n",
      "                no_log=no_log,\n",
      "                input_cost_per_second=input_cost_per_second,\n",
      "                input_cost_per_token=input_cost_per_token,\n",
      "                output_cost_per_second=output_cost_per_second,\n",
      "                output_cost_per_token=output_cost_per_token,\n",
      "                cooldown_time=cooldown_time,\n",
      "                text_completion=kwargs.get(\"text_completion\"),\n",
      "                azure_ad_token_provider=kwargs.get(\"azure_ad_token_provider\"),\n",
      "                user_continue_message=kwargs.get(\"user_continue_message\"),\n",
      "                base_model=base_model,\n",
      "                litellm_trace_id=kwargs.get(\"litellm_trace_id\"),\n",
      "                litellm_session_id=kwargs.get(\"litellm_session_id\"),\n",
      "                hf_model_name=hf_model_name,\n",
      "                custom_prompt_dict=custom_prompt_dict,\n",
      "                litellm_metadata=kwargs.get(\"litellm_metadata\"),\n",
      "                disable_add_transform_inline_image_block=disable_add_transform_inline_image_block,\n",
      "                drop_params=kwargs.get(\"drop_params\"),\n",
      "                prompt_id=prompt_id,\n",
      "                prompt_variables=prompt_variables,\n",
      "                ssl_verify=ssl_verify,\n",
      "                merge_reasoning_content_in_choices=kwargs.get(\n",
      "                    \"merge_reasoning_content_in_choices\", None\n",
      "                ),\n",
      "                use_litellm_proxy=kwargs.get(\"use_litellm_proxy\", False),\n",
      "                api_version=api_version,\n",
      "                azure_ad_token=kwargs.get(\"azure_ad_token\"),\n",
      "                tenant_id=kwargs.get(\"tenant_id\"),\n",
      "                client_id=kwargs.get(\"client_id\"),\n",
      "                client_secret=kwargs.get(\"client_secret\"),\n",
      "                azure_username=kwargs.get(\"azure_username\"),\n",
      "                azure_password=kwargs.get(\"azure_password\"),\n",
      "                max_retries=max_retries,\n",
      "                timeout=timeout,\n",
      "            )\n",
      "            cast(LiteLLMLoggingObj, logging).update_environment_variables(\n",
      "                model=model,\n",
      "                user=user,\n",
      "                optional_params=processed_non_default_params,  # [IMPORTANT] - using processed_non_default_params ensures consistent params logged to langfuse for finetuning / eval datasets.\n",
      "                litellm_params=litellm_params,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "            )\n",
      "            if mock_response or mock_tool_calls or mock_timeout:\n",
      "                kwargs.pop(\"mock_timeout\", None)  # remove for any fallbacks triggered\n",
      "                return mock_completion(\n",
      "                    model,\n",
      "                    messages,\n",
      "                    stream=stream,\n",
      "                    n=n,\n",
      "                    mock_response=mock_response,\n",
      "                    mock_tool_calls=mock_tool_calls,\n",
      "                    logging=logging,\n",
      "                    acompletion=acompletion,\n",
      "                    mock_delay=kwargs.get(\"mock_delay\", None),\n",
      "                    custom_llm_provider=custom_llm_provider,\n",
      "                    mock_timeout=mock_timeout,\n",
      "                    timeout=timeout,\n",
      "                )\n",
      "    \n",
      "            if custom_llm_provider == \"azure\":\n",
      "                # azure configs\n",
      "                ## check dynamic params ##\n",
      "                dynamic_params = False\n",
      "                if client is not None and (\n",
      "                    isinstance(client, openai.AzureOpenAI)\n",
      "                    or isinstance(client, openai.AsyncAzureOpenAI)\n",
      "                ):\n",
      "                    dynamic_params = _check_dynamic_azure_params(\n",
      "                        azure_client_params={\"api_version\": api_version},\n",
      "                        azure_client=client,\n",
      "                    )\n",
      "    \n",
      "                api_type = get_secret(\"AZURE_API_TYPE\") or \"azure\"\n",
      "    \n",
      "                api_base = api_base or litellm.api_base or get_secret(\"AZURE_API_BASE\")\n",
      "    \n",
      "                api_version = (\n",
      "                    api_version\n",
      "                    or litellm.api_version\n",
      "                    or get_secret(\"AZURE_API_VERSION\")\n",
      "                    or litellm.AZURE_DEFAULT_API_VERSION\n",
      "                )\n",
      "    \n",
      "                api_key = (\n",
      "                    api_key\n",
      "                    or litellm.api_key\n",
      "                    or litellm.azure_key\n",
      "                    or get_secret(\"AZURE_OPENAI_API_KEY\")\n",
      "                    or get_secret(\"AZURE_API_KEY\")\n",
      "                )\n",
      "    \n",
      "                azure_ad_token = optional_params.get(\"extra_body\", {}).pop(\n",
      "                    \"azure_ad_token\", None\n",
      "                ) or get_secret(\"AZURE_AD_TOKEN\")\n",
      "    \n",
      "                azure_ad_token_provider = litellm_params.get(\n",
      "                    \"azure_ad_token_provider\", None\n",
      "                )\n",
      "    \n",
      "                headers = headers or litellm.headers\n",
      "    \n",
      "                if extra_headers is not None:\n",
      "                    optional_params[\"extra_headers\"] = extra_headers\n",
      "                if max_retries is not None:\n",
      "                    optional_params[\"max_retries\"] = max_retries\n",
      "    \n",
      "                if litellm.AzureOpenAIO1Config().is_o_series_model(model=model):\n",
      "                    ## LOAD CONFIG - if set\n",
      "                    config = litellm.AzureOpenAIO1Config.get_config()\n",
      "                    for k, v in config.items():\n",
      "                        if (\n",
      "                            k not in optional_params\n",
      "                        ):  # completion(top_k=3) > azure_config(top_k=3) <- allows for dynamic variables to be passed in\n",
      "                            optional_params[k] = v\n",
      "    \n",
      "                    response = azure_o1_chat_completions.completion(\n",
      "                        model=model,\n",
      "                        messages=messages,\n",
      "                        headers=headers,\n",
      "                        api_key=api_key,\n",
      "                        api_base=api_base,\n",
      "                        api_version=api_version,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        model_response=model_response,\n",
      "                        print_verbose=print_verbose,\n",
      "                        optional_params=optional_params,\n",
      "                        litellm_params=litellm_params,\n",
      "                        logger_fn=logger_fn,\n",
      "                        logging_obj=logging,\n",
      "                        acompletion=acompletion,\n",
      "                        timeout=timeout,  # type: ignore\n",
      "                        client=client,  # pass AsyncAzureOpenAI, AzureOpenAI client\n",
      "                        custom_llm_provider=custom_llm_provider,\n",
      "                    )\n",
      "                else:\n",
      "                    ## LOAD CONFIG - if set\n",
      "                    config = litellm.AzureOpenAIConfig.get_config()\n",
      "                    for k, v in config.items():\n",
      "                        if (\n",
      "                            k not in optional_params\n",
      "                        ):  # completion(top_k=3) > azure_config(top_k=3) <- allows for dynamic variables to be passed in\n",
      "                            optional_params[k] = v\n",
      "    \n",
      "                    ## COMPLETION CALL\n",
      ">                   response = azure_chat_completions.completion(\n",
      "                        model=model,\n",
      "                        messages=messages,\n",
      "                        headers=headers,\n",
      "                        api_key=api_key,\n",
      "                        api_base=api_base,\n",
      "                        api_version=api_version,\n",
      "                        api_type=api_type,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        model_response=model_response,\n",
      "                        print_verbose=print_verbose,\n",
      "                        optional_params=optional_params,\n",
      "                        litellm_params=litellm_params,\n",
      "                        logger_fn=logger_fn,\n",
      "                        logging_obj=logging,\n",
      "                        acompletion=acompletion,\n",
      "                        timeout=timeout,  # type: ignore\n",
      "                        client=client,  # pass AsyncAzureOpenAI, AzureOpenAI client\n",
      "                    )\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/main.py\u001b[0m:1366: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <litellm.llms.azure.azure.AzureChatCompletion object at 0x10db8dd30>\n",
      "model = 'gpt-4-turbo'\n",
      "messages = [{'content': '<context>Putin started the war, Ukraine will not surrender and will finally win!</context>', 'role': 'us...on\\'t mention any special denotations such as \"T\", \"T1\", \"T+\", \"A-\", \"Ac\", \"Re\", etc.\\n</formatting>', 'role': 'user'}]\n",
      "model_response = ModelResponse(id='chatcmpl-0f0a4988-360d-4cde-864b-845245f55667', created=1748418118, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "api_key = '830bfa7d0fd34a5dae45e1a1d8f879de'\n",
      "api_base = 'https://openai-01-dialexityswedencentral.openai.azure.com/'\n",
      "api_version = '2024-10-21DIALEXITY_DEFAULT_MODEL=gpt-4', api_type = 'azure'\n",
      "azure_ad_token = None, azure_ad_token_provider = None, dynamic_params = False\n",
      "print_verbose = <function print_verbose at 0x10dbc3e20>, timeout = 600.0\n",
      "logging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ea301a0>\n",
      "optional_params = {'extra_body': {}, 'stream': False, 'tool_choice': 'required', 'tools': [{'function': {'description': 'Correctly forma..., 'properties': {'dialectical_components': {...}}, 'required': ['dialectical_components'], ...}}, 'type': 'function'}]}\n",
      "litellm_params = {'acompletion': False, 'aembedding': None, 'api_base': None, 'api_key': None, ...}\n",
      "logger_fn = None, acompletion = False, headers = None, client = None\n",
      "\n",
      "    def completion(  # noqa: PLR0915\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: list,\n",
      "        model_response: ModelResponse,\n",
      "        api_key: str,\n",
      "        api_base: str,\n",
      "        api_version: str,\n",
      "        api_type: str,\n",
      "        azure_ad_token: str,\n",
      "        azure_ad_token_provider: Callable,\n",
      "        dynamic_params: bool,\n",
      "        print_verbose: Callable,\n",
      "        timeout: Union[float, httpx.Timeout],\n",
      "        logging_obj: LiteLLMLoggingObj,\n",
      "        optional_params,\n",
      "        litellm_params,\n",
      "        logger_fn,\n",
      "        acompletion: bool = False,\n",
      "        headers: Optional[dict] = None,\n",
      "        client=None,\n",
      "    ):\n",
      "        if headers:\n",
      "            optional_params[\"extra_headers\"] = headers\n",
      "        try:\n",
      "            if model is None or messages is None:\n",
      "                raise AzureOpenAIError(\n",
      "                    status_code=422, message=\"Missing model or messages\"\n",
      "                )\n",
      "    \n",
      "            max_retries = optional_params.pop(\"max_retries\", None)\n",
      "            if max_retries is None:\n",
      "                max_retries = DEFAULT_MAX_RETRIES\n",
      "            json_mode: Optional[bool] = optional_params.pop(\"json_mode\", False)\n",
      "    \n",
      "            ### CHECK IF CLOUDFLARE AI GATEWAY ###\n",
      "            ### if so - set the model as part of the base url\n",
      "            if \"gateway.ai.cloudflare.com\" in api_base:\n",
      "                client = self._init_azure_client_for_cloudflare_ai_gateway(\n",
      "                    api_base=api_base,\n",
      "                    model=model,\n",
      "                    api_version=api_version,\n",
      "                    max_retries=max_retries,\n",
      "                    timeout=timeout,\n",
      "                    api_key=api_key,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    acompletion=acompletion,\n",
      "                    client=client,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "    \n",
      "                data = {\"model\": None, \"messages\": messages, **optional_params}\n",
      "            else:\n",
      "                data = litellm.AzureOpenAIConfig().transform_request(\n",
      "                    model=model,\n",
      "                    messages=messages,\n",
      "                    optional_params=optional_params,\n",
      "                    litellm_params=litellm_params,\n",
      "                    headers=headers or {},\n",
      "                )\n",
      "    \n",
      "            if acompletion is True:\n",
      "                if optional_params.get(\"stream\", False):\n",
      "                    return self.async_streaming(\n",
      "                        logging_obj=logging_obj,\n",
      "                        api_base=api_base,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        data=data,\n",
      "                        model=model,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        max_retries=max_retries,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "                else:\n",
      "                    return self.acompletion(\n",
      "                        api_base=api_base,\n",
      "                        data=data,\n",
      "                        model_response=model_response,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        model=model,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        logging_obj=logging_obj,\n",
      "                        max_retries=max_retries,\n",
      "                        convert_tool_call_to_json_mode=json_mode,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "            elif \"stream\" in optional_params and optional_params[\"stream\"] is True:\n",
      "                return self.streaming(\n",
      "                    logging_obj=logging_obj,\n",
      "                    api_base=api_base,\n",
      "                    dynamic_params=dynamic_params,\n",
      "                    data=data,\n",
      "                    model=model,\n",
      "                    api_key=api_key,\n",
      "                    api_version=api_version,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    timeout=timeout,\n",
      "                    client=client,\n",
      "                    max_retries=max_retries,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "            else:\n",
      "                ## LOGGING\n",
      "                logging_obj.pre_call(\n",
      "                    input=messages,\n",
      "                    api_key=api_key,\n",
      "                    additional_args={\n",
      "                        \"headers\": {\n",
      "                            \"api_key\": api_key,\n",
      "                            \"azure_ad_token\": azure_ad_token,\n",
      "                        },\n",
      "                        \"api_version\": api_version,\n",
      "                        \"api_base\": api_base,\n",
      "                        \"complete_input_dict\": data,\n",
      "                    },\n",
      "                )\n",
      "                if not isinstance(max_retries, int):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=422, message=\"max retries must be an int\"\n",
      "                    )\n",
      "                # init AzureOpenAI Client\n",
      "                azure_client = self.get_azure_openai_client(\n",
      "                    api_version=api_version,\n",
      "                    api_base=api_base,\n",
      "                    api_key=api_key,\n",
      "                    model=model,\n",
      "                    client=client,\n",
      "                    _is_async=False,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "                if not isinstance(azure_client, AzureOpenAI):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=500,\n",
      "                        message=\"azure_client is not an instance of AzureOpenAI\",\n",
      "                    )\n",
      "    \n",
      "                headers, response = self.make_sync_azure_openai_chat_completion_request(\n",
      "                    azure_client=azure_client, data=data, timeout=timeout\n",
      "                )\n",
      "                stringified_response = response.model_dump()\n",
      "                ## LOGGING\n",
      "                logging_obj.post_call(\n",
      "                    input=messages,\n",
      "                    api_key=api_key,\n",
      "                    original_response=stringified_response,\n",
      "                    additional_args={\n",
      "                        \"headers\": headers,\n",
      "                        \"api_version\": api_version,\n",
      "                        \"api_base\": api_base,\n",
      "                    },\n",
      "                )\n",
      "                return convert_to_model_response_object(\n",
      "                    response_object=stringified_response,\n",
      "                    model_response_object=model_response,\n",
      "                    convert_tool_call_to_json_mode=json_mode,\n",
      "                    _response_headers=headers,\n",
      "                )\n",
      "        except AzureOpenAIError as e:\n",
      "            raise e\n",
      "        except Exception as e:\n",
      "            status_code = getattr(e, \"status_code\", 500)\n",
      "            error_headers = getattr(e, \"headers\", None)\n",
      "            error_response = getattr(e, \"response\", None)\n",
      "            error_body = getattr(e, \"body\", None)\n",
      "            if error_headers is None and error_response:\n",
      "                error_headers = getattr(error_response, \"headers\", None)\n",
      ">           raise AzureOpenAIError(\n",
      "                status_code=status_code,\n",
      "                message=str(e),\n",
      "                headers=error_headers,\n",
      "                body=error_body,\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           litellm.llms.azure.common_utils.AzureOpenAIError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:358: AzureOpenAIError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "    @observe()\n",
      "    def test_thought_mapping():\n",
      "        nr_of_thoughts = 3\n",
      "        reasoner = ThoughtMapping(user_message)\n",
      ">       cycles: List[Cycle] = asyncio.run(reasoner.extract(nr_of_thoughts))\n",
      "\n",
      "\u001b[1m\u001b[31mtest_analyst.py\u001b[0m:92: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py\u001b[0m:195: in run\n",
      "    return runner.run(main)\n",
      "\u001b[1m\u001b[31m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py\u001b[0m:118: in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "\u001b[1m\u001b[31m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py\u001b[0m:719: in run_until_complete\n",
      "    return future.result()\n",
      "\u001b[1m\u001b[31m../src/dialectical_framework/analyst/thought_mapping.py\u001b[0m:296: in extract\n",
      "    box = await self.find_multiple(prompt_stuff=prompt_stuff)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/langfuse/decorators/langfuse_decorator.py\u001b[0m:219: in async_wrapper\n",
      "    self._handle_exception(observation, e)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/langfuse/decorators/langfuse_decorator.py\u001b[0m:520: in _handle_exception\n",
      "    raise e\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/langfuse/decorators/langfuse_decorator.py\u001b[0m:217: in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/integrations/_middleware_factory.py\u001b[0m:146: in wrapper_async\n",
      "    result = await fn(*args, **kwargs)  # pyright: ignore [reportGeneralTypeIssues]\n",
      "\u001b[1m\u001b[31m../src/dialectical_framework/analyst/thought_mapping.py\u001b[0m:141: in find_multiple\n",
      "    return _find_multiple_call()\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/llm/_call.py\u001b[0m:346: in inner\n",
      "    result = decorated(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/core/base/_extract.py\u001b[0m:158: in inner\n",
      "    call_response = create_decorator(fn=fn, **create_decorator_kwargs)(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/core/base/_create.py\u001b[0m:232: in inner\n",
      "    response = create(stream=False, **call_kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/utils.py\u001b[0m:1283: in wrapper\n",
      "    raise e\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/utils.py\u001b[0m:1161: in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/main.py\u001b[0m:3241: in completion\n",
      "    raise exception_type(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m:2239: in exception_type\n",
      "    raise e\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "model = 'gpt-4-turbo'\n",
      "original_exception = AzureOpenAIError(\"Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\")\n",
      "custom_llm_provider = 'azure'\n",
      "completion_kwargs = {'api_key': None, 'api_version': None, 'audio': None, 'base_url': None, ...}\n",
      "extra_kwargs = {'litellm_call_id': 'f71d263a-34bc-4266-aa7f-1684719d365f', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ea301a0>}\n",
      "\n",
      "    def exception_type(  # type: ignore  # noqa: PLR0915\n",
      "        model,\n",
      "        original_exception,\n",
      "        custom_llm_provider,\n",
      "        completion_kwargs={},\n",
      "        extra_kwargs={},\n",
      "    ):\n",
      "        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n",
      "        if any(\n",
      "            isinstance(original_exception, exc_type)\n",
      "            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n",
      "        ):\n",
      "            return original_exception\n",
      "        exception_mapping_worked = False\n",
      "        exception_provider = custom_llm_provider\n",
      "        if litellm.suppress_debug_info is False:\n",
      "            print()  # noqa\n",
      "            print(  # noqa\n",
      "                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n",
      "            )  # noqa\n",
      "            print(  # noqa\n",
      "                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n",
      "            )  # noqa\n",
      "            print()  # noqa\n",
      "    \n",
      "        litellm_response_headers = _get_response_headers(\n",
      "            original_exception=original_exception\n",
      "        )\n",
      "        try:\n",
      "            error_str = str(original_exception)\n",
      "            if model:\n",
      "                if hasattr(original_exception, \"message\"):\n",
      "                    error_str = str(original_exception.message)\n",
      "                if isinstance(original_exception, BaseException):\n",
      "                    exception_type = type(original_exception).__name__\n",
      "                else:\n",
      "                    exception_type = \"\"\n",
      "    \n",
      "                ################################################################################\n",
      "                # Common Extra information needed for all providers\n",
      "                # We pass num retries, api_base, vertex_deployment etc to the exception here\n",
      "                ################################################################################\n",
      "                extra_information = \"\"\n",
      "                try:\n",
      "                    _api_base = litellm.get_api_base(\n",
      "                        model=model, optional_params=extra_kwargs\n",
      "                    )\n",
      "                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n",
      "                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n",
      "                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n",
      "                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n",
      "                    _model_group = _metadata.get(\"model_group\")\n",
      "                    _deployment = _metadata.get(\"deployment\")\n",
      "                    extra_information = f\"\\nModel: {model}\"\n",
      "    \n",
      "                    if (\n",
      "                        isinstance(custom_llm_provider, str)\n",
      "                        and len(custom_llm_provider) > 0\n",
      "                    ):\n",
      "                        exception_provider = (\n",
      "                            custom_llm_provider[0].upper()\n",
      "                            + custom_llm_provider[1:]\n",
      "                            + \"Exception\"\n",
      "                        )\n",
      "    \n",
      "                    if _api_base:\n",
      "                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n",
      "                    if (\n",
      "                        messages\n",
      "                        and len(messages) > 0\n",
      "                        and litellm.redact_messages_in_exceptions is False\n",
      "                    ):\n",
      "                        extra_information += f\"\\nMessages: `{messages}`\"\n",
      "    \n",
      "                    if _model_group is not None:\n",
      "                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n",
      "                    if _deployment is not None:\n",
      "                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n",
      "                    if _vertex_project is not None:\n",
      "                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n",
      "                    if _vertex_location is not None:\n",
      "                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n",
      "    \n",
      "                    # on litellm proxy add key name + team to exceptions\n",
      "                    extra_information = _add_key_name_and_team_to_alert(\n",
      "                        request_info=extra_information, metadata=_metadata\n",
      "                    )\n",
      "                except Exception:\n",
      "                    # DO NOT LET this Block raising the original exception\n",
      "                    pass\n",
      "    \n",
      "                ################################################################################\n",
      "                # End of Common Extra information Needed for all providers\n",
      "                ################################################################################\n",
      "    \n",
      "                ################################################################################\n",
      "                #################### Start of Provider Exception mapping ####################\n",
      "                ################################################################################\n",
      "    \n",
      "                if (\n",
      "                    \"Request Timeout Error\" in error_str\n",
      "                    or \"Request timed out\" in error_str\n",
      "                    or \"Timed out generating response\" in error_str\n",
      "                    or \"The read operation timed out\" in error_str\n",
      "                ):\n",
      "                    exception_mapping_worked = True\n",
      "    \n",
      "                    raise Timeout(\n",
      "                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n",
      "                        model=model,\n",
      "                        llm_provider=custom_llm_provider,\n",
      "                        litellm_debug_info=extra_information,\n",
      "                    )\n",
      "    \n",
      "                if (\n",
      "                    custom_llm_provider == \"litellm_proxy\"\n",
      "                ):  # handle special case where calling litellm proxy + exception str contains error message\n",
      "                    extract_and_raise_litellm_exception(\n",
      "                        response=getattr(original_exception, \"response\", None),\n",
      "                        error_str=error_str,\n",
      "                        model=model,\n",
      "                        custom_llm_provider=custom_llm_provider,\n",
      "                    )\n",
      "                if (\n",
      "                    custom_llm_provider == \"openai\"\n",
      "                    or custom_llm_provider == \"text-completion-openai\"\n",
      "                    or custom_llm_provider == \"custom_openai\"\n",
      "                    or custom_llm_provider in litellm.openai_compatible_providers\n",
      "                ):\n",
      "                    # custom_llm_provider is openai, make it OpenAI\n",
      "                    message = get_error_message(error_obj=original_exception)\n",
      "                    if message is None:\n",
      "                        if hasattr(original_exception, \"message\"):\n",
      "                            message = original_exception.message\n",
      "                        else:\n",
      "                            message = str(original_exception)\n",
      "    \n",
      "                    if message is not None and isinstance(\n",
      "                        message, str\n",
      "                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n",
      "                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n",
      "                        message = message.replace(\n",
      "                            \"openai.OpenAIError\",\n",
      "                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n",
      "                        )\n",
      "                    if custom_llm_provider == \"openai\":\n",
      "                        exception_provider = \"OpenAI\" + \"Exception\"\n",
      "                    else:\n",
      "                        exception_provider = (\n",
      "                            custom_llm_provider[0].upper()\n",
      "                            + custom_llm_provider[1:]\n",
      "                            + \"Exception\"\n",
      "                        )\n",
      "    \n",
      "                    if \"429\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"RateLimitError: {exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"This model's maximum context length is\" in error_str\n",
      "                        or \"string too long. Expected a string with maximum length\"\n",
      "                        in error_str\n",
      "                        or \"model's maximum context limit\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"invalid_request_error\" in error_str\n",
      "                        and \"model_not_found\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise NotFoundError(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"A timeout occurred\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        (\n",
      "                            \"invalid_request_error\" in error_str\n",
      "                            and \"content_policy_violation\" in error_str\n",
      "                        )\n",
      "                        or (\n",
      "                            \"Invalid prompt\" in error_str\n",
      "                            and \"violating our usage policy\" in error_str\n",
      "                        )\n",
      "                        or (\n",
      "                            \"request was rejected as a result of the safety system\"\n",
      "                            in error_str.lower()\n",
      "                        )\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"invalid_request_error\" in error_str\n",
      "                        and \"Incorrect API key provided\" not in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            body=getattr(original_exception, \"body\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Web server is returning an unknown error\" in error_str\n",
      "                        or \"The server had an error processing your request.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                        )\n",
      "                    elif \"Request too large\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"RateLimitError: {exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"Mistral API raised a streaming error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        _request = httpx.Request(\n",
      "                            method=\"POST\", url=\"https://api.openai.com/v1\"\n",
      "                        )\n",
      "                        raise APIError(\n",
      "                            status_code=500,\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            request=_request,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        exception_mapping_worked = True\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{exception_provider} - {message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"NotFoundError: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"Timeout Error: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                body=getattr(original_exception, \"body\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"RateLimitError: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"Timeout Error: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"APIError: {exception_provider} - {message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                request=getattr(original_exception, \"request\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                    else:\n",
      "                        # if no status code then it is an APIConnectionError: https://github.com/openai/openai-python#handling-errors\n",
      "                        # exception_mapping_worked = True\n",
      "                        raise APIConnectionError(\n",
      "                            message=f\"APIConnectionError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            request=httpx.Request(\n",
      "                                method=\"POST\", url=\"https://api.openai.com/v1/\"\n",
      "                            ),\n",
      "                        )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"anthropic\"\n",
      "                    or custom_llm_provider == \"anthropic_text\"\n",
      "                ):  # one of the anthropics\n",
      "                    if \"prompt is too long\" in error_str or \"prompt: length\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    elif \"overloaded_error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise InternalServerError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if \"Invalid API Key\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if \"content filtering policy\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if \"Client error '400 Bad Request'\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        verbose_logger.debug(\n",
      "                            f\"status_code: {original_exception.status_code}\"\n",
      "                        )\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 413\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"anthropic\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"anthropic\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"anthropic\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 500\n",
      "                            or original_exception.status_code == 529\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.InternalServerError(\n",
      "                                message=f\"AnthropicException - {error_str}. Handle with `litellm.InternalServerError`.\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.ServiceUnavailableError(\n",
      "                                message=f\"AnthropicException - {error_str}. Handle with `litellm.ServiceUnavailableError`.\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"replicate\":\n",
      "                    if \"Incorrect authentication token\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            llm_provider=\"replicate\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"input is too long\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"replicate\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif exception_type == \"ModelError\":\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"replicate\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Request was throttled\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            llm_provider=\"replicate\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 413\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"replicate\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise UnprocessableEntityError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"replicate\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"replicate\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise UnprocessableEntityError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    exception_mapping_worked = True\n",
      "                    raise APIError(\n",
      "                        status_code=500,\n",
      "                        message=f\"ReplicateException - {str(original_exception)}\",\n",
      "                        llm_provider=\"replicate\",\n",
      "                        model=model,\n",
      "                        request=httpx.Request(\n",
      "                            method=\"POST\",\n",
      "                            url=\"https://api.replicate.com/v1/deployments\",\n",
      "                        ),\n",
      "                    )\n",
      "                elif custom_llm_provider in litellm._openai_like_providers:\n",
      "                    if \"authorization denied for\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "    \n",
      "                        # Predibase returns the raw API Key in the response - this block ensures it's not returned in the exception\n",
      "                        if (\n",
      "                            error_str is not None\n",
      "                            and isinstance(error_str, str)\n",
      "                            and \"bearer\" in error_str.lower()\n",
      "                        ):\n",
      "                            # only keep the first 10 chars after the occurnence of \"bearer\"\n",
      "                            _bearer_token_start_index = error_str.lower().find(\"bearer\")\n",
      "                            error_str = error_str[: _bearer_token_start_index + 14]\n",
      "                            error_str += \"XXXXXXX\" + '\"'\n",
      "    \n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"{custom_llm_provider}Exception: Authentication Error - {error_str}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"model's maximum context limit\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"{custom_llm_provider}Exception: Context Window Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                        )\n",
      "                    elif \"token_quota_reached\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"{custom_llm_provider}Exception: Rate Limit Errror - {error_str}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The server received an invalid response from an upstream server.\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                        )\n",
      "                    elif \"model_no_support_for_function\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"{custom_llm_provider}Exception - Use 'watsonx_text' route instead. IBM WatsonX does not support `/text/chat` endpoint. - {error_str}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.InternalServerError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 401\n",
      "                            or original_exception.status_code == 403\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 422\n",
      "                            or original_exception.status_code == 424\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"bedrock\":\n",
      "                    if (\n",
      "                        \"too many tokens\" in error_str\n",
      "                        or \"expected maxLength:\" in error_str\n",
      "                        or \"Input is too long\" in error_str\n",
      "                        or \"prompt is too long\" in error_str\n",
      "                        or \"prompt: length: 1..\" in error_str\n",
      "                        or \"Too many input tokens\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"BedrockException: Context Window Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Conversation blocks and tool result blocks cannot be provided in the same turn.\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"BedrockException - {error_str}\\n. Enable 'litellm.modify_params=True' (for PROXY do: `litellm_settings::modify_params: True`) to insert a dummy assistant message and fix this error.\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Malformed input request\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"BedrockException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"A conversation must start with a user message.\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"BedrockException - {error_str}\\n. Pass in default user message via `completion(..,user_continue_message=)` or enable `litellm.modify_params=True`.\\nFor Proxy: do via `litellm_settings::modify_params: True` or user_continue_message under `litellm_params`\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Unable to locate credentials\" in error_str\n",
      "                        or \"The security token included in the request is invalid\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"BedrockException Invalid Authentication - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"AccessDeniedException\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise PermissionDeniedError(\n",
      "                            message=f\"BedrockException PermissionDeniedError - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"throttlingException\" in error_str\n",
      "                        or \"ThrottlingException\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"BedrockException: Rate Limit Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Connect timeout on endpoint URL\" in error_str\n",
      "                        or \"timed out\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"BedrockException: Timeout Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                        )\n",
      "                    elif \"Could not process image\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"BedrockException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=500,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\", url=\"https://api.openai.com/v1/\"\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"sagemaker\"\n",
      "                    or custom_llm_provider == \"sagemaker_chat\"\n",
      "                ):\n",
      "                    if \"Unable to locate credentials\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"litellm.BadRequestError: SagemakerException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"sagemaker\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Input validation error: `best_of` must be > 0 and <= 2\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=\"SagemakerException - the value of 'n' must be > 0 and <= 2 for sagemaker endpoints\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"sagemaker\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"`inputs` tokens + `max_new_tokens` must be <=\" in error_str\n",
      "                        or \"instance type with more CPU capacity or memory\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"SagemakerException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"sagemaker\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=500,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\", url=\"https://api.openai.com/v1/\"\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 422\n",
      "                            or original_exception.status_code == 424\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"vertex_ai\"\n",
      "                    or custom_llm_provider == \"vertex_ai_beta\"\n",
      "                    or custom_llm_provider == \"gemini\"\n",
      "                ):\n",
      "                    if (\n",
      "                        \"Vertex AI API has not been used in project\" in error_str\n",
      "                        or \"Unable to find your project\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"litellm.BadRequestError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            response=httpx.Response(\n",
      "                                status_code=400,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    if \"400 Request payload size exceeds\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"VertexException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"None Unknown Error.\" in error_str\n",
      "                        or \"Content has no parts.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"litellm.InternalServerError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            response=httpx.Response(\n",
      "                                status_code=500,\n",
      "                                content=str(original_exception),\n",
      "                                request=httpx.Request(method=\"completion\", url=\"https://github.com/BerriAI/litellm\"),  # type: ignore\n",
      "                            ),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"API key not valid.\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"{custom_llm_provider}Exception - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"403\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"VertexAIException BadRequestError - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            response=httpx.Response(\n",
      "                                status_code=403,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The response was blocked.\" in error_str\n",
      "                        or \"Output blocked by content filtering policy\"\n",
      "                        in error_str  # anthropic on vertex ai\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=f\"VertexAIException ContentPolicyViolationError - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=httpx.Response(\n",
      "                                status_code=400,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"429 Quota exceeded\" in error_str\n",
      "                        or \"Quota exceeded for\" in error_str\n",
      "                        or \"IndexError: list index out of range\" in error_str\n",
      "                        or \"429 Unable to submit request because the service is temporarily out of capacity.\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"litellm.RateLimitError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=httpx.Response(\n",
      "                                status_code=429,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"500 Internal Server Error\" in error_str\n",
      "                        or \"The model is overloaded.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"litellm.InternalServerError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"VertexAIException BadRequestError - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"vertex_ai\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=400,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\",\n",
      "                                        url=\"https://cloud.google.com/vertex-ai/\",\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        if original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        if original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "    \n",
      "                        if original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"litellm.RateLimitError: VertexAIException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"vertex_ai\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=429,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\",\n",
      "                                        url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.InternalServerError(\n",
      "                                message=f\"VertexAIException InternalServerError - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"vertex_ai\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=500,\n",
      "                                    content=str(original_exception),\n",
      "                                    request=httpx.Request(method=\"completion\", url=\"https://github.com/BerriAI/litellm\"),  # type: ignore\n",
      "                                ),\n",
      "                            )\n",
      "                        if original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"palm\" or custom_llm_provider == \"gemini\":\n",
      "                    if \"503 Getting metadata\" in error_str:\n",
      "                        # auth errors look like this\n",
      "                        # 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=\"GeminiException - Invalid api key\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"palm\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if (\n",
      "                        \"504 Deadline expired before operation could complete.\" in error_str\n",
      "                        or \"504 Deadline Exceeded\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"GeminiException - {original_exception.message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"palm\",\n",
      "                            exception_status_code=original_exception.status_code,\n",
      "                        )\n",
      "                    if \"400 Request payload size exceeds\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"GeminiException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"palm\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if (\n",
      "                        \"500 An internal error has occurred.\" in error_str\n",
      "                        or \"list index out of range\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise APIError(\n",
      "                            status_code=getattr(original_exception, \"status_code\", 500),\n",
      "                            message=f\"GeminiException - {original_exception.message}\",\n",
      "                            llm_provider=\"palm\",\n",
      "                            model=model,\n",
      "                            request=httpx.Response(\n",
      "                                status_code=429,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"GeminiException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"palm\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    # Dailed: Error occurred: 400 Request payload size exceeds the limit: 20000 bytes\n",
      "                elif custom_llm_provider == \"cloudflare\":\n",
      "                    if \"Authentication error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"Cloudflare Exception - {original_exception.message}\",\n",
      "                            llm_provider=\"cloudflare\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if \"must have required property\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"Cloudflare Exception - {original_exception.message}\",\n",
      "                            llm_provider=\"cloudflare\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"cohere\" or custom_llm_provider == \"cohere_chat\"\n",
      "                ):  # Cohere\n",
      "                    if (\n",
      "                        \"invalid api token\" in error_str\n",
      "                        or \"No API key provided.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"too many tokens\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"cohere\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 498\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    elif (\n",
      "                        \"CohereConnectionError\" in exception_type\n",
      "                    ):  # cohere seems to fire these errors when we load test it (1k+ messages / min)\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"invalid type:\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Unexpected server error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ServiceUnavailableError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    else:\n",
      "                        if hasattr(original_exception, \"status_code\"):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                        raise original_exception\n",
      "                elif custom_llm_provider == \"huggingface\":\n",
      "                    if \"length limit exceeded\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=error_str,\n",
      "                            model=model,\n",
      "                            llm_provider=\"huggingface\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"A valid user token is required\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=error_str,\n",
      "                            llm_provider=\"huggingface\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Rate limit reached\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=error_str,\n",
      "                            llm_provider=\"huggingface\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"huggingface\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"ai21\":\n",
      "                    if hasattr(original_exception, \"message\"):\n",
      "                        if \"Prompt has too many tokens\" in original_exception.message:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ContextWindowExceededError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        if \"Bad or missing API token.\" in original_exception.message:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                llm_provider=\"ai21\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                            )\n",
      "                        if original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                llm_provider=\"ai21\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                llm_provider=\"ai21\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"nlp_cloud\":\n",
      "                    if \"detail\" in error_str:\n",
      "                        if \"Input text length should not exceed\" in error_str:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ContextWindowExceededError(\n",
      "                                message=f\"NLPCloudException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif \"value is not a valid\" in error_str:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"NLPCloudException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=500,\n",
      "                                message=f\"NLPCloudException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                    if hasattr(\n",
      "                        original_exception, \"status_code\"\n",
      "                    ):  # https://docs.nlpcloud.com/?shell#errors\n",
      "                        if (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 406\n",
      "                            or original_exception.status_code == 413\n",
      "                            or original_exception.status_code == 422\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 401\n",
      "                            or original_exception.status_code == 403\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 522\n",
      "                            or original_exception.status_code == 524\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 429\n",
      "                            or original_exception.status_code == 402\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 500\n",
      "                            or original_exception.status_code == 503\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 504\n",
      "                            or original_exception.status_code == 520\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"together_ai\":\n",
      "                    try:\n",
      "                        error_response = json.loads(error_str)\n",
      "                    except Exception:\n",
      "                        error_response = {\"error\": error_str}\n",
      "                    if (\n",
      "                        \"error\" in error_response\n",
      "                        and \"`inputs` tokens + `max_new_tokens` must be <=\"\n",
      "                        in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error\" in error_response\n",
      "                        and \"invalid private key\" in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error\" in error_response\n",
      "                        and \"INVALID_ARGUMENT\" in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"A timeout occurred\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"TogetherAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error\" in error_response\n",
      "                        and \"API key doesn't match expected format.\"\n",
      "                        in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error_type\" in error_response\n",
      "                        and error_response[\"error_type\"] == \"validation\"\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"together_ai\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"together_ai\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                                llm_provider=\"together_ai\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 524:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                                llm_provider=\"together_ai\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                    else:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise APIError(\n",
      "                            status_code=original_exception.status_code,\n",
      "                            message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            model=model,\n",
      "                            request=original_exception.request,\n",
      "                        )\n",
      "                elif custom_llm_provider == \"aleph_alpha\":\n",
      "                    if (\n",
      "                        \"This is longer than the model's maximum context length\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                            llm_provider=\"aleph_alpha\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"InvalidToken\" in error_str or \"No token provided\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                            llm_provider=\"aleph_alpha\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        verbose_logger.debug(\n",
      "                            f\"status code: {original_exception.status_code}\"\n",
      "                        )\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        raise original_exception\n",
      "                    raise original_exception\n",
      "                elif (\n",
      "                    custom_llm_provider == \"ollama\" or custom_llm_provider == \"ollama_chat\"\n",
      "                ):\n",
      "                    if isinstance(original_exception, dict):\n",
      "                        error_str = original_exception.get(\"error\", \"\")\n",
      "                    else:\n",
      "                        error_str = str(original_exception)\n",
      "                    if \"no such file or directory\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"OllamaException: Invalid Model/Model not loaded - {original_exception}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"ollama\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Failed to establish a new connection\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ServiceUnavailableError(\n",
      "                            message=f\"OllamaException: {original_exception}\",\n",
      "                            llm_provider=\"ollama\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Invalid response object from API\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"OllamaException: {original_exception}\",\n",
      "                            llm_provider=\"ollama\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Read timed out\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"OllamaException: {original_exception}\",\n",
      "                            llm_provider=\"ollama\",\n",
      "                            model=model,\n",
      "                        )\n",
      "                elif custom_llm_provider == \"vllm\":\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 0:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIConnectionError(\n",
      "                                message=f\"VLLMException - {original_exception.message}\",\n",
      "                                llm_provider=\"vllm\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"azure\" or custom_llm_provider == \"azure_text\":\n",
      "                    message = get_error_message(error_obj=original_exception)\n",
      "                    if message is None:\n",
      "                        if hasattr(original_exception, \"message\"):\n",
      "                            message = original_exception.message\n",
      "                        else:\n",
      "                            message = str(original_exception)\n",
      "    \n",
      "                    if \"Internal server error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"AzureException Internal server error - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"This model's maximum context length is\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"AzureException ContextWindowExceededError - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"DeploymentNotFound\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise NotFoundError(\n",
      "                            message=f\"AzureException NotFoundError - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        (\n",
      "                            \"invalid_request_error\" in error_str\n",
      "                            and \"content_policy_violation\" in error_str\n",
      "                        )\n",
      "                        or (\n",
      "                            \"The response was filtered due to the prompt triggering Azure OpenAI's content management\"\n",
      "                            in error_str\n",
      "                        )\n",
      "                        or \"Your task failed as a result of our safety system\" in error_str\n",
      "                        or \"The model produced invalid content\" in error_str\n",
      "                        or \"content_filter_policy\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=f\"litellm.ContentPolicyViolationError: AzureException - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"invalid_request_error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"AzureException BadRequestError - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            body=getattr(original_exception, \"body\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The api_key client option must be set either by passing api_key to the client or by setting\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"{exception_provider} AuthenticationError - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Connection error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise APIConnectionError(\n",
      "                            message=f\"{exception_provider} APIConnectionError - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        exception_mapping_worked = True\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AzureException - {message}\",\n",
      "                                llm_provider=\"azure\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                body=getattr(original_exception, \"body\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AzureException AuthenticationError - {message}\",\n",
      "                                llm_provider=\"azure\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AzureException Timeout - {message}\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                llm_provider=\"azure\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AzureException BadRequestError - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AzureException RateLimitError - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"AzureException ServiceUnavailableError - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AzureException Timeout - {message}\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                llm_provider=\"azure\",\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      ">                           raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"AzureException APIError - {message}\",\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                model=model,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\", url=\"https://openai.com/\"\n",
      "                                ),\n",
      "                            )\n",
      "\u001b[1m\u001b[31mE                           litellm.exceptions.APIError: litellm.APIError: AzureException APIError - Resource not found\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m:2077: APIError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\u001b[31m\u001b[1m_________________________________ test_wheel_2 _________________________________\u001b[0m\n",
      "\n",
      "self = <litellm.llms.azure.azure.AzureChatCompletion object at 0x10db8dd30>\n",
      "model = 'gpt-4-turbo'\n",
      "messages = [{'content': '<context>Putin started the war, Ukraine will not surrender and will finally win!</context>', 'role': 'us...on\\'t mention any special denotations such as \"T\", \"T1\", \"T+\", \"A-\", \"Ac\", \"Re\", etc.\\n</formatting>', 'role': 'user'}]\n",
      "model_response = ModelResponse(id='chatcmpl-f5decfff-d8c2-4612-beef-85b13ae33db6', created=1748418120, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "api_key = '830bfa7d0fd34a5dae45e1a1d8f879de'\n",
      "api_base = 'https://openai-01-dialexityswedencentral.openai.azure.com/'\n",
      "api_version = '2024-10-21DIALEXITY_DEFAULT_MODEL=gpt-4', api_type = 'azure'\n",
      "azure_ad_token = None, azure_ad_token_provider = None, dynamic_params = False\n",
      "print_verbose = <function print_verbose at 0x10dbc3e20>, timeout = 600.0\n",
      "logging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ed49bd0>\n",
      "optional_params = {'extra_body': {}, 'stream': False, 'tool_choice': 'required', 'tools': [{'function': {'description': 'Correctly forma..., 'properties': {'dialectical_components': {...}}, 'required': ['dialectical_components'], ...}}, 'type': 'function'}]}\n",
      "litellm_params = {'acompletion': False, 'aembedding': None, 'api_base': None, 'api_key': None, ...}\n",
      "logger_fn = None, acompletion = False, headers = None, client = None\n",
      "\n",
      "    def completion(  # noqa: PLR0915\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: list,\n",
      "        model_response: ModelResponse,\n",
      "        api_key: str,\n",
      "        api_base: str,\n",
      "        api_version: str,\n",
      "        api_type: str,\n",
      "        azure_ad_token: str,\n",
      "        azure_ad_token_provider: Callable,\n",
      "        dynamic_params: bool,\n",
      "        print_verbose: Callable,\n",
      "        timeout: Union[float, httpx.Timeout],\n",
      "        logging_obj: LiteLLMLoggingObj,\n",
      "        optional_params,\n",
      "        litellm_params,\n",
      "        logger_fn,\n",
      "        acompletion: bool = False,\n",
      "        headers: Optional[dict] = None,\n",
      "        client=None,\n",
      "    ):\n",
      "        if headers:\n",
      "            optional_params[\"extra_headers\"] = headers\n",
      "        try:\n",
      "            if model is None or messages is None:\n",
      "                raise AzureOpenAIError(\n",
      "                    status_code=422, message=\"Missing model or messages\"\n",
      "                )\n",
      "    \n",
      "            max_retries = optional_params.pop(\"max_retries\", None)\n",
      "            if max_retries is None:\n",
      "                max_retries = DEFAULT_MAX_RETRIES\n",
      "            json_mode: Optional[bool] = optional_params.pop(\"json_mode\", False)\n",
      "    \n",
      "            ### CHECK IF CLOUDFLARE AI GATEWAY ###\n",
      "            ### if so - set the model as part of the base url\n",
      "            if \"gateway.ai.cloudflare.com\" in api_base:\n",
      "                client = self._init_azure_client_for_cloudflare_ai_gateway(\n",
      "                    api_base=api_base,\n",
      "                    model=model,\n",
      "                    api_version=api_version,\n",
      "                    max_retries=max_retries,\n",
      "                    timeout=timeout,\n",
      "                    api_key=api_key,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    acompletion=acompletion,\n",
      "                    client=client,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "    \n",
      "                data = {\"model\": None, \"messages\": messages, **optional_params}\n",
      "            else:\n",
      "                data = litellm.AzureOpenAIConfig().transform_request(\n",
      "                    model=model,\n",
      "                    messages=messages,\n",
      "                    optional_params=optional_params,\n",
      "                    litellm_params=litellm_params,\n",
      "                    headers=headers or {},\n",
      "                )\n",
      "    \n",
      "            if acompletion is True:\n",
      "                if optional_params.get(\"stream\", False):\n",
      "                    return self.async_streaming(\n",
      "                        logging_obj=logging_obj,\n",
      "                        api_base=api_base,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        data=data,\n",
      "                        model=model,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        max_retries=max_retries,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "                else:\n",
      "                    return self.acompletion(\n",
      "                        api_base=api_base,\n",
      "                        data=data,\n",
      "                        model_response=model_response,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        model=model,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        logging_obj=logging_obj,\n",
      "                        max_retries=max_retries,\n",
      "                        convert_tool_call_to_json_mode=json_mode,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "            elif \"stream\" in optional_params and optional_params[\"stream\"] is True:\n",
      "                return self.streaming(\n",
      "                    logging_obj=logging_obj,\n",
      "                    api_base=api_base,\n",
      "                    dynamic_params=dynamic_params,\n",
      "                    data=data,\n",
      "                    model=model,\n",
      "                    api_key=api_key,\n",
      "                    api_version=api_version,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    timeout=timeout,\n",
      "                    client=client,\n",
      "                    max_retries=max_retries,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "            else:\n",
      "                ## LOGGING\n",
      "                logging_obj.pre_call(\n",
      "                    input=messages,\n",
      "                    api_key=api_key,\n",
      "                    additional_args={\n",
      "                        \"headers\": {\n",
      "                            \"api_key\": api_key,\n",
      "                            \"azure_ad_token\": azure_ad_token,\n",
      "                        },\n",
      "                        \"api_version\": api_version,\n",
      "                        \"api_base\": api_base,\n",
      "                        \"complete_input_dict\": data,\n",
      "                    },\n",
      "                )\n",
      "                if not isinstance(max_retries, int):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=422, message=\"max retries must be an int\"\n",
      "                    )\n",
      "                # init AzureOpenAI Client\n",
      "                azure_client = self.get_azure_openai_client(\n",
      "                    api_version=api_version,\n",
      "                    api_base=api_base,\n",
      "                    api_key=api_key,\n",
      "                    model=model,\n",
      "                    client=client,\n",
      "                    _is_async=False,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "                if not isinstance(azure_client, AzureOpenAI):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=500,\n",
      "                        message=\"azure_client is not an instance of AzureOpenAI\",\n",
      "                    )\n",
      "    \n",
      ">               headers, response = self.make_sync_azure_openai_chat_completion_request(\n",
      "                    azure_client=azure_client, data=data, timeout=timeout\n",
      "                )\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:328: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:148: in make_sync_azure_openai_chat_completion_request\n",
      "    raise e\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:140: in make_sync_azure_openai_chat_completion_request\n",
      "    raw_response = azure_client.chat.completions.with_raw_response.create(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_legacy_response.py\u001b[0m:364: in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_utils/_utils.py\u001b[0m:287: in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\u001b[0m:925: in create\n",
      "    return self._post(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_base_client.py\u001b[0m:1239: in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <openai.lib.azure.AzureOpenAI object at 0x10ed49f90>\n",
      "cast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\n",
      "options = FinalRequestOptions(method='post', url='/deployments/gpt-4-turbo/chat/completions', params={}, headers={'X-Stainless-R...ctical Components'}}, 'required': ['dialectical_components'], 'type': 'object'}}, 'type': 'function'}]}, extra_json={})\n",
      "\n",
      "    def request(\n",
      "        self,\n",
      "        cast_to: Type[ResponseT],\n",
      "        options: FinalRequestOptions,\n",
      "        *,\n",
      "        stream: bool = False,\n",
      "        stream_cls: type[_StreamT] | None = None,\n",
      "    ) -> ResponseT | _StreamT:\n",
      "        cast_to = self._maybe_override_cast_to(cast_to, options)\n",
      "    \n",
      "        # create a copy of the options we were given so that if the\n",
      "        # options are mutated later & we then retry, the retries are\n",
      "        # given the original options\n",
      "        input_options = model_copy(options)\n",
      "        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n",
      "            # ensure the idempotency key is reused between requests\n",
      "            input_options.idempotency_key = self._idempotency_key()\n",
      "    \n",
      "        response: httpx.Response | None = None\n",
      "        max_retries = input_options.get_max_retries(self.max_retries)\n",
      "    \n",
      "        retries_taken = 0\n",
      "        for retries_taken in range(max_retries + 1):\n",
      "            options = model_copy(input_options)\n",
      "            options = self._prepare_options(options)\n",
      "    \n",
      "            remaining_retries = max_retries - retries_taken\n",
      "            request = self._build_request(options, retries_taken=retries_taken)\n",
      "            self._prepare_request(request)\n",
      "    \n",
      "            kwargs: HttpxSendArgs = {}\n",
      "            if self.custom_auth is not None:\n",
      "                kwargs[\"auth\"] = self.custom_auth\n",
      "    \n",
      "            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n",
      "    \n",
      "            response = None\n",
      "            try:\n",
      "                response = self._client.send(\n",
      "                    request,\n",
      "                    stream=stream or self._should_stream_response_body(request=request),\n",
      "                    **kwargs,\n",
      "                )\n",
      "            except httpx.TimeoutException as err:\n",
      "                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n",
      "    \n",
      "                if remaining_retries > 0:\n",
      "                    self._sleep_for_retry(\n",
      "                        retries_taken=retries_taken,\n",
      "                        max_retries=max_retries,\n",
      "                        options=input_options,\n",
      "                        response=None,\n",
      "                    )\n",
      "                    continue\n",
      "    \n",
      "                log.debug(\"Raising timeout error\")\n",
      "                raise APITimeoutError(request=request) from err\n",
      "            except Exception as err:\n",
      "                log.debug(\"Encountered Exception\", exc_info=True)\n",
      "    \n",
      "                if remaining_retries > 0:\n",
      "                    self._sleep_for_retry(\n",
      "                        retries_taken=retries_taken,\n",
      "                        max_retries=max_retries,\n",
      "                        options=input_options,\n",
      "                        response=None,\n",
      "                    )\n",
      "                    continue\n",
      "    \n",
      "                log.debug(\"Raising connection error\")\n",
      "                raise APIConnectionError(request=request) from err\n",
      "    \n",
      "            log.debug(\n",
      "                'HTTP Response: %s %s \"%i %s\" %s',\n",
      "                request.method,\n",
      "                request.url,\n",
      "                response.status_code,\n",
      "                response.reason_phrase,\n",
      "                response.headers,\n",
      "            )\n",
      "            log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n",
      "    \n",
      "            try:\n",
      "                response.raise_for_status()\n",
      "            except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n",
      "                log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n",
      "    \n",
      "                if remaining_retries > 0 and self._should_retry(err.response):\n",
      "                    err.response.close()\n",
      "                    self._sleep_for_retry(\n",
      "                        retries_taken=retries_taken,\n",
      "                        max_retries=max_retries,\n",
      "                        options=input_options,\n",
      "                        response=response,\n",
      "                    )\n",
      "                    continue\n",
      "    \n",
      "                # If the response is streamed then we need to explicitly read the response\n",
      "                # to completion before attempting to access the response text.\n",
      "                if not err.response.is_closed:\n",
      "                    err.response.read()\n",
      "    \n",
      "                log.debug(\"Re-raising status error\")\n",
      ">               raise self._make_status_error_from_response(err.response) from None\n",
      "\u001b[1m\u001b[31mE               openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_base_client.py\u001b[0m:1034: NotFoundError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "model = 'gpt-4-turbo'\n",
      "messages = [{'content': '<context>Putin started the war, Ukraine will not surrender and will finally win!</context>', 'role': 'us...on\\'t mention any special denotations such as \"T\", \"T1\", \"T+\", \"A-\", \"Ac\", \"Re\", etc.\\n</formatting>', 'role': 'user'}]\n",
      "timeout = 600.0, temperature = None, top_p = None, n = None, stream = False\n",
      "stream_options = None, stop = None, max_completion_tokens = None\n",
      "max_tokens = None, modalities = None, prediction = None, audio = None\n",
      "presence_penalty = None, frequency_penalty = None, logit_bias = None\n",
      "user = None, reasoning_effort = None, response_format = None, seed = None\n",
      "tools = [{'function': {'description': 'Correctly formatted and typed parameters extracted from the completion. Must include re...itle': 'Dialectical Components', 'type': 'array'}}, 'required': ['dialectical_components'], ...}}, 'type': 'function'}]\n",
      "tool_choice = 'required', logprobs = None, top_logprobs = None\n",
      "parallel_tool_calls = None, web_search_options = None, deployment_id = None\n",
      "extra_headers = None, functions = None, function_call = None, base_url = None\n",
      "api_version = '2024-10-21DIALEXITY_DEFAULT_MODEL=gpt-4'\n",
      "api_key = '830bfa7d0fd34a5dae45e1a1d8f879de', model_list = None, thinking = None\n",
      "kwargs = {'litellm_call_id': '263e5be2-60ac-463e-858a-414baf0798bd', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ed49bd0>}\n",
      "args = {'api_key': None, 'api_version': None, 'audio': None, 'base_url': None, ...}\n",
      "api_base = 'https://openai-01-dialexityswedencentral.openai.azure.com/'\n",
      "mock_response = None, mock_tool_calls = None, mock_timeout = None\n",
      "force_timeout = 600, logger_fn = None\n",
      "\n",
      "    @client\n",
      "    def completion(  # type: ignore # noqa: PLR0915\n",
      "        model: str,\n",
      "        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n",
      "        messages: List = [],\n",
      "        timeout: Optional[Union[float, str, httpx.Timeout]] = None,\n",
      "        temperature: Optional[float] = None,\n",
      "        top_p: Optional[float] = None,\n",
      "        n: Optional[int] = None,\n",
      "        stream: Optional[bool] = None,\n",
      "        stream_options: Optional[dict] = None,\n",
      "        stop=None,\n",
      "        max_completion_tokens: Optional[int] = None,\n",
      "        max_tokens: Optional[int] = None,\n",
      "        modalities: Optional[List[ChatCompletionModality]] = None,\n",
      "        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n",
      "        audio: Optional[ChatCompletionAudioParam] = None,\n",
      "        presence_penalty: Optional[float] = None,\n",
      "        frequency_penalty: Optional[float] = None,\n",
      "        logit_bias: Optional[dict] = None,\n",
      "        user: Optional[str] = None,\n",
      "        # openai v1.0+ new params\n",
      "        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n",
      "        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n",
      "        seed: Optional[int] = None,\n",
      "        tools: Optional[List] = None,\n",
      "        tool_choice: Optional[Union[str, dict]] = None,\n",
      "        logprobs: Optional[bool] = None,\n",
      "        top_logprobs: Optional[int] = None,\n",
      "        parallel_tool_calls: Optional[bool] = None,\n",
      "        web_search_options: Optional[OpenAIWebSearchOptions] = None,\n",
      "        deployment_id=None,\n",
      "        extra_headers: Optional[dict] = None,\n",
      "        # soon to be deprecated params by OpenAI\n",
      "        functions: Optional[List] = None,\n",
      "        function_call: Optional[str] = None,\n",
      "        # set api_base, api_version, api_key\n",
      "        base_url: Optional[str] = None,\n",
      "        api_version: Optional[str] = None,\n",
      "        api_key: Optional[str] = None,\n",
      "        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n",
      "        # Optional liteLLM function params\n",
      "        thinking: Optional[AnthropicThinkingParam] = None,\n",
      "        **kwargs,\n",
      "    ) -> Union[ModelResponse, CustomStreamWrapper]:\n",
      "        \"\"\"\n",
      "        Perform a completion() using any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n",
      "        Parameters:\n",
      "            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n",
      "            messages (List): A list of message objects representing the conversation context (default is an empty list).\n",
      "    \n",
      "            OPTIONAL PARAMS\n",
      "            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n",
      "            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n",
      "            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n",
      "            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n",
      "            n (int, optional): The number of completions to generate (default is 1).\n",
      "            stream (bool, optional): If True, return a streaming response (default is False).\n",
      "            stream_options (dict, optional): A dictionary containing options for the streaming response. Only set this when you set stream: true.\n",
      "            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n",
      "            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n",
      "            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n",
      "            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request.. You can use `[\"text\", \"audio\"]`\n",
      "            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n",
      "            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n",
      "            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n",
      "            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n",
      "            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n",
      "            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n",
      "            logprobs (bool, optional): Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message\n",
      "            top_logprobs (int, optional): An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.\n",
      "            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n",
      "            api_base (str, optional): Base URL for the API (default is None).\n",
      "            api_version (str, optional): API version (default is None).\n",
      "            api_key (str, optional): API key (default is None).\n",
      "            model_list (list, optional): List of api base, version, keys\n",
      "            extra_headers (dict, optional): Additional headers to include in the request.\n",
      "    \n",
      "            LITELLM Specific Params\n",
      "            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n",
      "            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n",
      "            max_retries (int, optional): The number of retries to attempt (default is 0).\n",
      "        Returns:\n",
      "            ModelResponse: A response object containing the generated completion and associated metadata.\n",
      "    \n",
      "        Note:\n",
      "            - This function is used to perform completions() using the specified language model.\n",
      "            - It supports various optional parameters for customizing the completion behavior.\n",
      "            - If 'mock_response' is provided, a mock completion response is returned for testing or debugging.\n",
      "        \"\"\"\n",
      "        ### VALIDATE Request ###\n",
      "        if model is None:\n",
      "            raise ValueError(\"model param not passed in.\")\n",
      "        # validate messages\n",
      "        messages = validate_and_fix_openai_messages(messages=messages)\n",
      "        # validate tool_choice\n",
      "        tool_choice = validate_chat_completion_tool_choice(tool_choice=tool_choice)\n",
      "        ######### unpacking kwargs #####################\n",
      "        args = locals()\n",
      "        api_base = kwargs.get(\"api_base\", None)\n",
      "        mock_response = kwargs.get(\"mock_response\", None)\n",
      "        mock_tool_calls = kwargs.get(\"mock_tool_calls\", None)\n",
      "        mock_timeout = cast(Optional[bool], kwargs.get(\"mock_timeout\", None))\n",
      "        force_timeout = kwargs.get(\"force_timeout\", 600)  ## deprecated\n",
      "        logger_fn = kwargs.get(\"logger_fn\", None)\n",
      "        verbose = kwargs.get(\"verbose\", False)\n",
      "        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n",
      "        litellm_logging_obj = kwargs.get(\"litellm_logging_obj\", None)\n",
      "        id = kwargs.get(\"id\", None)\n",
      "        metadata = kwargs.get(\"metadata\", None)\n",
      "        model_info = kwargs.get(\"model_info\", None)\n",
      "        proxy_server_request = kwargs.get(\"proxy_server_request\", None)\n",
      "        fallbacks = kwargs.get(\"fallbacks\", None)\n",
      "        provider_specific_header = cast(\n",
      "            Optional[ProviderSpecificHeader], kwargs.get(\"provider_specific_header\", None)\n",
      "        )\n",
      "        headers = kwargs.get(\"headers\", None) or extra_headers\n",
      "    \n",
      "        ensure_alternating_roles: Optional[bool] = kwargs.get(\n",
      "            \"ensure_alternating_roles\", None\n",
      "        )\n",
      "        user_continue_message: Optional[ChatCompletionUserMessage] = kwargs.get(\n",
      "            \"user_continue_message\", None\n",
      "        )\n",
      "        assistant_continue_message: Optional[ChatCompletionAssistantMessage] = kwargs.get(\n",
      "            \"assistant_continue_message\", None\n",
      "        )\n",
      "        if headers is None:\n",
      "            headers = {}\n",
      "        if extra_headers is not None:\n",
      "            headers.update(extra_headers)\n",
      "        num_retries = kwargs.get(\n",
      "            \"num_retries\", None\n",
      "        )  ## alt. param for 'max_retries'. Use this to pass retries w/ instructor.\n",
      "        max_retries = kwargs.get(\"max_retries\", None)\n",
      "        cooldown_time = kwargs.get(\"cooldown_time\", None)\n",
      "        context_window_fallback_dict = kwargs.get(\"context_window_fallback_dict\", None)\n",
      "        organization = kwargs.get(\"organization\", None)\n",
      "        ### VERIFY SSL ###\n",
      "        ssl_verify = kwargs.get(\"ssl_verify\", None)\n",
      "        ### CUSTOM MODEL COST ###\n",
      "        input_cost_per_token = kwargs.get(\"input_cost_per_token\", None)\n",
      "        output_cost_per_token = kwargs.get(\"output_cost_per_token\", None)\n",
      "        input_cost_per_second = kwargs.get(\"input_cost_per_second\", None)\n",
      "        output_cost_per_second = kwargs.get(\"output_cost_per_second\", None)\n",
      "        ### CUSTOM PROMPT TEMPLATE ###\n",
      "        initial_prompt_value = kwargs.get(\"initial_prompt_value\", None)\n",
      "        roles = kwargs.get(\"roles\", None)\n",
      "        final_prompt_value = kwargs.get(\"final_prompt_value\", None)\n",
      "        bos_token = kwargs.get(\"bos_token\", None)\n",
      "        eos_token = kwargs.get(\"eos_token\", None)\n",
      "        preset_cache_key = kwargs.get(\"preset_cache_key\", None)\n",
      "        hf_model_name = kwargs.get(\"hf_model_name\", None)\n",
      "        supports_system_message = kwargs.get(\"supports_system_message\", None)\n",
      "        base_model = kwargs.get(\"base_model\", None)\n",
      "        ### DISABLE FLAGS ###\n",
      "        disable_add_transform_inline_image_block = kwargs.get(\n",
      "            \"disable_add_transform_inline_image_block\", None\n",
      "        )\n",
      "        ### TEXT COMPLETION CALLS ###\n",
      "        text_completion = kwargs.get(\"text_completion\", False)\n",
      "        atext_completion = kwargs.get(\"atext_completion\", False)\n",
      "        ### ASYNC CALLS ###\n",
      "        acompletion = kwargs.get(\"acompletion\", False)\n",
      "        client = kwargs.get(\"client\", None)\n",
      "        ### Admin Controls ###\n",
      "        no_log = kwargs.get(\"no-log\", False)\n",
      "        ### PROMPT MANAGEMENT ###\n",
      "        prompt_id = cast(Optional[str], kwargs.get(\"prompt_id\", None))\n",
      "        prompt_variables = cast(Optional[dict], kwargs.get(\"prompt_variables\", None))\n",
      "        ### COPY MESSAGES ### - related issue https://github.com/BerriAI/litellm/discussions/4489\n",
      "        messages = get_completion_messages(\n",
      "            messages=messages,\n",
      "            ensure_alternating_roles=ensure_alternating_roles or False,\n",
      "            user_continue_message=user_continue_message,\n",
      "            assistant_continue_message=assistant_continue_message,\n",
      "        )\n",
      "        ######## end of unpacking kwargs ###########\n",
      "        standard_openai_params = get_standard_openai_params(params=args)\n",
      "        non_default_params = get_non_default_completion_params(kwargs=kwargs)\n",
      "        litellm_params = {}  # used to prevent unbound var errors\n",
      "        ## PROMPT MANAGEMENT HOOKS ##\n",
      "        if isinstance(litellm_logging_obj, LiteLLMLoggingObj) and (\n",
      "            litellm_logging_obj.should_run_prompt_management_hooks(\n",
      "                prompt_id=prompt_id, non_default_params=non_default_params\n",
      "            )\n",
      "        ):\n",
      "            (\n",
      "                model,\n",
      "                messages,\n",
      "                optional_params,\n",
      "            ) = litellm_logging_obj.get_chat_completion_prompt(\n",
      "                model=model,\n",
      "                messages=messages,\n",
      "                non_default_params=non_default_params,\n",
      "                prompt_id=prompt_id,\n",
      "                prompt_variables=prompt_variables,\n",
      "                prompt_label=kwargs.get(\"prompt_label\", None),\n",
      "            )\n",
      "    \n",
      "        try:\n",
      "            if base_url is not None:\n",
      "                api_base = base_url\n",
      "            if num_retries is not None:\n",
      "                max_retries = num_retries\n",
      "            logging = litellm_logging_obj\n",
      "            fallbacks = fallbacks or litellm.model_fallbacks\n",
      "            if fallbacks is not None:\n",
      "                return completion_with_fallbacks(**args)\n",
      "            if model_list is not None:\n",
      "                deployments = [\n",
      "                    m[\"litellm_params\"] for m in model_list if m[\"model_name\"] == model\n",
      "                ]\n",
      "                return litellm.batch_completion_models(deployments=deployments, **args)\n",
      "            if litellm.model_alias_map and model in litellm.model_alias_map:\n",
      "                model = litellm.model_alias_map[\n",
      "                    model\n",
      "                ]  # update the model to the actual value if an alias has been passed in\n",
      "            model_response = ModelResponse()\n",
      "            setattr(model_response, \"usage\", litellm.Usage())\n",
      "            if (\n",
      "                kwargs.get(\"azure\", False) is True\n",
      "            ):  # don't remove flag check, to remain backwards compatible for repos like Codium\n",
      "                custom_llm_provider = \"azure\"\n",
      "            if deployment_id is not None:  # azure llms\n",
      "                model = deployment_id\n",
      "                custom_llm_provider = \"azure\"\n",
      "            model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n",
      "                model=model,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "                api_base=api_base,\n",
      "                api_key=api_key,\n",
      "            )\n",
      "    \n",
      "            if (\n",
      "                provider_specific_header is not None\n",
      "                and provider_specific_header[\"custom_llm_provider\"] == custom_llm_provider\n",
      "            ):\n",
      "                headers.update(provider_specific_header[\"extra_headers\"])\n",
      "    \n",
      "            if model_response is not None and hasattr(model_response, \"_hidden_params\"):\n",
      "                model_response._hidden_params[\"custom_llm_provider\"] = custom_llm_provider\n",
      "                model_response._hidden_params[\"region_name\"] = kwargs.get(\n",
      "                    \"aws_region_name\", None\n",
      "                )  # support region-based pricing for bedrock\n",
      "    \n",
      "            ### TIMEOUT LOGIC ###\n",
      "            timeout = timeout or kwargs.get(\"request_timeout\", 600) or 600\n",
      "            # set timeout for 10 minutes by default\n",
      "            if isinstance(timeout, httpx.Timeout) and not supports_httpx_timeout(\n",
      "                custom_llm_provider\n",
      "            ):\n",
      "                timeout = timeout.read or 600  # default 10 min timeout\n",
      "            elif not isinstance(timeout, httpx.Timeout):\n",
      "                timeout = float(timeout)  # type: ignore\n",
      "    \n",
      "            ### REGISTER CUSTOM MODEL PRICING -- IF GIVEN ###\n",
      "            if input_cost_per_token is not None and output_cost_per_token is not None:\n",
      "                litellm.register_model(\n",
      "                    {\n",
      "                        f\"{custom_llm_provider}/{model}\": {\n",
      "                            \"input_cost_per_token\": input_cost_per_token,\n",
      "                            \"output_cost_per_token\": output_cost_per_token,\n",
      "                            \"litellm_provider\": custom_llm_provider,\n",
      "                        }\n",
      "                    }\n",
      "                )\n",
      "            elif (\n",
      "                input_cost_per_second is not None\n",
      "            ):  # time based pricing just needs cost in place\n",
      "                output_cost_per_second = output_cost_per_second\n",
      "                litellm.register_model(\n",
      "                    {\n",
      "                        f\"{custom_llm_provider}/{model}\": {\n",
      "                            \"input_cost_per_second\": input_cost_per_second,\n",
      "                            \"output_cost_per_second\": output_cost_per_second,\n",
      "                            \"litellm_provider\": custom_llm_provider,\n",
      "                        }\n",
      "                    }\n",
      "                )\n",
      "            ### BUILD CUSTOM PROMPT TEMPLATE -- IF GIVEN ###\n",
      "            custom_prompt_dict = {}  # type: ignore\n",
      "            if (\n",
      "                initial_prompt_value\n",
      "                or roles\n",
      "                or final_prompt_value\n",
      "                or bos_token\n",
      "                or eos_token\n",
      "            ):\n",
      "                custom_prompt_dict = {model: {}}\n",
      "                if initial_prompt_value:\n",
      "                    custom_prompt_dict[model][\"initial_prompt_value\"] = initial_prompt_value\n",
      "                if roles:\n",
      "                    custom_prompt_dict[model][\"roles\"] = roles\n",
      "                if final_prompt_value:\n",
      "                    custom_prompt_dict[model][\"final_prompt_value\"] = final_prompt_value\n",
      "                if bos_token:\n",
      "                    custom_prompt_dict[model][\"bos_token\"] = bos_token\n",
      "                if eos_token:\n",
      "                    custom_prompt_dict[model][\"eos_token\"] = eos_token\n",
      "    \n",
      "            if kwargs.get(\"model_file_id_mapping\"):\n",
      "                messages = update_messages_with_model_file_ids(\n",
      "                    messages=messages,\n",
      "                    model_id=kwargs.get(\"model_info\", {}).get(\"id\", None),\n",
      "                    model_file_id_mapping=cast(\n",
      "                        Dict[str, Dict[str, str]], kwargs.get(\"model_file_id_mapping\")\n",
      "                    ),\n",
      "                )\n",
      "    \n",
      "            provider_config: Optional[BaseConfig] = None\n",
      "            if custom_llm_provider is not None and custom_llm_provider in [\n",
      "                provider.value for provider in LlmProviders\n",
      "            ]:\n",
      "                provider_config = ProviderConfigManager.get_provider_chat_config(\n",
      "                    model=model, provider=LlmProviders(custom_llm_provider)\n",
      "                )\n",
      "    \n",
      "            if provider_config is not None:\n",
      "                messages = provider_config.translate_developer_role_to_system_role(\n",
      "                    messages=messages\n",
      "                )\n",
      "    \n",
      "            if (\n",
      "                supports_system_message is not None\n",
      "                and isinstance(supports_system_message, bool)\n",
      "                and supports_system_message is False\n",
      "            ):\n",
      "                messages = map_system_message_pt(messages=messages)\n",
      "    \n",
      "            if dynamic_api_key is not None:\n",
      "                api_key = dynamic_api_key\n",
      "            # check if user passed in any of the OpenAI optional params\n",
      "            optional_param_args = {\n",
      "                \"functions\": functions,\n",
      "                \"function_call\": function_call,\n",
      "                \"temperature\": temperature,\n",
      "                \"top_p\": top_p,\n",
      "                \"n\": n,\n",
      "                \"stream\": stream,\n",
      "                \"stream_options\": stream_options,\n",
      "                \"stop\": stop,\n",
      "                \"max_tokens\": max_tokens,\n",
      "                \"max_completion_tokens\": max_completion_tokens,\n",
      "                \"modalities\": modalities,\n",
      "                \"prediction\": prediction,\n",
      "                \"audio\": audio,\n",
      "                \"presence_penalty\": presence_penalty,\n",
      "                \"frequency_penalty\": frequency_penalty,\n",
      "                \"logit_bias\": logit_bias,\n",
      "                \"user\": user,\n",
      "                # params to identify the model\n",
      "                \"model\": model,\n",
      "                \"custom_llm_provider\": custom_llm_provider,\n",
      "                \"response_format\": response_format,\n",
      "                \"seed\": seed,\n",
      "                \"tools\": tools,\n",
      "                \"tool_choice\": tool_choice,\n",
      "                \"max_retries\": max_retries,\n",
      "                \"logprobs\": logprobs,\n",
      "                \"top_logprobs\": top_logprobs,\n",
      "                \"api_version\": api_version,\n",
      "                \"parallel_tool_calls\": parallel_tool_calls,\n",
      "                \"messages\": messages,\n",
      "                \"reasoning_effort\": reasoning_effort,\n",
      "                \"thinking\": thinking,\n",
      "                \"web_search_options\": web_search_options,\n",
      "                \"allowed_openai_params\": kwargs.get(\"allowed_openai_params\"),\n",
      "            }\n",
      "            optional_params = get_optional_params(\n",
      "                **optional_param_args, **non_default_params\n",
      "            )\n",
      "            processed_non_default_params = pre_process_non_default_params(\n",
      "                model=model,\n",
      "                passed_params=optional_param_args,\n",
      "                special_params=non_default_params,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "                additional_drop_params=kwargs.get(\"additional_drop_params\"),\n",
      "            )\n",
      "            processed_non_default_params = add_provider_specific_params_to_optional_params(\n",
      "                optional_params=processed_non_default_params,\n",
      "                passed_params=non_default_params,\n",
      "            )\n",
      "    \n",
      "            if litellm.add_function_to_prompt and optional_params.get(\n",
      "                \"functions_unsupported_model\", None\n",
      "            ):  # if user opts to add it to prompt, when API doesn't support function calling\n",
      "                functions_unsupported_model = optional_params.pop(\n",
      "                    \"functions_unsupported_model\"\n",
      "                )\n",
      "                messages = function_call_prompt(\n",
      "                    messages=messages, functions=functions_unsupported_model\n",
      "                )\n",
      "    \n",
      "            # For logging - save the values of the litellm-specific params passed in\n",
      "            litellm_params = get_litellm_params(\n",
      "                acompletion=acompletion,\n",
      "                api_key=api_key,\n",
      "                force_timeout=force_timeout,\n",
      "                logger_fn=logger_fn,\n",
      "                verbose=verbose,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "                api_base=api_base,\n",
      "                litellm_call_id=kwargs.get(\"litellm_call_id\", None),\n",
      "                model_alias_map=litellm.model_alias_map,\n",
      "                completion_call_id=id,\n",
      "                metadata=metadata,\n",
      "                model_info=model_info,\n",
      "                proxy_server_request=proxy_server_request,\n",
      "                preset_cache_key=preset_cache_key,\n",
      "                no_log=no_log,\n",
      "                input_cost_per_second=input_cost_per_second,\n",
      "                input_cost_per_token=input_cost_per_token,\n",
      "                output_cost_per_second=output_cost_per_second,\n",
      "                output_cost_per_token=output_cost_per_token,\n",
      "                cooldown_time=cooldown_time,\n",
      "                text_completion=kwargs.get(\"text_completion\"),\n",
      "                azure_ad_token_provider=kwargs.get(\"azure_ad_token_provider\"),\n",
      "                user_continue_message=kwargs.get(\"user_continue_message\"),\n",
      "                base_model=base_model,\n",
      "                litellm_trace_id=kwargs.get(\"litellm_trace_id\"),\n",
      "                litellm_session_id=kwargs.get(\"litellm_session_id\"),\n",
      "                hf_model_name=hf_model_name,\n",
      "                custom_prompt_dict=custom_prompt_dict,\n",
      "                litellm_metadata=kwargs.get(\"litellm_metadata\"),\n",
      "                disable_add_transform_inline_image_block=disable_add_transform_inline_image_block,\n",
      "                drop_params=kwargs.get(\"drop_params\"),\n",
      "                prompt_id=prompt_id,\n",
      "                prompt_variables=prompt_variables,\n",
      "                ssl_verify=ssl_verify,\n",
      "                merge_reasoning_content_in_choices=kwargs.get(\n",
      "                    \"merge_reasoning_content_in_choices\", None\n",
      "                ),\n",
      "                use_litellm_proxy=kwargs.get(\"use_litellm_proxy\", False),\n",
      "                api_version=api_version,\n",
      "                azure_ad_token=kwargs.get(\"azure_ad_token\"),\n",
      "                tenant_id=kwargs.get(\"tenant_id\"),\n",
      "                client_id=kwargs.get(\"client_id\"),\n",
      "                client_secret=kwargs.get(\"client_secret\"),\n",
      "                azure_username=kwargs.get(\"azure_username\"),\n",
      "                azure_password=kwargs.get(\"azure_password\"),\n",
      "                max_retries=max_retries,\n",
      "                timeout=timeout,\n",
      "            )\n",
      "            cast(LiteLLMLoggingObj, logging).update_environment_variables(\n",
      "                model=model,\n",
      "                user=user,\n",
      "                optional_params=processed_non_default_params,  # [IMPORTANT] - using processed_non_default_params ensures consistent params logged to langfuse for finetuning / eval datasets.\n",
      "                litellm_params=litellm_params,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "            )\n",
      "            if mock_response or mock_tool_calls or mock_timeout:\n",
      "                kwargs.pop(\"mock_timeout\", None)  # remove for any fallbacks triggered\n",
      "                return mock_completion(\n",
      "                    model,\n",
      "                    messages,\n",
      "                    stream=stream,\n",
      "                    n=n,\n",
      "                    mock_response=mock_response,\n",
      "                    mock_tool_calls=mock_tool_calls,\n",
      "                    logging=logging,\n",
      "                    acompletion=acompletion,\n",
      "                    mock_delay=kwargs.get(\"mock_delay\", None),\n",
      "                    custom_llm_provider=custom_llm_provider,\n",
      "                    mock_timeout=mock_timeout,\n",
      "                    timeout=timeout,\n",
      "                )\n",
      "    \n",
      "            if custom_llm_provider == \"azure\":\n",
      "                # azure configs\n",
      "                ## check dynamic params ##\n",
      "                dynamic_params = False\n",
      "                if client is not None and (\n",
      "                    isinstance(client, openai.AzureOpenAI)\n",
      "                    or isinstance(client, openai.AsyncAzureOpenAI)\n",
      "                ):\n",
      "                    dynamic_params = _check_dynamic_azure_params(\n",
      "                        azure_client_params={\"api_version\": api_version},\n",
      "                        azure_client=client,\n",
      "                    )\n",
      "    \n",
      "                api_type = get_secret(\"AZURE_API_TYPE\") or \"azure\"\n",
      "    \n",
      "                api_base = api_base or litellm.api_base or get_secret(\"AZURE_API_BASE\")\n",
      "    \n",
      "                api_version = (\n",
      "                    api_version\n",
      "                    or litellm.api_version\n",
      "                    or get_secret(\"AZURE_API_VERSION\")\n",
      "                    or litellm.AZURE_DEFAULT_API_VERSION\n",
      "                )\n",
      "    \n",
      "                api_key = (\n",
      "                    api_key\n",
      "                    or litellm.api_key\n",
      "                    or litellm.azure_key\n",
      "                    or get_secret(\"AZURE_OPENAI_API_KEY\")\n",
      "                    or get_secret(\"AZURE_API_KEY\")\n",
      "                )\n",
      "    \n",
      "                azure_ad_token = optional_params.get(\"extra_body\", {}).pop(\n",
      "                    \"azure_ad_token\", None\n",
      "                ) or get_secret(\"AZURE_AD_TOKEN\")\n",
      "    \n",
      "                azure_ad_token_provider = litellm_params.get(\n",
      "                    \"azure_ad_token_provider\", None\n",
      "                )\n",
      "    \n",
      "                headers = headers or litellm.headers\n",
      "    \n",
      "                if extra_headers is not None:\n",
      "                    optional_params[\"extra_headers\"] = extra_headers\n",
      "                if max_retries is not None:\n",
      "                    optional_params[\"max_retries\"] = max_retries\n",
      "    \n",
      "                if litellm.AzureOpenAIO1Config().is_o_series_model(model=model):\n",
      "                    ## LOAD CONFIG - if set\n",
      "                    config = litellm.AzureOpenAIO1Config.get_config()\n",
      "                    for k, v in config.items():\n",
      "                        if (\n",
      "                            k not in optional_params\n",
      "                        ):  # completion(top_k=3) > azure_config(top_k=3) <- allows for dynamic variables to be passed in\n",
      "                            optional_params[k] = v\n",
      "    \n",
      "                    response = azure_o1_chat_completions.completion(\n",
      "                        model=model,\n",
      "                        messages=messages,\n",
      "                        headers=headers,\n",
      "                        api_key=api_key,\n",
      "                        api_base=api_base,\n",
      "                        api_version=api_version,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        model_response=model_response,\n",
      "                        print_verbose=print_verbose,\n",
      "                        optional_params=optional_params,\n",
      "                        litellm_params=litellm_params,\n",
      "                        logger_fn=logger_fn,\n",
      "                        logging_obj=logging,\n",
      "                        acompletion=acompletion,\n",
      "                        timeout=timeout,  # type: ignore\n",
      "                        client=client,  # pass AsyncAzureOpenAI, AzureOpenAI client\n",
      "                        custom_llm_provider=custom_llm_provider,\n",
      "                    )\n",
      "                else:\n",
      "                    ## LOAD CONFIG - if set\n",
      "                    config = litellm.AzureOpenAIConfig.get_config()\n",
      "                    for k, v in config.items():\n",
      "                        if (\n",
      "                            k not in optional_params\n",
      "                        ):  # completion(top_k=3) > azure_config(top_k=3) <- allows for dynamic variables to be passed in\n",
      "                            optional_params[k] = v\n",
      "    \n",
      "                    ## COMPLETION CALL\n",
      ">                   response = azure_chat_completions.completion(\n",
      "                        model=model,\n",
      "                        messages=messages,\n",
      "                        headers=headers,\n",
      "                        api_key=api_key,\n",
      "                        api_base=api_base,\n",
      "                        api_version=api_version,\n",
      "                        api_type=api_type,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        model_response=model_response,\n",
      "                        print_verbose=print_verbose,\n",
      "                        optional_params=optional_params,\n",
      "                        litellm_params=litellm_params,\n",
      "                        logger_fn=logger_fn,\n",
      "                        logging_obj=logging,\n",
      "                        acompletion=acompletion,\n",
      "                        timeout=timeout,  # type: ignore\n",
      "                        client=client,  # pass AsyncAzureOpenAI, AzureOpenAI client\n",
      "                    )\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/main.py\u001b[0m:1366: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <litellm.llms.azure.azure.AzureChatCompletion object at 0x10db8dd30>\n",
      "model = 'gpt-4-turbo'\n",
      "messages = [{'content': '<context>Putin started the war, Ukraine will not surrender and will finally win!</context>', 'role': 'us...on\\'t mention any special denotations such as \"T\", \"T1\", \"T+\", \"A-\", \"Ac\", \"Re\", etc.\\n</formatting>', 'role': 'user'}]\n",
      "model_response = ModelResponse(id='chatcmpl-f5decfff-d8c2-4612-beef-85b13ae33db6', created=1748418120, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "api_key = '830bfa7d0fd34a5dae45e1a1d8f879de'\n",
      "api_base = 'https://openai-01-dialexityswedencentral.openai.azure.com/'\n",
      "api_version = '2024-10-21DIALEXITY_DEFAULT_MODEL=gpt-4', api_type = 'azure'\n",
      "azure_ad_token = None, azure_ad_token_provider = None, dynamic_params = False\n",
      "print_verbose = <function print_verbose at 0x10dbc3e20>, timeout = 600.0\n",
      "logging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ed49bd0>\n",
      "optional_params = {'extra_body': {}, 'stream': False, 'tool_choice': 'required', 'tools': [{'function': {'description': 'Correctly forma..., 'properties': {'dialectical_components': {...}}, 'required': ['dialectical_components'], ...}}, 'type': 'function'}]}\n",
      "litellm_params = {'acompletion': False, 'aembedding': None, 'api_base': None, 'api_key': None, ...}\n",
      "logger_fn = None, acompletion = False, headers = None, client = None\n",
      "\n",
      "    def completion(  # noqa: PLR0915\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: list,\n",
      "        model_response: ModelResponse,\n",
      "        api_key: str,\n",
      "        api_base: str,\n",
      "        api_version: str,\n",
      "        api_type: str,\n",
      "        azure_ad_token: str,\n",
      "        azure_ad_token_provider: Callable,\n",
      "        dynamic_params: bool,\n",
      "        print_verbose: Callable,\n",
      "        timeout: Union[float, httpx.Timeout],\n",
      "        logging_obj: LiteLLMLoggingObj,\n",
      "        optional_params,\n",
      "        litellm_params,\n",
      "        logger_fn,\n",
      "        acompletion: bool = False,\n",
      "        headers: Optional[dict] = None,\n",
      "        client=None,\n",
      "    ):\n",
      "        if headers:\n",
      "            optional_params[\"extra_headers\"] = headers\n",
      "        try:\n",
      "            if model is None or messages is None:\n",
      "                raise AzureOpenAIError(\n",
      "                    status_code=422, message=\"Missing model or messages\"\n",
      "                )\n",
      "    \n",
      "            max_retries = optional_params.pop(\"max_retries\", None)\n",
      "            if max_retries is None:\n",
      "                max_retries = DEFAULT_MAX_RETRIES\n",
      "            json_mode: Optional[bool] = optional_params.pop(\"json_mode\", False)\n",
      "    \n",
      "            ### CHECK IF CLOUDFLARE AI GATEWAY ###\n",
      "            ### if so - set the model as part of the base url\n",
      "            if \"gateway.ai.cloudflare.com\" in api_base:\n",
      "                client = self._init_azure_client_for_cloudflare_ai_gateway(\n",
      "                    api_base=api_base,\n",
      "                    model=model,\n",
      "                    api_version=api_version,\n",
      "                    max_retries=max_retries,\n",
      "                    timeout=timeout,\n",
      "                    api_key=api_key,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    acompletion=acompletion,\n",
      "                    client=client,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "    \n",
      "                data = {\"model\": None, \"messages\": messages, **optional_params}\n",
      "            else:\n",
      "                data = litellm.AzureOpenAIConfig().transform_request(\n",
      "                    model=model,\n",
      "                    messages=messages,\n",
      "                    optional_params=optional_params,\n",
      "                    litellm_params=litellm_params,\n",
      "                    headers=headers or {},\n",
      "                )\n",
      "    \n",
      "            if acompletion is True:\n",
      "                if optional_params.get(\"stream\", False):\n",
      "                    return self.async_streaming(\n",
      "                        logging_obj=logging_obj,\n",
      "                        api_base=api_base,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        data=data,\n",
      "                        model=model,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        max_retries=max_retries,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "                else:\n",
      "                    return self.acompletion(\n",
      "                        api_base=api_base,\n",
      "                        data=data,\n",
      "                        model_response=model_response,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        model=model,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        logging_obj=logging_obj,\n",
      "                        max_retries=max_retries,\n",
      "                        convert_tool_call_to_json_mode=json_mode,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "            elif \"stream\" in optional_params and optional_params[\"stream\"] is True:\n",
      "                return self.streaming(\n",
      "                    logging_obj=logging_obj,\n",
      "                    api_base=api_base,\n",
      "                    dynamic_params=dynamic_params,\n",
      "                    data=data,\n",
      "                    model=model,\n",
      "                    api_key=api_key,\n",
      "                    api_version=api_version,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    timeout=timeout,\n",
      "                    client=client,\n",
      "                    max_retries=max_retries,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "            else:\n",
      "                ## LOGGING\n",
      "                logging_obj.pre_call(\n",
      "                    input=messages,\n",
      "                    api_key=api_key,\n",
      "                    additional_args={\n",
      "                        \"headers\": {\n",
      "                            \"api_key\": api_key,\n",
      "                            \"azure_ad_token\": azure_ad_token,\n",
      "                        },\n",
      "                        \"api_version\": api_version,\n",
      "                        \"api_base\": api_base,\n",
      "                        \"complete_input_dict\": data,\n",
      "                    },\n",
      "                )\n",
      "                if not isinstance(max_retries, int):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=422, message=\"max retries must be an int\"\n",
      "                    )\n",
      "                # init AzureOpenAI Client\n",
      "                azure_client = self.get_azure_openai_client(\n",
      "                    api_version=api_version,\n",
      "                    api_base=api_base,\n",
      "                    api_key=api_key,\n",
      "                    model=model,\n",
      "                    client=client,\n",
      "                    _is_async=False,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "                if not isinstance(azure_client, AzureOpenAI):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=500,\n",
      "                        message=\"azure_client is not an instance of AzureOpenAI\",\n",
      "                    )\n",
      "    \n",
      "                headers, response = self.make_sync_azure_openai_chat_completion_request(\n",
      "                    azure_client=azure_client, data=data, timeout=timeout\n",
      "                )\n",
      "                stringified_response = response.model_dump()\n",
      "                ## LOGGING\n",
      "                logging_obj.post_call(\n",
      "                    input=messages,\n",
      "                    api_key=api_key,\n",
      "                    original_response=stringified_response,\n",
      "                    additional_args={\n",
      "                        \"headers\": headers,\n",
      "                        \"api_version\": api_version,\n",
      "                        \"api_base\": api_base,\n",
      "                    },\n",
      "                )\n",
      "                return convert_to_model_response_object(\n",
      "                    response_object=stringified_response,\n",
      "                    model_response_object=model_response,\n",
      "                    convert_tool_call_to_json_mode=json_mode,\n",
      "                    _response_headers=headers,\n",
      "                )\n",
      "        except AzureOpenAIError as e:\n",
      "            raise e\n",
      "        except Exception as e:\n",
      "            status_code = getattr(e, \"status_code\", 500)\n",
      "            error_headers = getattr(e, \"headers\", None)\n",
      "            error_response = getattr(e, \"response\", None)\n",
      "            error_body = getattr(e, \"body\", None)\n",
      "            if error_headers is None and error_response:\n",
      "                error_headers = getattr(error_response, \"headers\", None)\n",
      ">           raise AzureOpenAIError(\n",
      "                status_code=status_code,\n",
      "                message=str(e),\n",
      "                headers=error_headers,\n",
      "                body=error_body,\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           litellm.llms.azure.common_utils.AzureOpenAIError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:358: AzureOpenAIError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "    @observe()\n",
      "    def test_wheel_2():\n",
      ">       wheels = asyncio.run(factory.build(theses=[\"\", None]))\n",
      "\n",
      "\u001b[1m\u001b[31mtest_analyst.py\u001b[0m:100: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py\u001b[0m:195: in run\n",
      "    return runner.run(main)\n",
      "\u001b[1m\u001b[31m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py\u001b[0m:118: in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "\u001b[1m\u001b[31m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py\u001b[0m:719: in run_until_complete\n",
      "    return future.result()\n",
      "\u001b[1m\u001b[31m../src/dialectical_framework/synthesist/factories/wheel_builder.py\u001b[0m:59: in build\n",
      "    cycles: List[Cycle] = await analyst.extract(wu_count)\n",
      "\u001b[1m\u001b[31m../src/dialectical_framework/analyst/thought_mapping.py\u001b[0m:293: in extract\n",
      "    box = await self.find_multiple(prompt_stuff=prompt_stuff)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/langfuse/decorators/langfuse_decorator.py\u001b[0m:219: in async_wrapper\n",
      "    self._handle_exception(observation, e)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/langfuse/decorators/langfuse_decorator.py\u001b[0m:520: in _handle_exception\n",
      "    raise e\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/langfuse/decorators/langfuse_decorator.py\u001b[0m:217: in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/integrations/_middleware_factory.py\u001b[0m:146: in wrapper_async\n",
      "    result = await fn(*args, **kwargs)  # pyright: ignore [reportGeneralTypeIssues]\n",
      "\u001b[1m\u001b[31m../src/dialectical_framework/analyst/thought_mapping.py\u001b[0m:141: in find_multiple\n",
      "    return _find_multiple_call()\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/llm/_call.py\u001b[0m:346: in inner\n",
      "    result = decorated(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/core/base/_extract.py\u001b[0m:158: in inner\n",
      "    call_response = create_decorator(fn=fn, **create_decorator_kwargs)(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/core/base/_create.py\u001b[0m:232: in inner\n",
      "    response = create(stream=False, **call_kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/utils.py\u001b[0m:1283: in wrapper\n",
      "    raise e\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/utils.py\u001b[0m:1161: in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/main.py\u001b[0m:3241: in completion\n",
      "    raise exception_type(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m:2239: in exception_type\n",
      "    raise e\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "model = 'gpt-4-turbo'\n",
      "original_exception = AzureOpenAIError(\"Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\")\n",
      "custom_llm_provider = 'azure'\n",
      "completion_kwargs = {'api_key': None, 'api_version': None, 'audio': None, 'base_url': None, ...}\n",
      "extra_kwargs = {'litellm_call_id': '263e5be2-60ac-463e-858a-414baf0798bd', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ed49bd0>}\n",
      "\n",
      "    def exception_type(  # type: ignore  # noqa: PLR0915\n",
      "        model,\n",
      "        original_exception,\n",
      "        custom_llm_provider,\n",
      "        completion_kwargs={},\n",
      "        extra_kwargs={},\n",
      "    ):\n",
      "        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n",
      "        if any(\n",
      "            isinstance(original_exception, exc_type)\n",
      "            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n",
      "        ):\n",
      "            return original_exception\n",
      "        exception_mapping_worked = False\n",
      "        exception_provider = custom_llm_provider\n",
      "        if litellm.suppress_debug_info is False:\n",
      "            print()  # noqa\n",
      "            print(  # noqa\n",
      "                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n",
      "            )  # noqa\n",
      "            print(  # noqa\n",
      "                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n",
      "            )  # noqa\n",
      "            print()  # noqa\n",
      "    \n",
      "        litellm_response_headers = _get_response_headers(\n",
      "            original_exception=original_exception\n",
      "        )\n",
      "        try:\n",
      "            error_str = str(original_exception)\n",
      "            if model:\n",
      "                if hasattr(original_exception, \"message\"):\n",
      "                    error_str = str(original_exception.message)\n",
      "                if isinstance(original_exception, BaseException):\n",
      "                    exception_type = type(original_exception).__name__\n",
      "                else:\n",
      "                    exception_type = \"\"\n",
      "    \n",
      "                ################################################################################\n",
      "                # Common Extra information needed for all providers\n",
      "                # We pass num retries, api_base, vertex_deployment etc to the exception here\n",
      "                ################################################################################\n",
      "                extra_information = \"\"\n",
      "                try:\n",
      "                    _api_base = litellm.get_api_base(\n",
      "                        model=model, optional_params=extra_kwargs\n",
      "                    )\n",
      "                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n",
      "                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n",
      "                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n",
      "                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n",
      "                    _model_group = _metadata.get(\"model_group\")\n",
      "                    _deployment = _metadata.get(\"deployment\")\n",
      "                    extra_information = f\"\\nModel: {model}\"\n",
      "    \n",
      "                    if (\n",
      "                        isinstance(custom_llm_provider, str)\n",
      "                        and len(custom_llm_provider) > 0\n",
      "                    ):\n",
      "                        exception_provider = (\n",
      "                            custom_llm_provider[0].upper()\n",
      "                            + custom_llm_provider[1:]\n",
      "                            + \"Exception\"\n",
      "                        )\n",
      "    \n",
      "                    if _api_base:\n",
      "                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n",
      "                    if (\n",
      "                        messages\n",
      "                        and len(messages) > 0\n",
      "                        and litellm.redact_messages_in_exceptions is False\n",
      "                    ):\n",
      "                        extra_information += f\"\\nMessages: `{messages}`\"\n",
      "    \n",
      "                    if _model_group is not None:\n",
      "                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n",
      "                    if _deployment is not None:\n",
      "                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n",
      "                    if _vertex_project is not None:\n",
      "                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n",
      "                    if _vertex_location is not None:\n",
      "                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n",
      "    \n",
      "                    # on litellm proxy add key name + team to exceptions\n",
      "                    extra_information = _add_key_name_and_team_to_alert(\n",
      "                        request_info=extra_information, metadata=_metadata\n",
      "                    )\n",
      "                except Exception:\n",
      "                    # DO NOT LET this Block raising the original exception\n",
      "                    pass\n",
      "    \n",
      "                ################################################################################\n",
      "                # End of Common Extra information Needed for all providers\n",
      "                ################################################################################\n",
      "    \n",
      "                ################################################################################\n",
      "                #################### Start of Provider Exception mapping ####################\n",
      "                ################################################################################\n",
      "    \n",
      "                if (\n",
      "                    \"Request Timeout Error\" in error_str\n",
      "                    or \"Request timed out\" in error_str\n",
      "                    or \"Timed out generating response\" in error_str\n",
      "                    or \"The read operation timed out\" in error_str\n",
      "                ):\n",
      "                    exception_mapping_worked = True\n",
      "    \n",
      "                    raise Timeout(\n",
      "                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n",
      "                        model=model,\n",
      "                        llm_provider=custom_llm_provider,\n",
      "                        litellm_debug_info=extra_information,\n",
      "                    )\n",
      "    \n",
      "                if (\n",
      "                    custom_llm_provider == \"litellm_proxy\"\n",
      "                ):  # handle special case where calling litellm proxy + exception str contains error message\n",
      "                    extract_and_raise_litellm_exception(\n",
      "                        response=getattr(original_exception, \"response\", None),\n",
      "                        error_str=error_str,\n",
      "                        model=model,\n",
      "                        custom_llm_provider=custom_llm_provider,\n",
      "                    )\n",
      "                if (\n",
      "                    custom_llm_provider == \"openai\"\n",
      "                    or custom_llm_provider == \"text-completion-openai\"\n",
      "                    or custom_llm_provider == \"custom_openai\"\n",
      "                    or custom_llm_provider in litellm.openai_compatible_providers\n",
      "                ):\n",
      "                    # custom_llm_provider is openai, make it OpenAI\n",
      "                    message = get_error_message(error_obj=original_exception)\n",
      "                    if message is None:\n",
      "                        if hasattr(original_exception, \"message\"):\n",
      "                            message = original_exception.message\n",
      "                        else:\n",
      "                            message = str(original_exception)\n",
      "    \n",
      "                    if message is not None and isinstance(\n",
      "                        message, str\n",
      "                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n",
      "                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n",
      "                        message = message.replace(\n",
      "                            \"openai.OpenAIError\",\n",
      "                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n",
      "                        )\n",
      "                    if custom_llm_provider == \"openai\":\n",
      "                        exception_provider = \"OpenAI\" + \"Exception\"\n",
      "                    else:\n",
      "                        exception_provider = (\n",
      "                            custom_llm_provider[0].upper()\n",
      "                            + custom_llm_provider[1:]\n",
      "                            + \"Exception\"\n",
      "                        )\n",
      "    \n",
      "                    if \"429\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"RateLimitError: {exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"This model's maximum context length is\" in error_str\n",
      "                        or \"string too long. Expected a string with maximum length\"\n",
      "                        in error_str\n",
      "                        or \"model's maximum context limit\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"invalid_request_error\" in error_str\n",
      "                        and \"model_not_found\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise NotFoundError(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"A timeout occurred\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        (\n",
      "                            \"invalid_request_error\" in error_str\n",
      "                            and \"content_policy_violation\" in error_str\n",
      "                        )\n",
      "                        or (\n",
      "                            \"Invalid prompt\" in error_str\n",
      "                            and \"violating our usage policy\" in error_str\n",
      "                        )\n",
      "                        or (\n",
      "                            \"request was rejected as a result of the safety system\"\n",
      "                            in error_str.lower()\n",
      "                        )\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"invalid_request_error\" in error_str\n",
      "                        and \"Incorrect API key provided\" not in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            body=getattr(original_exception, \"body\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Web server is returning an unknown error\" in error_str\n",
      "                        or \"The server had an error processing your request.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                        )\n",
      "                    elif \"Request too large\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"RateLimitError: {exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"Mistral API raised a streaming error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        _request = httpx.Request(\n",
      "                            method=\"POST\", url=\"https://api.openai.com/v1\"\n",
      "                        )\n",
      "                        raise APIError(\n",
      "                            status_code=500,\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            request=_request,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        exception_mapping_worked = True\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{exception_provider} - {message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"NotFoundError: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"Timeout Error: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                body=getattr(original_exception, \"body\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"RateLimitError: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"Timeout Error: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"APIError: {exception_provider} - {message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                request=getattr(original_exception, \"request\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                    else:\n",
      "                        # if no status code then it is an APIConnectionError: https://github.com/openai/openai-python#handling-errors\n",
      "                        # exception_mapping_worked = True\n",
      "                        raise APIConnectionError(\n",
      "                            message=f\"APIConnectionError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            request=httpx.Request(\n",
      "                                method=\"POST\", url=\"https://api.openai.com/v1/\"\n",
      "                            ),\n",
      "                        )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"anthropic\"\n",
      "                    or custom_llm_provider == \"anthropic_text\"\n",
      "                ):  # one of the anthropics\n",
      "                    if \"prompt is too long\" in error_str or \"prompt: length\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    elif \"overloaded_error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise InternalServerError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if \"Invalid API Key\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if \"content filtering policy\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if \"Client error '400 Bad Request'\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        verbose_logger.debug(\n",
      "                            f\"status_code: {original_exception.status_code}\"\n",
      "                        )\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 413\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"anthropic\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"anthropic\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"anthropic\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 500\n",
      "                            or original_exception.status_code == 529\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.InternalServerError(\n",
      "                                message=f\"AnthropicException - {error_str}. Handle with `litellm.InternalServerError`.\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.ServiceUnavailableError(\n",
      "                                message=f\"AnthropicException - {error_str}. Handle with `litellm.ServiceUnavailableError`.\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"replicate\":\n",
      "                    if \"Incorrect authentication token\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            llm_provider=\"replicate\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"input is too long\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"replicate\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif exception_type == \"ModelError\":\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"replicate\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Request was throttled\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            llm_provider=\"replicate\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 413\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"replicate\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise UnprocessableEntityError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"replicate\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"replicate\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise UnprocessableEntityError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    exception_mapping_worked = True\n",
      "                    raise APIError(\n",
      "                        status_code=500,\n",
      "                        message=f\"ReplicateException - {str(original_exception)}\",\n",
      "                        llm_provider=\"replicate\",\n",
      "                        model=model,\n",
      "                        request=httpx.Request(\n",
      "                            method=\"POST\",\n",
      "                            url=\"https://api.replicate.com/v1/deployments\",\n",
      "                        ),\n",
      "                    )\n",
      "                elif custom_llm_provider in litellm._openai_like_providers:\n",
      "                    if \"authorization denied for\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "    \n",
      "                        # Predibase returns the raw API Key in the response - this block ensures it's not returned in the exception\n",
      "                        if (\n",
      "                            error_str is not None\n",
      "                            and isinstance(error_str, str)\n",
      "                            and \"bearer\" in error_str.lower()\n",
      "                        ):\n",
      "                            # only keep the first 10 chars after the occurnence of \"bearer\"\n",
      "                            _bearer_token_start_index = error_str.lower().find(\"bearer\")\n",
      "                            error_str = error_str[: _bearer_token_start_index + 14]\n",
      "                            error_str += \"XXXXXXX\" + '\"'\n",
      "    \n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"{custom_llm_provider}Exception: Authentication Error - {error_str}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"model's maximum context limit\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"{custom_llm_provider}Exception: Context Window Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                        )\n",
      "                    elif \"token_quota_reached\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"{custom_llm_provider}Exception: Rate Limit Errror - {error_str}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The server received an invalid response from an upstream server.\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                        )\n",
      "                    elif \"model_no_support_for_function\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"{custom_llm_provider}Exception - Use 'watsonx_text' route instead. IBM WatsonX does not support `/text/chat` endpoint. - {error_str}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.InternalServerError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 401\n",
      "                            or original_exception.status_code == 403\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 422\n",
      "                            or original_exception.status_code == 424\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"bedrock\":\n",
      "                    if (\n",
      "                        \"too many tokens\" in error_str\n",
      "                        or \"expected maxLength:\" in error_str\n",
      "                        or \"Input is too long\" in error_str\n",
      "                        or \"prompt is too long\" in error_str\n",
      "                        or \"prompt: length: 1..\" in error_str\n",
      "                        or \"Too many input tokens\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"BedrockException: Context Window Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Conversation blocks and tool result blocks cannot be provided in the same turn.\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"BedrockException - {error_str}\\n. Enable 'litellm.modify_params=True' (for PROXY do: `litellm_settings::modify_params: True`) to insert a dummy assistant message and fix this error.\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Malformed input request\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"BedrockException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"A conversation must start with a user message.\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"BedrockException - {error_str}\\n. Pass in default user message via `completion(..,user_continue_message=)` or enable `litellm.modify_params=True`.\\nFor Proxy: do via `litellm_settings::modify_params: True` or user_continue_message under `litellm_params`\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Unable to locate credentials\" in error_str\n",
      "                        or \"The security token included in the request is invalid\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"BedrockException Invalid Authentication - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"AccessDeniedException\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise PermissionDeniedError(\n",
      "                            message=f\"BedrockException PermissionDeniedError - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"throttlingException\" in error_str\n",
      "                        or \"ThrottlingException\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"BedrockException: Rate Limit Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Connect timeout on endpoint URL\" in error_str\n",
      "                        or \"timed out\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"BedrockException: Timeout Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                        )\n",
      "                    elif \"Could not process image\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"BedrockException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=500,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\", url=\"https://api.openai.com/v1/\"\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"sagemaker\"\n",
      "                    or custom_llm_provider == \"sagemaker_chat\"\n",
      "                ):\n",
      "                    if \"Unable to locate credentials\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"litellm.BadRequestError: SagemakerException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"sagemaker\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Input validation error: `best_of` must be > 0 and <= 2\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=\"SagemakerException - the value of 'n' must be > 0 and <= 2 for sagemaker endpoints\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"sagemaker\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"`inputs` tokens + `max_new_tokens` must be <=\" in error_str\n",
      "                        or \"instance type with more CPU capacity or memory\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"SagemakerException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"sagemaker\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=500,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\", url=\"https://api.openai.com/v1/\"\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 422\n",
      "                            or original_exception.status_code == 424\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"vertex_ai\"\n",
      "                    or custom_llm_provider == \"vertex_ai_beta\"\n",
      "                    or custom_llm_provider == \"gemini\"\n",
      "                ):\n",
      "                    if (\n",
      "                        \"Vertex AI API has not been used in project\" in error_str\n",
      "                        or \"Unable to find your project\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"litellm.BadRequestError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            response=httpx.Response(\n",
      "                                status_code=400,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    if \"400 Request payload size exceeds\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"VertexException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"None Unknown Error.\" in error_str\n",
      "                        or \"Content has no parts.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"litellm.InternalServerError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            response=httpx.Response(\n",
      "                                status_code=500,\n",
      "                                content=str(original_exception),\n",
      "                                request=httpx.Request(method=\"completion\", url=\"https://github.com/BerriAI/litellm\"),  # type: ignore\n",
      "                            ),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"API key not valid.\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"{custom_llm_provider}Exception - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"403\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"VertexAIException BadRequestError - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            response=httpx.Response(\n",
      "                                status_code=403,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The response was blocked.\" in error_str\n",
      "                        or \"Output blocked by content filtering policy\"\n",
      "                        in error_str  # anthropic on vertex ai\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=f\"VertexAIException ContentPolicyViolationError - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=httpx.Response(\n",
      "                                status_code=400,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"429 Quota exceeded\" in error_str\n",
      "                        or \"Quota exceeded for\" in error_str\n",
      "                        or \"IndexError: list index out of range\" in error_str\n",
      "                        or \"429 Unable to submit request because the service is temporarily out of capacity.\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"litellm.RateLimitError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=httpx.Response(\n",
      "                                status_code=429,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"500 Internal Server Error\" in error_str\n",
      "                        or \"The model is overloaded.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"litellm.InternalServerError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"VertexAIException BadRequestError - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"vertex_ai\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=400,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\",\n",
      "                                        url=\"https://cloud.google.com/vertex-ai/\",\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        if original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        if original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "    \n",
      "                        if original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"litellm.RateLimitError: VertexAIException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"vertex_ai\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=429,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\",\n",
      "                                        url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.InternalServerError(\n",
      "                                message=f\"VertexAIException InternalServerError - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"vertex_ai\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=500,\n",
      "                                    content=str(original_exception),\n",
      "                                    request=httpx.Request(method=\"completion\", url=\"https://github.com/BerriAI/litellm\"),  # type: ignore\n",
      "                                ),\n",
      "                            )\n",
      "                        if original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"palm\" or custom_llm_provider == \"gemini\":\n",
      "                    if \"503 Getting metadata\" in error_str:\n",
      "                        # auth errors look like this\n",
      "                        # 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=\"GeminiException - Invalid api key\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"palm\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if (\n",
      "                        \"504 Deadline expired before operation could complete.\" in error_str\n",
      "                        or \"504 Deadline Exceeded\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"GeminiException - {original_exception.message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"palm\",\n",
      "                            exception_status_code=original_exception.status_code,\n",
      "                        )\n",
      "                    if \"400 Request payload size exceeds\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"GeminiException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"palm\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if (\n",
      "                        \"500 An internal error has occurred.\" in error_str\n",
      "                        or \"list index out of range\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise APIError(\n",
      "                            status_code=getattr(original_exception, \"status_code\", 500),\n",
      "                            message=f\"GeminiException - {original_exception.message}\",\n",
      "                            llm_provider=\"palm\",\n",
      "                            model=model,\n",
      "                            request=httpx.Response(\n",
      "                                status_code=429,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"GeminiException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"palm\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    # Dailed: Error occurred: 400 Request payload size exceeds the limit: 20000 bytes\n",
      "                elif custom_llm_provider == \"cloudflare\":\n",
      "                    if \"Authentication error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"Cloudflare Exception - {original_exception.message}\",\n",
      "                            llm_provider=\"cloudflare\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if \"must have required property\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"Cloudflare Exception - {original_exception.message}\",\n",
      "                            llm_provider=\"cloudflare\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"cohere\" or custom_llm_provider == \"cohere_chat\"\n",
      "                ):  # Cohere\n",
      "                    if (\n",
      "                        \"invalid api token\" in error_str\n",
      "                        or \"No API key provided.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"too many tokens\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"cohere\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 498\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    elif (\n",
      "                        \"CohereConnectionError\" in exception_type\n",
      "                    ):  # cohere seems to fire these errors when we load test it (1k+ messages / min)\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"invalid type:\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Unexpected server error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ServiceUnavailableError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    else:\n",
      "                        if hasattr(original_exception, \"status_code\"):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                        raise original_exception\n",
      "                elif custom_llm_provider == \"huggingface\":\n",
      "                    if \"length limit exceeded\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=error_str,\n",
      "                            model=model,\n",
      "                            llm_provider=\"huggingface\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"A valid user token is required\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=error_str,\n",
      "                            llm_provider=\"huggingface\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Rate limit reached\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=error_str,\n",
      "                            llm_provider=\"huggingface\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"huggingface\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"ai21\":\n",
      "                    if hasattr(original_exception, \"message\"):\n",
      "                        if \"Prompt has too many tokens\" in original_exception.message:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ContextWindowExceededError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        if \"Bad or missing API token.\" in original_exception.message:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                llm_provider=\"ai21\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                            )\n",
      "                        if original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                llm_provider=\"ai21\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                llm_provider=\"ai21\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"nlp_cloud\":\n",
      "                    if \"detail\" in error_str:\n",
      "                        if \"Input text length should not exceed\" in error_str:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ContextWindowExceededError(\n",
      "                                message=f\"NLPCloudException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif \"value is not a valid\" in error_str:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"NLPCloudException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=500,\n",
      "                                message=f\"NLPCloudException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                    if hasattr(\n",
      "                        original_exception, \"status_code\"\n",
      "                    ):  # https://docs.nlpcloud.com/?shell#errors\n",
      "                        if (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 406\n",
      "                            or original_exception.status_code == 413\n",
      "                            or original_exception.status_code == 422\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 401\n",
      "                            or original_exception.status_code == 403\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 522\n",
      "                            or original_exception.status_code == 524\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 429\n",
      "                            or original_exception.status_code == 402\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 500\n",
      "                            or original_exception.status_code == 503\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 504\n",
      "                            or original_exception.status_code == 520\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"together_ai\":\n",
      "                    try:\n",
      "                        error_response = json.loads(error_str)\n",
      "                    except Exception:\n",
      "                        error_response = {\"error\": error_str}\n",
      "                    if (\n",
      "                        \"error\" in error_response\n",
      "                        and \"`inputs` tokens + `max_new_tokens` must be <=\"\n",
      "                        in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error\" in error_response\n",
      "                        and \"invalid private key\" in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error\" in error_response\n",
      "                        and \"INVALID_ARGUMENT\" in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"A timeout occurred\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"TogetherAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error\" in error_response\n",
      "                        and \"API key doesn't match expected format.\"\n",
      "                        in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error_type\" in error_response\n",
      "                        and error_response[\"error_type\"] == \"validation\"\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"together_ai\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"together_ai\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                                llm_provider=\"together_ai\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 524:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                                llm_provider=\"together_ai\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                    else:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise APIError(\n",
      "                            status_code=original_exception.status_code,\n",
      "                            message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            model=model,\n",
      "                            request=original_exception.request,\n",
      "                        )\n",
      "                elif custom_llm_provider == \"aleph_alpha\":\n",
      "                    if (\n",
      "                        \"This is longer than the model's maximum context length\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                            llm_provider=\"aleph_alpha\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"InvalidToken\" in error_str or \"No token provided\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                            llm_provider=\"aleph_alpha\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        verbose_logger.debug(\n",
      "                            f\"status code: {original_exception.status_code}\"\n",
      "                        )\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        raise original_exception\n",
      "                    raise original_exception\n",
      "                elif (\n",
      "                    custom_llm_provider == \"ollama\" or custom_llm_provider == \"ollama_chat\"\n",
      "                ):\n",
      "                    if isinstance(original_exception, dict):\n",
      "                        error_str = original_exception.get(\"error\", \"\")\n",
      "                    else:\n",
      "                        error_str = str(original_exception)\n",
      "                    if \"no such file or directory\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"OllamaException: Invalid Model/Model not loaded - {original_exception}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"ollama\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Failed to establish a new connection\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ServiceUnavailableError(\n",
      "                            message=f\"OllamaException: {original_exception}\",\n",
      "                            llm_provider=\"ollama\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Invalid response object from API\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"OllamaException: {original_exception}\",\n",
      "                            llm_provider=\"ollama\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Read timed out\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"OllamaException: {original_exception}\",\n",
      "                            llm_provider=\"ollama\",\n",
      "                            model=model,\n",
      "                        )\n",
      "                elif custom_llm_provider == \"vllm\":\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 0:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIConnectionError(\n",
      "                                message=f\"VLLMException - {original_exception.message}\",\n",
      "                                llm_provider=\"vllm\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"azure\" or custom_llm_provider == \"azure_text\":\n",
      "                    message = get_error_message(error_obj=original_exception)\n",
      "                    if message is None:\n",
      "                        if hasattr(original_exception, \"message\"):\n",
      "                            message = original_exception.message\n",
      "                        else:\n",
      "                            message = str(original_exception)\n",
      "    \n",
      "                    if \"Internal server error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"AzureException Internal server error - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"This model's maximum context length is\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"AzureException ContextWindowExceededError - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"DeploymentNotFound\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise NotFoundError(\n",
      "                            message=f\"AzureException NotFoundError - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        (\n",
      "                            \"invalid_request_error\" in error_str\n",
      "                            and \"content_policy_violation\" in error_str\n",
      "                        )\n",
      "                        or (\n",
      "                            \"The response was filtered due to the prompt triggering Azure OpenAI's content management\"\n",
      "                            in error_str\n",
      "                        )\n",
      "                        or \"Your task failed as a result of our safety system\" in error_str\n",
      "                        or \"The model produced invalid content\" in error_str\n",
      "                        or \"content_filter_policy\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=f\"litellm.ContentPolicyViolationError: AzureException - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"invalid_request_error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"AzureException BadRequestError - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            body=getattr(original_exception, \"body\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The api_key client option must be set either by passing api_key to the client or by setting\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"{exception_provider} AuthenticationError - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Connection error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise APIConnectionError(\n",
      "                            message=f\"{exception_provider} APIConnectionError - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        exception_mapping_worked = True\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AzureException - {message}\",\n",
      "                                llm_provider=\"azure\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                body=getattr(original_exception, \"body\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AzureException AuthenticationError - {message}\",\n",
      "                                llm_provider=\"azure\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AzureException Timeout - {message}\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                llm_provider=\"azure\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AzureException BadRequestError - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AzureException RateLimitError - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"AzureException ServiceUnavailableError - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AzureException Timeout - {message}\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                llm_provider=\"azure\",\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      ">                           raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"AzureException APIError - {message}\",\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                model=model,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\", url=\"https://openai.com/\"\n",
      "                                ),\n",
      "                            )\n",
      "\u001b[1m\u001b[31mE                           litellm.exceptions.APIError: litellm.APIError: AzureException APIError - Resource not found\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m:2077: APIError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\u001b[31m\u001b[1m_________________________________ test_wheel_3 _________________________________\u001b[0m\n",
      "\n",
      "self = <litellm.llms.azure.azure.AzureChatCompletion object at 0x10db8dd30>\n",
      "model = 'gpt-4-turbo'\n",
      "messages = [{'content': '<context>Putin started the war, Ukraine will not surrender and will finally win!</context>', 'role': 'us...on\\'t mention any special denotations such as \"T\", \"T1\", \"T+\", \"A-\", \"Ac\", \"Re\", etc.\\n</formatting>', 'role': 'user'}]\n",
      "model_response = ModelResponse(id='chatcmpl-b5d69443-dfae-4243-8e14-2f4c1cebda0b', created=1748418121, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "api_key = '830bfa7d0fd34a5dae45e1a1d8f879de'\n",
      "api_base = 'https://openai-01-dialexityswedencentral.openai.azure.com/'\n",
      "api_version = '2024-10-21DIALEXITY_DEFAULT_MODEL=gpt-4', api_type = 'azure'\n",
      "azure_ad_token = None, azure_ad_token_provider = None, dynamic_params = False\n",
      "print_verbose = <function print_verbose at 0x10dbc3e20>, timeout = 600.0\n",
      "logging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ed49a90>\n",
      "optional_params = {'extra_body': {}, 'stream': False, 'tool_choice': 'required', 'tools': [{'function': {'description': 'Correctly forma..., 'properties': {'dialectical_components': {...}}, 'required': ['dialectical_components'], ...}}, 'type': 'function'}]}\n",
      "litellm_params = {'acompletion': False, 'aembedding': None, 'api_base': None, 'api_key': None, ...}\n",
      "logger_fn = None, acompletion = False, headers = None, client = None\n",
      "\n",
      "    def completion(  # noqa: PLR0915\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: list,\n",
      "        model_response: ModelResponse,\n",
      "        api_key: str,\n",
      "        api_base: str,\n",
      "        api_version: str,\n",
      "        api_type: str,\n",
      "        azure_ad_token: str,\n",
      "        azure_ad_token_provider: Callable,\n",
      "        dynamic_params: bool,\n",
      "        print_verbose: Callable,\n",
      "        timeout: Union[float, httpx.Timeout],\n",
      "        logging_obj: LiteLLMLoggingObj,\n",
      "        optional_params,\n",
      "        litellm_params,\n",
      "        logger_fn,\n",
      "        acompletion: bool = False,\n",
      "        headers: Optional[dict] = None,\n",
      "        client=None,\n",
      "    ):\n",
      "        if headers:\n",
      "            optional_params[\"extra_headers\"] = headers\n",
      "        try:\n",
      "            if model is None or messages is None:\n",
      "                raise AzureOpenAIError(\n",
      "                    status_code=422, message=\"Missing model or messages\"\n",
      "                )\n",
      "    \n",
      "            max_retries = optional_params.pop(\"max_retries\", None)\n",
      "            if max_retries is None:\n",
      "                max_retries = DEFAULT_MAX_RETRIES\n",
      "            json_mode: Optional[bool] = optional_params.pop(\"json_mode\", False)\n",
      "    \n",
      "            ### CHECK IF CLOUDFLARE AI GATEWAY ###\n",
      "            ### if so - set the model as part of the base url\n",
      "            if \"gateway.ai.cloudflare.com\" in api_base:\n",
      "                client = self._init_azure_client_for_cloudflare_ai_gateway(\n",
      "                    api_base=api_base,\n",
      "                    model=model,\n",
      "                    api_version=api_version,\n",
      "                    max_retries=max_retries,\n",
      "                    timeout=timeout,\n",
      "                    api_key=api_key,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    acompletion=acompletion,\n",
      "                    client=client,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "    \n",
      "                data = {\"model\": None, \"messages\": messages, **optional_params}\n",
      "            else:\n",
      "                data = litellm.AzureOpenAIConfig().transform_request(\n",
      "                    model=model,\n",
      "                    messages=messages,\n",
      "                    optional_params=optional_params,\n",
      "                    litellm_params=litellm_params,\n",
      "                    headers=headers or {},\n",
      "                )\n",
      "    \n",
      "            if acompletion is True:\n",
      "                if optional_params.get(\"stream\", False):\n",
      "                    return self.async_streaming(\n",
      "                        logging_obj=logging_obj,\n",
      "                        api_base=api_base,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        data=data,\n",
      "                        model=model,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        max_retries=max_retries,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "                else:\n",
      "                    return self.acompletion(\n",
      "                        api_base=api_base,\n",
      "                        data=data,\n",
      "                        model_response=model_response,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        model=model,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        logging_obj=logging_obj,\n",
      "                        max_retries=max_retries,\n",
      "                        convert_tool_call_to_json_mode=json_mode,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "            elif \"stream\" in optional_params and optional_params[\"stream\"] is True:\n",
      "                return self.streaming(\n",
      "                    logging_obj=logging_obj,\n",
      "                    api_base=api_base,\n",
      "                    dynamic_params=dynamic_params,\n",
      "                    data=data,\n",
      "                    model=model,\n",
      "                    api_key=api_key,\n",
      "                    api_version=api_version,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    timeout=timeout,\n",
      "                    client=client,\n",
      "                    max_retries=max_retries,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "            else:\n",
      "                ## LOGGING\n",
      "                logging_obj.pre_call(\n",
      "                    input=messages,\n",
      "                    api_key=api_key,\n",
      "                    additional_args={\n",
      "                        \"headers\": {\n",
      "                            \"api_key\": api_key,\n",
      "                            \"azure_ad_token\": azure_ad_token,\n",
      "                        },\n",
      "                        \"api_version\": api_version,\n",
      "                        \"api_base\": api_base,\n",
      "                        \"complete_input_dict\": data,\n",
      "                    },\n",
      "                )\n",
      "                if not isinstance(max_retries, int):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=422, message=\"max retries must be an int\"\n",
      "                    )\n",
      "                # init AzureOpenAI Client\n",
      "                azure_client = self.get_azure_openai_client(\n",
      "                    api_version=api_version,\n",
      "                    api_base=api_base,\n",
      "                    api_key=api_key,\n",
      "                    model=model,\n",
      "                    client=client,\n",
      "                    _is_async=False,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "                if not isinstance(azure_client, AzureOpenAI):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=500,\n",
      "                        message=\"azure_client is not an instance of AzureOpenAI\",\n",
      "                    )\n",
      "    \n",
      ">               headers, response = self.make_sync_azure_openai_chat_completion_request(\n",
      "                    azure_client=azure_client, data=data, timeout=timeout\n",
      "                )\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:328: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:148: in make_sync_azure_openai_chat_completion_request\n",
      "    raise e\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:140: in make_sync_azure_openai_chat_completion_request\n",
      "    raw_response = azure_client.chat.completions.with_raw_response.create(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_legacy_response.py\u001b[0m:364: in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_utils/_utils.py\u001b[0m:287: in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\u001b[0m:925: in create\n",
      "    return self._post(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_base_client.py\u001b[0m:1239: in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <openai.lib.azure.AzureOpenAI object at 0x10ed4bd90>\n",
      "cast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\n",
      "options = FinalRequestOptions(method='post', url='/deployments/gpt-4-turbo/chat/completions', params={}, headers={'X-Stainless-R...ctical Components'}}, 'required': ['dialectical_components'], 'type': 'object'}}, 'type': 'function'}]}, extra_json={})\n",
      "\n",
      "    def request(\n",
      "        self,\n",
      "        cast_to: Type[ResponseT],\n",
      "        options: FinalRequestOptions,\n",
      "        *,\n",
      "        stream: bool = False,\n",
      "        stream_cls: type[_StreamT] | None = None,\n",
      "    ) -> ResponseT | _StreamT:\n",
      "        cast_to = self._maybe_override_cast_to(cast_to, options)\n",
      "    \n",
      "        # create a copy of the options we were given so that if the\n",
      "        # options are mutated later & we then retry, the retries are\n",
      "        # given the original options\n",
      "        input_options = model_copy(options)\n",
      "        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n",
      "            # ensure the idempotency key is reused between requests\n",
      "            input_options.idempotency_key = self._idempotency_key()\n",
      "    \n",
      "        response: httpx.Response | None = None\n",
      "        max_retries = input_options.get_max_retries(self.max_retries)\n",
      "    \n",
      "        retries_taken = 0\n",
      "        for retries_taken in range(max_retries + 1):\n",
      "            options = model_copy(input_options)\n",
      "            options = self._prepare_options(options)\n",
      "    \n",
      "            remaining_retries = max_retries - retries_taken\n",
      "            request = self._build_request(options, retries_taken=retries_taken)\n",
      "            self._prepare_request(request)\n",
      "    \n",
      "            kwargs: HttpxSendArgs = {}\n",
      "            if self.custom_auth is not None:\n",
      "                kwargs[\"auth\"] = self.custom_auth\n",
      "    \n",
      "            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n",
      "    \n",
      "            response = None\n",
      "            try:\n",
      "                response = self._client.send(\n",
      "                    request,\n",
      "                    stream=stream or self._should_stream_response_body(request=request),\n",
      "                    **kwargs,\n",
      "                )\n",
      "            except httpx.TimeoutException as err:\n",
      "                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n",
      "    \n",
      "                if remaining_retries > 0:\n",
      "                    self._sleep_for_retry(\n",
      "                        retries_taken=retries_taken,\n",
      "                        max_retries=max_retries,\n",
      "                        options=input_options,\n",
      "                        response=None,\n",
      "                    )\n",
      "                    continue\n",
      "    \n",
      "                log.debug(\"Raising timeout error\")\n",
      "                raise APITimeoutError(request=request) from err\n",
      "            except Exception as err:\n",
      "                log.debug(\"Encountered Exception\", exc_info=True)\n",
      "    \n",
      "                if remaining_retries > 0:\n",
      "                    self._sleep_for_retry(\n",
      "                        retries_taken=retries_taken,\n",
      "                        max_retries=max_retries,\n",
      "                        options=input_options,\n",
      "                        response=None,\n",
      "                    )\n",
      "                    continue\n",
      "    \n",
      "                log.debug(\"Raising connection error\")\n",
      "                raise APIConnectionError(request=request) from err\n",
      "    \n",
      "            log.debug(\n",
      "                'HTTP Response: %s %s \"%i %s\" %s',\n",
      "                request.method,\n",
      "                request.url,\n",
      "                response.status_code,\n",
      "                response.reason_phrase,\n",
      "                response.headers,\n",
      "            )\n",
      "            log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n",
      "    \n",
      "            try:\n",
      "                response.raise_for_status()\n",
      "            except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n",
      "                log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n",
      "    \n",
      "                if remaining_retries > 0 and self._should_retry(err.response):\n",
      "                    err.response.close()\n",
      "                    self._sleep_for_retry(\n",
      "                        retries_taken=retries_taken,\n",
      "                        max_retries=max_retries,\n",
      "                        options=input_options,\n",
      "                        response=response,\n",
      "                    )\n",
      "                    continue\n",
      "    \n",
      "                # If the response is streamed then we need to explicitly read the response\n",
      "                # to completion before attempting to access the response text.\n",
      "                if not err.response.is_closed:\n",
      "                    err.response.read()\n",
      "    \n",
      "                log.debug(\"Re-raising status error\")\n",
      ">               raise self._make_status_error_from_response(err.response) from None\n",
      "\u001b[1m\u001b[31mE               openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_base_client.py\u001b[0m:1034: NotFoundError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "model = 'gpt-4-turbo'\n",
      "messages = [{'content': '<context>Putin started the war, Ukraine will not surrender and will finally win!</context>', 'role': 'us...on\\'t mention any special denotations such as \"T\", \"T1\", \"T+\", \"A-\", \"Ac\", \"Re\", etc.\\n</formatting>', 'role': 'user'}]\n",
      "timeout = 600.0, temperature = None, top_p = None, n = None, stream = False\n",
      "stream_options = None, stop = None, max_completion_tokens = None\n",
      "max_tokens = None, modalities = None, prediction = None, audio = None\n",
      "presence_penalty = None, frequency_penalty = None, logit_bias = None\n",
      "user = None, reasoning_effort = None, response_format = None, seed = None\n",
      "tools = [{'function': {'description': 'Correctly formatted and typed parameters extracted from the completion. Must include re...itle': 'Dialectical Components', 'type': 'array'}}, 'required': ['dialectical_components'], ...}}, 'type': 'function'}]\n",
      "tool_choice = 'required', logprobs = None, top_logprobs = None\n",
      "parallel_tool_calls = None, web_search_options = None, deployment_id = None\n",
      "extra_headers = None, functions = None, function_call = None, base_url = None\n",
      "api_version = '2024-10-21DIALEXITY_DEFAULT_MODEL=gpt-4'\n",
      "api_key = '830bfa7d0fd34a5dae45e1a1d8f879de', model_list = None, thinking = None\n",
      "kwargs = {'litellm_call_id': '2cbc83fa-dcab-4e29-8461-81d0187ee4c5', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ed49a90>}\n",
      "args = {'api_key': None, 'api_version': None, 'audio': None, 'base_url': None, ...}\n",
      "api_base = 'https://openai-01-dialexityswedencentral.openai.azure.com/'\n",
      "mock_response = None, mock_tool_calls = None, mock_timeout = None\n",
      "force_timeout = 600, logger_fn = None\n",
      "\n",
      "    @client\n",
      "    def completion(  # type: ignore # noqa: PLR0915\n",
      "        model: str,\n",
      "        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n",
      "        messages: List = [],\n",
      "        timeout: Optional[Union[float, str, httpx.Timeout]] = None,\n",
      "        temperature: Optional[float] = None,\n",
      "        top_p: Optional[float] = None,\n",
      "        n: Optional[int] = None,\n",
      "        stream: Optional[bool] = None,\n",
      "        stream_options: Optional[dict] = None,\n",
      "        stop=None,\n",
      "        max_completion_tokens: Optional[int] = None,\n",
      "        max_tokens: Optional[int] = None,\n",
      "        modalities: Optional[List[ChatCompletionModality]] = None,\n",
      "        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n",
      "        audio: Optional[ChatCompletionAudioParam] = None,\n",
      "        presence_penalty: Optional[float] = None,\n",
      "        frequency_penalty: Optional[float] = None,\n",
      "        logit_bias: Optional[dict] = None,\n",
      "        user: Optional[str] = None,\n",
      "        # openai v1.0+ new params\n",
      "        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n",
      "        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n",
      "        seed: Optional[int] = None,\n",
      "        tools: Optional[List] = None,\n",
      "        tool_choice: Optional[Union[str, dict]] = None,\n",
      "        logprobs: Optional[bool] = None,\n",
      "        top_logprobs: Optional[int] = None,\n",
      "        parallel_tool_calls: Optional[bool] = None,\n",
      "        web_search_options: Optional[OpenAIWebSearchOptions] = None,\n",
      "        deployment_id=None,\n",
      "        extra_headers: Optional[dict] = None,\n",
      "        # soon to be deprecated params by OpenAI\n",
      "        functions: Optional[List] = None,\n",
      "        function_call: Optional[str] = None,\n",
      "        # set api_base, api_version, api_key\n",
      "        base_url: Optional[str] = None,\n",
      "        api_version: Optional[str] = None,\n",
      "        api_key: Optional[str] = None,\n",
      "        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n",
      "        # Optional liteLLM function params\n",
      "        thinking: Optional[AnthropicThinkingParam] = None,\n",
      "        **kwargs,\n",
      "    ) -> Union[ModelResponse, CustomStreamWrapper]:\n",
      "        \"\"\"\n",
      "        Perform a completion() using any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n",
      "        Parameters:\n",
      "            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n",
      "            messages (List): A list of message objects representing the conversation context (default is an empty list).\n",
      "    \n",
      "            OPTIONAL PARAMS\n",
      "            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n",
      "            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n",
      "            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n",
      "            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n",
      "            n (int, optional): The number of completions to generate (default is 1).\n",
      "            stream (bool, optional): If True, return a streaming response (default is False).\n",
      "            stream_options (dict, optional): A dictionary containing options for the streaming response. Only set this when you set stream: true.\n",
      "            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n",
      "            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n",
      "            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n",
      "            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request.. You can use `[\"text\", \"audio\"]`\n",
      "            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n",
      "            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n",
      "            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n",
      "            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n",
      "            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n",
      "            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n",
      "            logprobs (bool, optional): Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message\n",
      "            top_logprobs (int, optional): An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.\n",
      "            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n",
      "            api_base (str, optional): Base URL for the API (default is None).\n",
      "            api_version (str, optional): API version (default is None).\n",
      "            api_key (str, optional): API key (default is None).\n",
      "            model_list (list, optional): List of api base, version, keys\n",
      "            extra_headers (dict, optional): Additional headers to include in the request.\n",
      "    \n",
      "            LITELLM Specific Params\n",
      "            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n",
      "            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n",
      "            max_retries (int, optional): The number of retries to attempt (default is 0).\n",
      "        Returns:\n",
      "            ModelResponse: A response object containing the generated completion and associated metadata.\n",
      "    \n",
      "        Note:\n",
      "            - This function is used to perform completions() using the specified language model.\n",
      "            - It supports various optional parameters for customizing the completion behavior.\n",
      "            - If 'mock_response' is provided, a mock completion response is returned for testing or debugging.\n",
      "        \"\"\"\n",
      "        ### VALIDATE Request ###\n",
      "        if model is None:\n",
      "            raise ValueError(\"model param not passed in.\")\n",
      "        # validate messages\n",
      "        messages = validate_and_fix_openai_messages(messages=messages)\n",
      "        # validate tool_choice\n",
      "        tool_choice = validate_chat_completion_tool_choice(tool_choice=tool_choice)\n",
      "        ######### unpacking kwargs #####################\n",
      "        args = locals()\n",
      "        api_base = kwargs.get(\"api_base\", None)\n",
      "        mock_response = kwargs.get(\"mock_response\", None)\n",
      "        mock_tool_calls = kwargs.get(\"mock_tool_calls\", None)\n",
      "        mock_timeout = cast(Optional[bool], kwargs.get(\"mock_timeout\", None))\n",
      "        force_timeout = kwargs.get(\"force_timeout\", 600)  ## deprecated\n",
      "        logger_fn = kwargs.get(\"logger_fn\", None)\n",
      "        verbose = kwargs.get(\"verbose\", False)\n",
      "        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n",
      "        litellm_logging_obj = kwargs.get(\"litellm_logging_obj\", None)\n",
      "        id = kwargs.get(\"id\", None)\n",
      "        metadata = kwargs.get(\"metadata\", None)\n",
      "        model_info = kwargs.get(\"model_info\", None)\n",
      "        proxy_server_request = kwargs.get(\"proxy_server_request\", None)\n",
      "        fallbacks = kwargs.get(\"fallbacks\", None)\n",
      "        provider_specific_header = cast(\n",
      "            Optional[ProviderSpecificHeader], kwargs.get(\"provider_specific_header\", None)\n",
      "        )\n",
      "        headers = kwargs.get(\"headers\", None) or extra_headers\n",
      "    \n",
      "        ensure_alternating_roles: Optional[bool] = kwargs.get(\n",
      "            \"ensure_alternating_roles\", None\n",
      "        )\n",
      "        user_continue_message: Optional[ChatCompletionUserMessage] = kwargs.get(\n",
      "            \"user_continue_message\", None\n",
      "        )\n",
      "        assistant_continue_message: Optional[ChatCompletionAssistantMessage] = kwargs.get(\n",
      "            \"assistant_continue_message\", None\n",
      "        )\n",
      "        if headers is None:\n",
      "            headers = {}\n",
      "        if extra_headers is not None:\n",
      "            headers.update(extra_headers)\n",
      "        num_retries = kwargs.get(\n",
      "            \"num_retries\", None\n",
      "        )  ## alt. param for 'max_retries'. Use this to pass retries w/ instructor.\n",
      "        max_retries = kwargs.get(\"max_retries\", None)\n",
      "        cooldown_time = kwargs.get(\"cooldown_time\", None)\n",
      "        context_window_fallback_dict = kwargs.get(\"context_window_fallback_dict\", None)\n",
      "        organization = kwargs.get(\"organization\", None)\n",
      "        ### VERIFY SSL ###\n",
      "        ssl_verify = kwargs.get(\"ssl_verify\", None)\n",
      "        ### CUSTOM MODEL COST ###\n",
      "        input_cost_per_token = kwargs.get(\"input_cost_per_token\", None)\n",
      "        output_cost_per_token = kwargs.get(\"output_cost_per_token\", None)\n",
      "        input_cost_per_second = kwargs.get(\"input_cost_per_second\", None)\n",
      "        output_cost_per_second = kwargs.get(\"output_cost_per_second\", None)\n",
      "        ### CUSTOM PROMPT TEMPLATE ###\n",
      "        initial_prompt_value = kwargs.get(\"initial_prompt_value\", None)\n",
      "        roles = kwargs.get(\"roles\", None)\n",
      "        final_prompt_value = kwargs.get(\"final_prompt_value\", None)\n",
      "        bos_token = kwargs.get(\"bos_token\", None)\n",
      "        eos_token = kwargs.get(\"eos_token\", None)\n",
      "        preset_cache_key = kwargs.get(\"preset_cache_key\", None)\n",
      "        hf_model_name = kwargs.get(\"hf_model_name\", None)\n",
      "        supports_system_message = kwargs.get(\"supports_system_message\", None)\n",
      "        base_model = kwargs.get(\"base_model\", None)\n",
      "        ### DISABLE FLAGS ###\n",
      "        disable_add_transform_inline_image_block = kwargs.get(\n",
      "            \"disable_add_transform_inline_image_block\", None\n",
      "        )\n",
      "        ### TEXT COMPLETION CALLS ###\n",
      "        text_completion = kwargs.get(\"text_completion\", False)\n",
      "        atext_completion = kwargs.get(\"atext_completion\", False)\n",
      "        ### ASYNC CALLS ###\n",
      "        acompletion = kwargs.get(\"acompletion\", False)\n",
      "        client = kwargs.get(\"client\", None)\n",
      "        ### Admin Controls ###\n",
      "        no_log = kwargs.get(\"no-log\", False)\n",
      "        ### PROMPT MANAGEMENT ###\n",
      "        prompt_id = cast(Optional[str], kwargs.get(\"prompt_id\", None))\n",
      "        prompt_variables = cast(Optional[dict], kwargs.get(\"prompt_variables\", None))\n",
      "        ### COPY MESSAGES ### - related issue https://github.com/BerriAI/litellm/discussions/4489\n",
      "        messages = get_completion_messages(\n",
      "            messages=messages,\n",
      "            ensure_alternating_roles=ensure_alternating_roles or False,\n",
      "            user_continue_message=user_continue_message,\n",
      "            assistant_continue_message=assistant_continue_message,\n",
      "        )\n",
      "        ######## end of unpacking kwargs ###########\n",
      "        standard_openai_params = get_standard_openai_params(params=args)\n",
      "        non_default_params = get_non_default_completion_params(kwargs=kwargs)\n",
      "        litellm_params = {}  # used to prevent unbound var errors\n",
      "        ## PROMPT MANAGEMENT HOOKS ##\n",
      "        if isinstance(litellm_logging_obj, LiteLLMLoggingObj) and (\n",
      "            litellm_logging_obj.should_run_prompt_management_hooks(\n",
      "                prompt_id=prompt_id, non_default_params=non_default_params\n",
      "            )\n",
      "        ):\n",
      "            (\n",
      "                model,\n",
      "                messages,\n",
      "                optional_params,\n",
      "            ) = litellm_logging_obj.get_chat_completion_prompt(\n",
      "                model=model,\n",
      "                messages=messages,\n",
      "                non_default_params=non_default_params,\n",
      "                prompt_id=prompt_id,\n",
      "                prompt_variables=prompt_variables,\n",
      "                prompt_label=kwargs.get(\"prompt_label\", None),\n",
      "            )\n",
      "    \n",
      "        try:\n",
      "            if base_url is not None:\n",
      "                api_base = base_url\n",
      "            if num_retries is not None:\n",
      "                max_retries = num_retries\n",
      "            logging = litellm_logging_obj\n",
      "            fallbacks = fallbacks or litellm.model_fallbacks\n",
      "            if fallbacks is not None:\n",
      "                return completion_with_fallbacks(**args)\n",
      "            if model_list is not None:\n",
      "                deployments = [\n",
      "                    m[\"litellm_params\"] for m in model_list if m[\"model_name\"] == model\n",
      "                ]\n",
      "                return litellm.batch_completion_models(deployments=deployments, **args)\n",
      "            if litellm.model_alias_map and model in litellm.model_alias_map:\n",
      "                model = litellm.model_alias_map[\n",
      "                    model\n",
      "                ]  # update the model to the actual value if an alias has been passed in\n",
      "            model_response = ModelResponse()\n",
      "            setattr(model_response, \"usage\", litellm.Usage())\n",
      "            if (\n",
      "                kwargs.get(\"azure\", False) is True\n",
      "            ):  # don't remove flag check, to remain backwards compatible for repos like Codium\n",
      "                custom_llm_provider = \"azure\"\n",
      "            if deployment_id is not None:  # azure llms\n",
      "                model = deployment_id\n",
      "                custom_llm_provider = \"azure\"\n",
      "            model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n",
      "                model=model,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "                api_base=api_base,\n",
      "                api_key=api_key,\n",
      "            )\n",
      "    \n",
      "            if (\n",
      "                provider_specific_header is not None\n",
      "                and provider_specific_header[\"custom_llm_provider\"] == custom_llm_provider\n",
      "            ):\n",
      "                headers.update(provider_specific_header[\"extra_headers\"])\n",
      "    \n",
      "            if model_response is not None and hasattr(model_response, \"_hidden_params\"):\n",
      "                model_response._hidden_params[\"custom_llm_provider\"] = custom_llm_provider\n",
      "                model_response._hidden_params[\"region_name\"] = kwargs.get(\n",
      "                    \"aws_region_name\", None\n",
      "                )  # support region-based pricing for bedrock\n",
      "    \n",
      "            ### TIMEOUT LOGIC ###\n",
      "            timeout = timeout or kwargs.get(\"request_timeout\", 600) or 600\n",
      "            # set timeout for 10 minutes by default\n",
      "            if isinstance(timeout, httpx.Timeout) and not supports_httpx_timeout(\n",
      "                custom_llm_provider\n",
      "            ):\n",
      "                timeout = timeout.read or 600  # default 10 min timeout\n",
      "            elif not isinstance(timeout, httpx.Timeout):\n",
      "                timeout = float(timeout)  # type: ignore\n",
      "    \n",
      "            ### REGISTER CUSTOM MODEL PRICING -- IF GIVEN ###\n",
      "            if input_cost_per_token is not None and output_cost_per_token is not None:\n",
      "                litellm.register_model(\n",
      "                    {\n",
      "                        f\"{custom_llm_provider}/{model}\": {\n",
      "                            \"input_cost_per_token\": input_cost_per_token,\n",
      "                            \"output_cost_per_token\": output_cost_per_token,\n",
      "                            \"litellm_provider\": custom_llm_provider,\n",
      "                        }\n",
      "                    }\n",
      "                )\n",
      "            elif (\n",
      "                input_cost_per_second is not None\n",
      "            ):  # time based pricing just needs cost in place\n",
      "                output_cost_per_second = output_cost_per_second\n",
      "                litellm.register_model(\n",
      "                    {\n",
      "                        f\"{custom_llm_provider}/{model}\": {\n",
      "                            \"input_cost_per_second\": input_cost_per_second,\n",
      "                            \"output_cost_per_second\": output_cost_per_second,\n",
      "                            \"litellm_provider\": custom_llm_provider,\n",
      "                        }\n",
      "                    }\n",
      "                )\n",
      "            ### BUILD CUSTOM PROMPT TEMPLATE -- IF GIVEN ###\n",
      "            custom_prompt_dict = {}  # type: ignore\n",
      "            if (\n",
      "                initial_prompt_value\n",
      "                or roles\n",
      "                or final_prompt_value\n",
      "                or bos_token\n",
      "                or eos_token\n",
      "            ):\n",
      "                custom_prompt_dict = {model: {}}\n",
      "                if initial_prompt_value:\n",
      "                    custom_prompt_dict[model][\"initial_prompt_value\"] = initial_prompt_value\n",
      "                if roles:\n",
      "                    custom_prompt_dict[model][\"roles\"] = roles\n",
      "                if final_prompt_value:\n",
      "                    custom_prompt_dict[model][\"final_prompt_value\"] = final_prompt_value\n",
      "                if bos_token:\n",
      "                    custom_prompt_dict[model][\"bos_token\"] = bos_token\n",
      "                if eos_token:\n",
      "                    custom_prompt_dict[model][\"eos_token\"] = eos_token\n",
      "    \n",
      "            if kwargs.get(\"model_file_id_mapping\"):\n",
      "                messages = update_messages_with_model_file_ids(\n",
      "                    messages=messages,\n",
      "                    model_id=kwargs.get(\"model_info\", {}).get(\"id\", None),\n",
      "                    model_file_id_mapping=cast(\n",
      "                        Dict[str, Dict[str, str]], kwargs.get(\"model_file_id_mapping\")\n",
      "                    ),\n",
      "                )\n",
      "    \n",
      "            provider_config: Optional[BaseConfig] = None\n",
      "            if custom_llm_provider is not None and custom_llm_provider in [\n",
      "                provider.value for provider in LlmProviders\n",
      "            ]:\n",
      "                provider_config = ProviderConfigManager.get_provider_chat_config(\n",
      "                    model=model, provider=LlmProviders(custom_llm_provider)\n",
      "                )\n",
      "    \n",
      "            if provider_config is not None:\n",
      "                messages = provider_config.translate_developer_role_to_system_role(\n",
      "                    messages=messages\n",
      "                )\n",
      "    \n",
      "            if (\n",
      "                supports_system_message is not None\n",
      "                and isinstance(supports_system_message, bool)\n",
      "                and supports_system_message is False\n",
      "            ):\n",
      "                messages = map_system_message_pt(messages=messages)\n",
      "    \n",
      "            if dynamic_api_key is not None:\n",
      "                api_key = dynamic_api_key\n",
      "            # check if user passed in any of the OpenAI optional params\n",
      "            optional_param_args = {\n",
      "                \"functions\": functions,\n",
      "                \"function_call\": function_call,\n",
      "                \"temperature\": temperature,\n",
      "                \"top_p\": top_p,\n",
      "                \"n\": n,\n",
      "                \"stream\": stream,\n",
      "                \"stream_options\": stream_options,\n",
      "                \"stop\": stop,\n",
      "                \"max_tokens\": max_tokens,\n",
      "                \"max_completion_tokens\": max_completion_tokens,\n",
      "                \"modalities\": modalities,\n",
      "                \"prediction\": prediction,\n",
      "                \"audio\": audio,\n",
      "                \"presence_penalty\": presence_penalty,\n",
      "                \"frequency_penalty\": frequency_penalty,\n",
      "                \"logit_bias\": logit_bias,\n",
      "                \"user\": user,\n",
      "                # params to identify the model\n",
      "                \"model\": model,\n",
      "                \"custom_llm_provider\": custom_llm_provider,\n",
      "                \"response_format\": response_format,\n",
      "                \"seed\": seed,\n",
      "                \"tools\": tools,\n",
      "                \"tool_choice\": tool_choice,\n",
      "                \"max_retries\": max_retries,\n",
      "                \"logprobs\": logprobs,\n",
      "                \"top_logprobs\": top_logprobs,\n",
      "                \"api_version\": api_version,\n",
      "                \"parallel_tool_calls\": parallel_tool_calls,\n",
      "                \"messages\": messages,\n",
      "                \"reasoning_effort\": reasoning_effort,\n",
      "                \"thinking\": thinking,\n",
      "                \"web_search_options\": web_search_options,\n",
      "                \"allowed_openai_params\": kwargs.get(\"allowed_openai_params\"),\n",
      "            }\n",
      "            optional_params = get_optional_params(\n",
      "                **optional_param_args, **non_default_params\n",
      "            )\n",
      "            processed_non_default_params = pre_process_non_default_params(\n",
      "                model=model,\n",
      "                passed_params=optional_param_args,\n",
      "                special_params=non_default_params,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "                additional_drop_params=kwargs.get(\"additional_drop_params\"),\n",
      "            )\n",
      "            processed_non_default_params = add_provider_specific_params_to_optional_params(\n",
      "                optional_params=processed_non_default_params,\n",
      "                passed_params=non_default_params,\n",
      "            )\n",
      "    \n",
      "            if litellm.add_function_to_prompt and optional_params.get(\n",
      "                \"functions_unsupported_model\", None\n",
      "            ):  # if user opts to add it to prompt, when API doesn't support function calling\n",
      "                functions_unsupported_model = optional_params.pop(\n",
      "                    \"functions_unsupported_model\"\n",
      "                )\n",
      "                messages = function_call_prompt(\n",
      "                    messages=messages, functions=functions_unsupported_model\n",
      "                )\n",
      "    \n",
      "            # For logging - save the values of the litellm-specific params passed in\n",
      "            litellm_params = get_litellm_params(\n",
      "                acompletion=acompletion,\n",
      "                api_key=api_key,\n",
      "                force_timeout=force_timeout,\n",
      "                logger_fn=logger_fn,\n",
      "                verbose=verbose,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "                api_base=api_base,\n",
      "                litellm_call_id=kwargs.get(\"litellm_call_id\", None),\n",
      "                model_alias_map=litellm.model_alias_map,\n",
      "                completion_call_id=id,\n",
      "                metadata=metadata,\n",
      "                model_info=model_info,\n",
      "                proxy_server_request=proxy_server_request,\n",
      "                preset_cache_key=preset_cache_key,\n",
      "                no_log=no_log,\n",
      "                input_cost_per_second=input_cost_per_second,\n",
      "                input_cost_per_token=input_cost_per_token,\n",
      "                output_cost_per_second=output_cost_per_second,\n",
      "                output_cost_per_token=output_cost_per_token,\n",
      "                cooldown_time=cooldown_time,\n",
      "                text_completion=kwargs.get(\"text_completion\"),\n",
      "                azure_ad_token_provider=kwargs.get(\"azure_ad_token_provider\"),\n",
      "                user_continue_message=kwargs.get(\"user_continue_message\"),\n",
      "                base_model=base_model,\n",
      "                litellm_trace_id=kwargs.get(\"litellm_trace_id\"),\n",
      "                litellm_session_id=kwargs.get(\"litellm_session_id\"),\n",
      "                hf_model_name=hf_model_name,\n",
      "                custom_prompt_dict=custom_prompt_dict,\n",
      "                litellm_metadata=kwargs.get(\"litellm_metadata\"),\n",
      "                disable_add_transform_inline_image_block=disable_add_transform_inline_image_block,\n",
      "                drop_params=kwargs.get(\"drop_params\"),\n",
      "                prompt_id=prompt_id,\n",
      "                prompt_variables=prompt_variables,\n",
      "                ssl_verify=ssl_verify,\n",
      "                merge_reasoning_content_in_choices=kwargs.get(\n",
      "                    \"merge_reasoning_content_in_choices\", None\n",
      "                ),\n",
      "                use_litellm_proxy=kwargs.get(\"use_litellm_proxy\", False),\n",
      "                api_version=api_version,\n",
      "                azure_ad_token=kwargs.get(\"azure_ad_token\"),\n",
      "                tenant_id=kwargs.get(\"tenant_id\"),\n",
      "                client_id=kwargs.get(\"client_id\"),\n",
      "                client_secret=kwargs.get(\"client_secret\"),\n",
      "                azure_username=kwargs.get(\"azure_username\"),\n",
      "                azure_password=kwargs.get(\"azure_password\"),\n",
      "                max_retries=max_retries,\n",
      "                timeout=timeout,\n",
      "            )\n",
      "            cast(LiteLLMLoggingObj, logging).update_environment_variables(\n",
      "                model=model,\n",
      "                user=user,\n",
      "                optional_params=processed_non_default_params,  # [IMPORTANT] - using processed_non_default_params ensures consistent params logged to langfuse for finetuning / eval datasets.\n",
      "                litellm_params=litellm_params,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "            )\n",
      "            if mock_response or mock_tool_calls or mock_timeout:\n",
      "                kwargs.pop(\"mock_timeout\", None)  # remove for any fallbacks triggered\n",
      "                return mock_completion(\n",
      "                    model,\n",
      "                    messages,\n",
      "                    stream=stream,\n",
      "                    n=n,\n",
      "                    mock_response=mock_response,\n",
      "                    mock_tool_calls=mock_tool_calls,\n",
      "                    logging=logging,\n",
      "                    acompletion=acompletion,\n",
      "                    mock_delay=kwargs.get(\"mock_delay\", None),\n",
      "                    custom_llm_provider=custom_llm_provider,\n",
      "                    mock_timeout=mock_timeout,\n",
      "                    timeout=timeout,\n",
      "                )\n",
      "    \n",
      "            if custom_llm_provider == \"azure\":\n",
      "                # azure configs\n",
      "                ## check dynamic params ##\n",
      "                dynamic_params = False\n",
      "                if client is not None and (\n",
      "                    isinstance(client, openai.AzureOpenAI)\n",
      "                    or isinstance(client, openai.AsyncAzureOpenAI)\n",
      "                ):\n",
      "                    dynamic_params = _check_dynamic_azure_params(\n",
      "                        azure_client_params={\"api_version\": api_version},\n",
      "                        azure_client=client,\n",
      "                    )\n",
      "    \n",
      "                api_type = get_secret(\"AZURE_API_TYPE\") or \"azure\"\n",
      "    \n",
      "                api_base = api_base or litellm.api_base or get_secret(\"AZURE_API_BASE\")\n",
      "    \n",
      "                api_version = (\n",
      "                    api_version\n",
      "                    or litellm.api_version\n",
      "                    or get_secret(\"AZURE_API_VERSION\")\n",
      "                    or litellm.AZURE_DEFAULT_API_VERSION\n",
      "                )\n",
      "    \n",
      "                api_key = (\n",
      "                    api_key\n",
      "                    or litellm.api_key\n",
      "                    or litellm.azure_key\n",
      "                    or get_secret(\"AZURE_OPENAI_API_KEY\")\n",
      "                    or get_secret(\"AZURE_API_KEY\")\n",
      "                )\n",
      "    \n",
      "                azure_ad_token = optional_params.get(\"extra_body\", {}).pop(\n",
      "                    \"azure_ad_token\", None\n",
      "                ) or get_secret(\"AZURE_AD_TOKEN\")\n",
      "    \n",
      "                azure_ad_token_provider = litellm_params.get(\n",
      "                    \"azure_ad_token_provider\", None\n",
      "                )\n",
      "    \n",
      "                headers = headers or litellm.headers\n",
      "    \n",
      "                if extra_headers is not None:\n",
      "                    optional_params[\"extra_headers\"] = extra_headers\n",
      "                if max_retries is not None:\n",
      "                    optional_params[\"max_retries\"] = max_retries\n",
      "    \n",
      "                if litellm.AzureOpenAIO1Config().is_o_series_model(model=model):\n",
      "                    ## LOAD CONFIG - if set\n",
      "                    config = litellm.AzureOpenAIO1Config.get_config()\n",
      "                    for k, v in config.items():\n",
      "                        if (\n",
      "                            k not in optional_params\n",
      "                        ):  # completion(top_k=3) > azure_config(top_k=3) <- allows for dynamic variables to be passed in\n",
      "                            optional_params[k] = v\n",
      "    \n",
      "                    response = azure_o1_chat_completions.completion(\n",
      "                        model=model,\n",
      "                        messages=messages,\n",
      "                        headers=headers,\n",
      "                        api_key=api_key,\n",
      "                        api_base=api_base,\n",
      "                        api_version=api_version,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        model_response=model_response,\n",
      "                        print_verbose=print_verbose,\n",
      "                        optional_params=optional_params,\n",
      "                        litellm_params=litellm_params,\n",
      "                        logger_fn=logger_fn,\n",
      "                        logging_obj=logging,\n",
      "                        acompletion=acompletion,\n",
      "                        timeout=timeout,  # type: ignore\n",
      "                        client=client,  # pass AsyncAzureOpenAI, AzureOpenAI client\n",
      "                        custom_llm_provider=custom_llm_provider,\n",
      "                    )\n",
      "                else:\n",
      "                    ## LOAD CONFIG - if set\n",
      "                    config = litellm.AzureOpenAIConfig.get_config()\n",
      "                    for k, v in config.items():\n",
      "                        if (\n",
      "                            k not in optional_params\n",
      "                        ):  # completion(top_k=3) > azure_config(top_k=3) <- allows for dynamic variables to be passed in\n",
      "                            optional_params[k] = v\n",
      "    \n",
      "                    ## COMPLETION CALL\n",
      ">                   response = azure_chat_completions.completion(\n",
      "                        model=model,\n",
      "                        messages=messages,\n",
      "                        headers=headers,\n",
      "                        api_key=api_key,\n",
      "                        api_base=api_base,\n",
      "                        api_version=api_version,\n",
      "                        api_type=api_type,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        model_response=model_response,\n",
      "                        print_verbose=print_verbose,\n",
      "                        optional_params=optional_params,\n",
      "                        litellm_params=litellm_params,\n",
      "                        logger_fn=logger_fn,\n",
      "                        logging_obj=logging,\n",
      "                        acompletion=acompletion,\n",
      "                        timeout=timeout,  # type: ignore\n",
      "                        client=client,  # pass AsyncAzureOpenAI, AzureOpenAI client\n",
      "                    )\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/main.py\u001b[0m:1366: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <litellm.llms.azure.azure.AzureChatCompletion object at 0x10db8dd30>\n",
      "model = 'gpt-4-turbo'\n",
      "messages = [{'content': '<context>Putin started the war, Ukraine will not surrender and will finally win!</context>', 'role': 'us...on\\'t mention any special denotations such as \"T\", \"T1\", \"T+\", \"A-\", \"Ac\", \"Re\", etc.\\n</formatting>', 'role': 'user'}]\n",
      "model_response = ModelResponse(id='chatcmpl-b5d69443-dfae-4243-8e14-2f4c1cebda0b', created=1748418121, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "api_key = '830bfa7d0fd34a5dae45e1a1d8f879de'\n",
      "api_base = 'https://openai-01-dialexityswedencentral.openai.azure.com/'\n",
      "api_version = '2024-10-21DIALEXITY_DEFAULT_MODEL=gpt-4', api_type = 'azure'\n",
      "azure_ad_token = None, azure_ad_token_provider = None, dynamic_params = False\n",
      "print_verbose = <function print_verbose at 0x10dbc3e20>, timeout = 600.0\n",
      "logging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ed49a90>\n",
      "optional_params = {'extra_body': {}, 'stream': False, 'tool_choice': 'required', 'tools': [{'function': {'description': 'Correctly forma..., 'properties': {'dialectical_components': {...}}, 'required': ['dialectical_components'], ...}}, 'type': 'function'}]}\n",
      "litellm_params = {'acompletion': False, 'aembedding': None, 'api_base': None, 'api_key': None, ...}\n",
      "logger_fn = None, acompletion = False, headers = None, client = None\n",
      "\n",
      "    def completion(  # noqa: PLR0915\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: list,\n",
      "        model_response: ModelResponse,\n",
      "        api_key: str,\n",
      "        api_base: str,\n",
      "        api_version: str,\n",
      "        api_type: str,\n",
      "        azure_ad_token: str,\n",
      "        azure_ad_token_provider: Callable,\n",
      "        dynamic_params: bool,\n",
      "        print_verbose: Callable,\n",
      "        timeout: Union[float, httpx.Timeout],\n",
      "        logging_obj: LiteLLMLoggingObj,\n",
      "        optional_params,\n",
      "        litellm_params,\n",
      "        logger_fn,\n",
      "        acompletion: bool = False,\n",
      "        headers: Optional[dict] = None,\n",
      "        client=None,\n",
      "    ):\n",
      "        if headers:\n",
      "            optional_params[\"extra_headers\"] = headers\n",
      "        try:\n",
      "            if model is None or messages is None:\n",
      "                raise AzureOpenAIError(\n",
      "                    status_code=422, message=\"Missing model or messages\"\n",
      "                )\n",
      "    \n",
      "            max_retries = optional_params.pop(\"max_retries\", None)\n",
      "            if max_retries is None:\n",
      "                max_retries = DEFAULT_MAX_RETRIES\n",
      "            json_mode: Optional[bool] = optional_params.pop(\"json_mode\", False)\n",
      "    \n",
      "            ### CHECK IF CLOUDFLARE AI GATEWAY ###\n",
      "            ### if so - set the model as part of the base url\n",
      "            if \"gateway.ai.cloudflare.com\" in api_base:\n",
      "                client = self._init_azure_client_for_cloudflare_ai_gateway(\n",
      "                    api_base=api_base,\n",
      "                    model=model,\n",
      "                    api_version=api_version,\n",
      "                    max_retries=max_retries,\n",
      "                    timeout=timeout,\n",
      "                    api_key=api_key,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    acompletion=acompletion,\n",
      "                    client=client,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "    \n",
      "                data = {\"model\": None, \"messages\": messages, **optional_params}\n",
      "            else:\n",
      "                data = litellm.AzureOpenAIConfig().transform_request(\n",
      "                    model=model,\n",
      "                    messages=messages,\n",
      "                    optional_params=optional_params,\n",
      "                    litellm_params=litellm_params,\n",
      "                    headers=headers or {},\n",
      "                )\n",
      "    \n",
      "            if acompletion is True:\n",
      "                if optional_params.get(\"stream\", False):\n",
      "                    return self.async_streaming(\n",
      "                        logging_obj=logging_obj,\n",
      "                        api_base=api_base,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        data=data,\n",
      "                        model=model,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        max_retries=max_retries,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "                else:\n",
      "                    return self.acompletion(\n",
      "                        api_base=api_base,\n",
      "                        data=data,\n",
      "                        model_response=model_response,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        model=model,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        logging_obj=logging_obj,\n",
      "                        max_retries=max_retries,\n",
      "                        convert_tool_call_to_json_mode=json_mode,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "            elif \"stream\" in optional_params and optional_params[\"stream\"] is True:\n",
      "                return self.streaming(\n",
      "                    logging_obj=logging_obj,\n",
      "                    api_base=api_base,\n",
      "                    dynamic_params=dynamic_params,\n",
      "                    data=data,\n",
      "                    model=model,\n",
      "                    api_key=api_key,\n",
      "                    api_version=api_version,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    timeout=timeout,\n",
      "                    client=client,\n",
      "                    max_retries=max_retries,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "            else:\n",
      "                ## LOGGING\n",
      "                logging_obj.pre_call(\n",
      "                    input=messages,\n",
      "                    api_key=api_key,\n",
      "                    additional_args={\n",
      "                        \"headers\": {\n",
      "                            \"api_key\": api_key,\n",
      "                            \"azure_ad_token\": azure_ad_token,\n",
      "                        },\n",
      "                        \"api_version\": api_version,\n",
      "                        \"api_base\": api_base,\n",
      "                        \"complete_input_dict\": data,\n",
      "                    },\n",
      "                )\n",
      "                if not isinstance(max_retries, int):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=422, message=\"max retries must be an int\"\n",
      "                    )\n",
      "                # init AzureOpenAI Client\n",
      "                azure_client = self.get_azure_openai_client(\n",
      "                    api_version=api_version,\n",
      "                    api_base=api_base,\n",
      "                    api_key=api_key,\n",
      "                    model=model,\n",
      "                    client=client,\n",
      "                    _is_async=False,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "                if not isinstance(azure_client, AzureOpenAI):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=500,\n",
      "                        message=\"azure_client is not an instance of AzureOpenAI\",\n",
      "                    )\n",
      "    \n",
      "                headers, response = self.make_sync_azure_openai_chat_completion_request(\n",
      "                    azure_client=azure_client, data=data, timeout=timeout\n",
      "                )\n",
      "                stringified_response = response.model_dump()\n",
      "                ## LOGGING\n",
      "                logging_obj.post_call(\n",
      "                    input=messages,\n",
      "                    api_key=api_key,\n",
      "                    original_response=stringified_response,\n",
      "                    additional_args={\n",
      "                        \"headers\": headers,\n",
      "                        \"api_version\": api_version,\n",
      "                        \"api_base\": api_base,\n",
      "                    },\n",
      "                )\n",
      "                return convert_to_model_response_object(\n",
      "                    response_object=stringified_response,\n",
      "                    model_response_object=model_response,\n",
      "                    convert_tool_call_to_json_mode=json_mode,\n",
      "                    _response_headers=headers,\n",
      "                )\n",
      "        except AzureOpenAIError as e:\n",
      "            raise e\n",
      "        except Exception as e:\n",
      "            status_code = getattr(e, \"status_code\", 500)\n",
      "            error_headers = getattr(e, \"headers\", None)\n",
      "            error_response = getattr(e, \"response\", None)\n",
      "            error_body = getattr(e, \"body\", None)\n",
      "            if error_headers is None and error_response:\n",
      "                error_headers = getattr(error_response, \"headers\", None)\n",
      ">           raise AzureOpenAIError(\n",
      "                status_code=status_code,\n",
      "                message=str(e),\n",
      "                headers=error_headers,\n",
      "                body=error_body,\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           litellm.llms.azure.common_utils.AzureOpenAIError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:358: AzureOpenAIError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "    @observe()\n",
      "    def test_wheel_3():\n",
      "        number_of_thoughts = 3\n",
      ">       wheels = asyncio.run(factory.build(theses=[None]*number_of_thoughts))\n",
      "\n",
      "\u001b[1m\u001b[31mtest_analyst.py\u001b[0m:109: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py\u001b[0m:195: in run\n",
      "    return runner.run(main)\n",
      "\u001b[1m\u001b[31m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py\u001b[0m:118: in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "\u001b[1m\u001b[31m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py\u001b[0m:719: in run_until_complete\n",
      "    return future.result()\n",
      "\u001b[1m\u001b[31m../src/dialectical_framework/synthesist/factories/wheel_builder.py\u001b[0m:59: in build\n",
      "    cycles: List[Cycle] = await analyst.extract(wu_count)\n",
      "\u001b[1m\u001b[31m../src/dialectical_framework/analyst/thought_mapping.py\u001b[0m:296: in extract\n",
      "    box = await self.find_multiple(prompt_stuff=prompt_stuff)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/langfuse/decorators/langfuse_decorator.py\u001b[0m:219: in async_wrapper\n",
      "    self._handle_exception(observation, e)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/langfuse/decorators/langfuse_decorator.py\u001b[0m:520: in _handle_exception\n",
      "    raise e\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/langfuse/decorators/langfuse_decorator.py\u001b[0m:217: in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/integrations/_middleware_factory.py\u001b[0m:146: in wrapper_async\n",
      "    result = await fn(*args, **kwargs)  # pyright: ignore [reportGeneralTypeIssues]\n",
      "\u001b[1m\u001b[31m../src/dialectical_framework/analyst/thought_mapping.py\u001b[0m:141: in find_multiple\n",
      "    return _find_multiple_call()\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/llm/_call.py\u001b[0m:346: in inner\n",
      "    result = decorated(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/core/base/_extract.py\u001b[0m:158: in inner\n",
      "    call_response = create_decorator(fn=fn, **create_decorator_kwargs)(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/core/base/_create.py\u001b[0m:232: in inner\n",
      "    response = create(stream=False, **call_kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/utils.py\u001b[0m:1283: in wrapper\n",
      "    raise e\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/utils.py\u001b[0m:1161: in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/main.py\u001b[0m:3241: in completion\n",
      "    raise exception_type(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m:2239: in exception_type\n",
      "    raise e\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "model = 'gpt-4-turbo'\n",
      "original_exception = AzureOpenAIError(\"Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\")\n",
      "custom_llm_provider = 'azure'\n",
      "completion_kwargs = {'api_key': None, 'api_version': None, 'audio': None, 'base_url': None, ...}\n",
      "extra_kwargs = {'litellm_call_id': '2cbc83fa-dcab-4e29-8461-81d0187ee4c5', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ed49a90>}\n",
      "\n",
      "    def exception_type(  # type: ignore  # noqa: PLR0915\n",
      "        model,\n",
      "        original_exception,\n",
      "        custom_llm_provider,\n",
      "        completion_kwargs={},\n",
      "        extra_kwargs={},\n",
      "    ):\n",
      "        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n",
      "        if any(\n",
      "            isinstance(original_exception, exc_type)\n",
      "            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n",
      "        ):\n",
      "            return original_exception\n",
      "        exception_mapping_worked = False\n",
      "        exception_provider = custom_llm_provider\n",
      "        if litellm.suppress_debug_info is False:\n",
      "            print()  # noqa\n",
      "            print(  # noqa\n",
      "                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n",
      "            )  # noqa\n",
      "            print(  # noqa\n",
      "                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n",
      "            )  # noqa\n",
      "            print()  # noqa\n",
      "    \n",
      "        litellm_response_headers = _get_response_headers(\n",
      "            original_exception=original_exception\n",
      "        )\n",
      "        try:\n",
      "            error_str = str(original_exception)\n",
      "            if model:\n",
      "                if hasattr(original_exception, \"message\"):\n",
      "                    error_str = str(original_exception.message)\n",
      "                if isinstance(original_exception, BaseException):\n",
      "                    exception_type = type(original_exception).__name__\n",
      "                else:\n",
      "                    exception_type = \"\"\n",
      "    \n",
      "                ################################################################################\n",
      "                # Common Extra information needed for all providers\n",
      "                # We pass num retries, api_base, vertex_deployment etc to the exception here\n",
      "                ################################################################################\n",
      "                extra_information = \"\"\n",
      "                try:\n",
      "                    _api_base = litellm.get_api_base(\n",
      "                        model=model, optional_params=extra_kwargs\n",
      "                    )\n",
      "                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n",
      "                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n",
      "                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n",
      "                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n",
      "                    _model_group = _metadata.get(\"model_group\")\n",
      "                    _deployment = _metadata.get(\"deployment\")\n",
      "                    extra_information = f\"\\nModel: {model}\"\n",
      "    \n",
      "                    if (\n",
      "                        isinstance(custom_llm_provider, str)\n",
      "                        and len(custom_llm_provider) > 0\n",
      "                    ):\n",
      "                        exception_provider = (\n",
      "                            custom_llm_provider[0].upper()\n",
      "                            + custom_llm_provider[1:]\n",
      "                            + \"Exception\"\n",
      "                        )\n",
      "    \n",
      "                    if _api_base:\n",
      "                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n",
      "                    if (\n",
      "                        messages\n",
      "                        and len(messages) > 0\n",
      "                        and litellm.redact_messages_in_exceptions is False\n",
      "                    ):\n",
      "                        extra_information += f\"\\nMessages: `{messages}`\"\n",
      "    \n",
      "                    if _model_group is not None:\n",
      "                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n",
      "                    if _deployment is not None:\n",
      "                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n",
      "                    if _vertex_project is not None:\n",
      "                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n",
      "                    if _vertex_location is not None:\n",
      "                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n",
      "    \n",
      "                    # on litellm proxy add key name + team to exceptions\n",
      "                    extra_information = _add_key_name_and_team_to_alert(\n",
      "                        request_info=extra_information, metadata=_metadata\n",
      "                    )\n",
      "                except Exception:\n",
      "                    # DO NOT LET this Block raising the original exception\n",
      "                    pass\n",
      "    \n",
      "                ################################################################################\n",
      "                # End of Common Extra information Needed for all providers\n",
      "                ################################################################################\n",
      "    \n",
      "                ################################################################################\n",
      "                #################### Start of Provider Exception mapping ####################\n",
      "                ################################################################################\n",
      "    \n",
      "                if (\n",
      "                    \"Request Timeout Error\" in error_str\n",
      "                    or \"Request timed out\" in error_str\n",
      "                    or \"Timed out generating response\" in error_str\n",
      "                    or \"The read operation timed out\" in error_str\n",
      "                ):\n",
      "                    exception_mapping_worked = True\n",
      "    \n",
      "                    raise Timeout(\n",
      "                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n",
      "                        model=model,\n",
      "                        llm_provider=custom_llm_provider,\n",
      "                        litellm_debug_info=extra_information,\n",
      "                    )\n",
      "    \n",
      "                if (\n",
      "                    custom_llm_provider == \"litellm_proxy\"\n",
      "                ):  # handle special case where calling litellm proxy + exception str contains error message\n",
      "                    extract_and_raise_litellm_exception(\n",
      "                        response=getattr(original_exception, \"response\", None),\n",
      "                        error_str=error_str,\n",
      "                        model=model,\n",
      "                        custom_llm_provider=custom_llm_provider,\n",
      "                    )\n",
      "                if (\n",
      "                    custom_llm_provider == \"openai\"\n",
      "                    or custom_llm_provider == \"text-completion-openai\"\n",
      "                    or custom_llm_provider == \"custom_openai\"\n",
      "                    or custom_llm_provider in litellm.openai_compatible_providers\n",
      "                ):\n",
      "                    # custom_llm_provider is openai, make it OpenAI\n",
      "                    message = get_error_message(error_obj=original_exception)\n",
      "                    if message is None:\n",
      "                        if hasattr(original_exception, \"message\"):\n",
      "                            message = original_exception.message\n",
      "                        else:\n",
      "                            message = str(original_exception)\n",
      "    \n",
      "                    if message is not None and isinstance(\n",
      "                        message, str\n",
      "                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n",
      "                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n",
      "                        message = message.replace(\n",
      "                            \"openai.OpenAIError\",\n",
      "                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n",
      "                        )\n",
      "                    if custom_llm_provider == \"openai\":\n",
      "                        exception_provider = \"OpenAI\" + \"Exception\"\n",
      "                    else:\n",
      "                        exception_provider = (\n",
      "                            custom_llm_provider[0].upper()\n",
      "                            + custom_llm_provider[1:]\n",
      "                            + \"Exception\"\n",
      "                        )\n",
      "    \n",
      "                    if \"429\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"RateLimitError: {exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"This model's maximum context length is\" in error_str\n",
      "                        or \"string too long. Expected a string with maximum length\"\n",
      "                        in error_str\n",
      "                        or \"model's maximum context limit\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"invalid_request_error\" in error_str\n",
      "                        and \"model_not_found\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise NotFoundError(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"A timeout occurred\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        (\n",
      "                            \"invalid_request_error\" in error_str\n",
      "                            and \"content_policy_violation\" in error_str\n",
      "                        )\n",
      "                        or (\n",
      "                            \"Invalid prompt\" in error_str\n",
      "                            and \"violating our usage policy\" in error_str\n",
      "                        )\n",
      "                        or (\n",
      "                            \"request was rejected as a result of the safety system\"\n",
      "                            in error_str.lower()\n",
      "                        )\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"invalid_request_error\" in error_str\n",
      "                        and \"Incorrect API key provided\" not in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            body=getattr(original_exception, \"body\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Web server is returning an unknown error\" in error_str\n",
      "                        or \"The server had an error processing your request.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                        )\n",
      "                    elif \"Request too large\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"RateLimitError: {exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"Mistral API raised a streaming error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        _request = httpx.Request(\n",
      "                            method=\"POST\", url=\"https://api.openai.com/v1\"\n",
      "                        )\n",
      "                        raise APIError(\n",
      "                            status_code=500,\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            request=_request,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        exception_mapping_worked = True\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{exception_provider} - {message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"NotFoundError: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"Timeout Error: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                body=getattr(original_exception, \"body\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"RateLimitError: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"Timeout Error: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"APIError: {exception_provider} - {message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                request=getattr(original_exception, \"request\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                    else:\n",
      "                        # if no status code then it is an APIConnectionError: https://github.com/openai/openai-python#handling-errors\n",
      "                        # exception_mapping_worked = True\n",
      "                        raise APIConnectionError(\n",
      "                            message=f\"APIConnectionError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            request=httpx.Request(\n",
      "                                method=\"POST\", url=\"https://api.openai.com/v1/\"\n",
      "                            ),\n",
      "                        )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"anthropic\"\n",
      "                    or custom_llm_provider == \"anthropic_text\"\n",
      "                ):  # one of the anthropics\n",
      "                    if \"prompt is too long\" in error_str or \"prompt: length\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    elif \"overloaded_error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise InternalServerError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if \"Invalid API Key\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if \"content filtering policy\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if \"Client error '400 Bad Request'\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        verbose_logger.debug(\n",
      "                            f\"status_code: {original_exception.status_code}\"\n",
      "                        )\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 413\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"anthropic\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"anthropic\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"anthropic\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 500\n",
      "                            or original_exception.status_code == 529\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.InternalServerError(\n",
      "                                message=f\"AnthropicException - {error_str}. Handle with `litellm.InternalServerError`.\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.ServiceUnavailableError(\n",
      "                                message=f\"AnthropicException - {error_str}. Handle with `litellm.ServiceUnavailableError`.\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"replicate\":\n",
      "                    if \"Incorrect authentication token\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            llm_provider=\"replicate\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"input is too long\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"replicate\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif exception_type == \"ModelError\":\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"replicate\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Request was throttled\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            llm_provider=\"replicate\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 413\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"replicate\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise UnprocessableEntityError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"replicate\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"replicate\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise UnprocessableEntityError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    exception_mapping_worked = True\n",
      "                    raise APIError(\n",
      "                        status_code=500,\n",
      "                        message=f\"ReplicateException - {str(original_exception)}\",\n",
      "                        llm_provider=\"replicate\",\n",
      "                        model=model,\n",
      "                        request=httpx.Request(\n",
      "                            method=\"POST\",\n",
      "                            url=\"https://api.replicate.com/v1/deployments\",\n",
      "                        ),\n",
      "                    )\n",
      "                elif custom_llm_provider in litellm._openai_like_providers:\n",
      "                    if \"authorization denied for\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "    \n",
      "                        # Predibase returns the raw API Key in the response - this block ensures it's not returned in the exception\n",
      "                        if (\n",
      "                            error_str is not None\n",
      "                            and isinstance(error_str, str)\n",
      "                            and \"bearer\" in error_str.lower()\n",
      "                        ):\n",
      "                            # only keep the first 10 chars after the occurnence of \"bearer\"\n",
      "                            _bearer_token_start_index = error_str.lower().find(\"bearer\")\n",
      "                            error_str = error_str[: _bearer_token_start_index + 14]\n",
      "                            error_str += \"XXXXXXX\" + '\"'\n",
      "    \n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"{custom_llm_provider}Exception: Authentication Error - {error_str}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"model's maximum context limit\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"{custom_llm_provider}Exception: Context Window Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                        )\n",
      "                    elif \"token_quota_reached\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"{custom_llm_provider}Exception: Rate Limit Errror - {error_str}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The server received an invalid response from an upstream server.\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                        )\n",
      "                    elif \"model_no_support_for_function\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"{custom_llm_provider}Exception - Use 'watsonx_text' route instead. IBM WatsonX does not support `/text/chat` endpoint. - {error_str}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.InternalServerError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 401\n",
      "                            or original_exception.status_code == 403\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 422\n",
      "                            or original_exception.status_code == 424\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"bedrock\":\n",
      "                    if (\n",
      "                        \"too many tokens\" in error_str\n",
      "                        or \"expected maxLength:\" in error_str\n",
      "                        or \"Input is too long\" in error_str\n",
      "                        or \"prompt is too long\" in error_str\n",
      "                        or \"prompt: length: 1..\" in error_str\n",
      "                        or \"Too many input tokens\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"BedrockException: Context Window Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Conversation blocks and tool result blocks cannot be provided in the same turn.\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"BedrockException - {error_str}\\n. Enable 'litellm.modify_params=True' (for PROXY do: `litellm_settings::modify_params: True`) to insert a dummy assistant message and fix this error.\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Malformed input request\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"BedrockException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"A conversation must start with a user message.\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"BedrockException - {error_str}\\n. Pass in default user message via `completion(..,user_continue_message=)` or enable `litellm.modify_params=True`.\\nFor Proxy: do via `litellm_settings::modify_params: True` or user_continue_message under `litellm_params`\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Unable to locate credentials\" in error_str\n",
      "                        or \"The security token included in the request is invalid\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"BedrockException Invalid Authentication - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"AccessDeniedException\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise PermissionDeniedError(\n",
      "                            message=f\"BedrockException PermissionDeniedError - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"throttlingException\" in error_str\n",
      "                        or \"ThrottlingException\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"BedrockException: Rate Limit Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Connect timeout on endpoint URL\" in error_str\n",
      "                        or \"timed out\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"BedrockException: Timeout Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                        )\n",
      "                    elif \"Could not process image\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"BedrockException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=500,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\", url=\"https://api.openai.com/v1/\"\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"sagemaker\"\n",
      "                    or custom_llm_provider == \"sagemaker_chat\"\n",
      "                ):\n",
      "                    if \"Unable to locate credentials\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"litellm.BadRequestError: SagemakerException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"sagemaker\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Input validation error: `best_of` must be > 0 and <= 2\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=\"SagemakerException - the value of 'n' must be > 0 and <= 2 for sagemaker endpoints\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"sagemaker\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"`inputs` tokens + `max_new_tokens` must be <=\" in error_str\n",
      "                        or \"instance type with more CPU capacity or memory\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"SagemakerException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"sagemaker\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=500,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\", url=\"https://api.openai.com/v1/\"\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 422\n",
      "                            or original_exception.status_code == 424\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"vertex_ai\"\n",
      "                    or custom_llm_provider == \"vertex_ai_beta\"\n",
      "                    or custom_llm_provider == \"gemini\"\n",
      "                ):\n",
      "                    if (\n",
      "                        \"Vertex AI API has not been used in project\" in error_str\n",
      "                        or \"Unable to find your project\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"litellm.BadRequestError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            response=httpx.Response(\n",
      "                                status_code=400,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    if \"400 Request payload size exceeds\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"VertexException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"None Unknown Error.\" in error_str\n",
      "                        or \"Content has no parts.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"litellm.InternalServerError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            response=httpx.Response(\n",
      "                                status_code=500,\n",
      "                                content=str(original_exception),\n",
      "                                request=httpx.Request(method=\"completion\", url=\"https://github.com/BerriAI/litellm\"),  # type: ignore\n",
      "                            ),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"API key not valid.\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"{custom_llm_provider}Exception - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"403\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"VertexAIException BadRequestError - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            response=httpx.Response(\n",
      "                                status_code=403,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The response was blocked.\" in error_str\n",
      "                        or \"Output blocked by content filtering policy\"\n",
      "                        in error_str  # anthropic on vertex ai\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=f\"VertexAIException ContentPolicyViolationError - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=httpx.Response(\n",
      "                                status_code=400,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"429 Quota exceeded\" in error_str\n",
      "                        or \"Quota exceeded for\" in error_str\n",
      "                        or \"IndexError: list index out of range\" in error_str\n",
      "                        or \"429 Unable to submit request because the service is temporarily out of capacity.\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"litellm.RateLimitError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=httpx.Response(\n",
      "                                status_code=429,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"500 Internal Server Error\" in error_str\n",
      "                        or \"The model is overloaded.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"litellm.InternalServerError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"VertexAIException BadRequestError - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"vertex_ai\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=400,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\",\n",
      "                                        url=\"https://cloud.google.com/vertex-ai/\",\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        if original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        if original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "    \n",
      "                        if original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"litellm.RateLimitError: VertexAIException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"vertex_ai\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=429,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\",\n",
      "                                        url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.InternalServerError(\n",
      "                                message=f\"VertexAIException InternalServerError - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"vertex_ai\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=500,\n",
      "                                    content=str(original_exception),\n",
      "                                    request=httpx.Request(method=\"completion\", url=\"https://github.com/BerriAI/litellm\"),  # type: ignore\n",
      "                                ),\n",
      "                            )\n",
      "                        if original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"palm\" or custom_llm_provider == \"gemini\":\n",
      "                    if \"503 Getting metadata\" in error_str:\n",
      "                        # auth errors look like this\n",
      "                        # 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=\"GeminiException - Invalid api key\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"palm\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if (\n",
      "                        \"504 Deadline expired before operation could complete.\" in error_str\n",
      "                        or \"504 Deadline Exceeded\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"GeminiException - {original_exception.message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"palm\",\n",
      "                            exception_status_code=original_exception.status_code,\n",
      "                        )\n",
      "                    if \"400 Request payload size exceeds\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"GeminiException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"palm\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if (\n",
      "                        \"500 An internal error has occurred.\" in error_str\n",
      "                        or \"list index out of range\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise APIError(\n",
      "                            status_code=getattr(original_exception, \"status_code\", 500),\n",
      "                            message=f\"GeminiException - {original_exception.message}\",\n",
      "                            llm_provider=\"palm\",\n",
      "                            model=model,\n",
      "                            request=httpx.Response(\n",
      "                                status_code=429,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"GeminiException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"palm\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    # Dailed: Error occurred: 400 Request payload size exceeds the limit: 20000 bytes\n",
      "                elif custom_llm_provider == \"cloudflare\":\n",
      "                    if \"Authentication error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"Cloudflare Exception - {original_exception.message}\",\n",
      "                            llm_provider=\"cloudflare\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if \"must have required property\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"Cloudflare Exception - {original_exception.message}\",\n",
      "                            llm_provider=\"cloudflare\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"cohere\" or custom_llm_provider == \"cohere_chat\"\n",
      "                ):  # Cohere\n",
      "                    if (\n",
      "                        \"invalid api token\" in error_str\n",
      "                        or \"No API key provided.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"too many tokens\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"cohere\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 498\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    elif (\n",
      "                        \"CohereConnectionError\" in exception_type\n",
      "                    ):  # cohere seems to fire these errors when we load test it (1k+ messages / min)\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"invalid type:\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Unexpected server error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ServiceUnavailableError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    else:\n",
      "                        if hasattr(original_exception, \"status_code\"):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                        raise original_exception\n",
      "                elif custom_llm_provider == \"huggingface\":\n",
      "                    if \"length limit exceeded\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=error_str,\n",
      "                            model=model,\n",
      "                            llm_provider=\"huggingface\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"A valid user token is required\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=error_str,\n",
      "                            llm_provider=\"huggingface\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Rate limit reached\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=error_str,\n",
      "                            llm_provider=\"huggingface\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"huggingface\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"ai21\":\n",
      "                    if hasattr(original_exception, \"message\"):\n",
      "                        if \"Prompt has too many tokens\" in original_exception.message:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ContextWindowExceededError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        if \"Bad or missing API token.\" in original_exception.message:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                llm_provider=\"ai21\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                            )\n",
      "                        if original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                llm_provider=\"ai21\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                llm_provider=\"ai21\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"nlp_cloud\":\n",
      "                    if \"detail\" in error_str:\n",
      "                        if \"Input text length should not exceed\" in error_str:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ContextWindowExceededError(\n",
      "                                message=f\"NLPCloudException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif \"value is not a valid\" in error_str:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"NLPCloudException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=500,\n",
      "                                message=f\"NLPCloudException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                    if hasattr(\n",
      "                        original_exception, \"status_code\"\n",
      "                    ):  # https://docs.nlpcloud.com/?shell#errors\n",
      "                        if (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 406\n",
      "                            or original_exception.status_code == 413\n",
      "                            or original_exception.status_code == 422\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 401\n",
      "                            or original_exception.status_code == 403\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 522\n",
      "                            or original_exception.status_code == 524\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 429\n",
      "                            or original_exception.status_code == 402\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 500\n",
      "                            or original_exception.status_code == 503\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 504\n",
      "                            or original_exception.status_code == 520\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"together_ai\":\n",
      "                    try:\n",
      "                        error_response = json.loads(error_str)\n",
      "                    except Exception:\n",
      "                        error_response = {\"error\": error_str}\n",
      "                    if (\n",
      "                        \"error\" in error_response\n",
      "                        and \"`inputs` tokens + `max_new_tokens` must be <=\"\n",
      "                        in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error\" in error_response\n",
      "                        and \"invalid private key\" in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error\" in error_response\n",
      "                        and \"INVALID_ARGUMENT\" in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"A timeout occurred\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"TogetherAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error\" in error_response\n",
      "                        and \"API key doesn't match expected format.\"\n",
      "                        in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error_type\" in error_response\n",
      "                        and error_response[\"error_type\"] == \"validation\"\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"together_ai\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"together_ai\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                                llm_provider=\"together_ai\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 524:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                                llm_provider=\"together_ai\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                    else:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise APIError(\n",
      "                            status_code=original_exception.status_code,\n",
      "                            message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            model=model,\n",
      "                            request=original_exception.request,\n",
      "                        )\n",
      "                elif custom_llm_provider == \"aleph_alpha\":\n",
      "                    if (\n",
      "                        \"This is longer than the model's maximum context length\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                            llm_provider=\"aleph_alpha\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"InvalidToken\" in error_str or \"No token provided\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                            llm_provider=\"aleph_alpha\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        verbose_logger.debug(\n",
      "                            f\"status code: {original_exception.status_code}\"\n",
      "                        )\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        raise original_exception\n",
      "                    raise original_exception\n",
      "                elif (\n",
      "                    custom_llm_provider == \"ollama\" or custom_llm_provider == \"ollama_chat\"\n",
      "                ):\n",
      "                    if isinstance(original_exception, dict):\n",
      "                        error_str = original_exception.get(\"error\", \"\")\n",
      "                    else:\n",
      "                        error_str = str(original_exception)\n",
      "                    if \"no such file or directory\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"OllamaException: Invalid Model/Model not loaded - {original_exception}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"ollama\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Failed to establish a new connection\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ServiceUnavailableError(\n",
      "                            message=f\"OllamaException: {original_exception}\",\n",
      "                            llm_provider=\"ollama\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Invalid response object from API\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"OllamaException: {original_exception}\",\n",
      "                            llm_provider=\"ollama\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Read timed out\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"OllamaException: {original_exception}\",\n",
      "                            llm_provider=\"ollama\",\n",
      "                            model=model,\n",
      "                        )\n",
      "                elif custom_llm_provider == \"vllm\":\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 0:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIConnectionError(\n",
      "                                message=f\"VLLMException - {original_exception.message}\",\n",
      "                                llm_provider=\"vllm\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"azure\" or custom_llm_provider == \"azure_text\":\n",
      "                    message = get_error_message(error_obj=original_exception)\n",
      "                    if message is None:\n",
      "                        if hasattr(original_exception, \"message\"):\n",
      "                            message = original_exception.message\n",
      "                        else:\n",
      "                            message = str(original_exception)\n",
      "    \n",
      "                    if \"Internal server error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"AzureException Internal server error - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"This model's maximum context length is\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"AzureException ContextWindowExceededError - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"DeploymentNotFound\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise NotFoundError(\n",
      "                            message=f\"AzureException NotFoundError - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        (\n",
      "                            \"invalid_request_error\" in error_str\n",
      "                            and \"content_policy_violation\" in error_str\n",
      "                        )\n",
      "                        or (\n",
      "                            \"The response was filtered due to the prompt triggering Azure OpenAI's content management\"\n",
      "                            in error_str\n",
      "                        )\n",
      "                        or \"Your task failed as a result of our safety system\" in error_str\n",
      "                        or \"The model produced invalid content\" in error_str\n",
      "                        or \"content_filter_policy\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=f\"litellm.ContentPolicyViolationError: AzureException - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"invalid_request_error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"AzureException BadRequestError - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            body=getattr(original_exception, \"body\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The api_key client option must be set either by passing api_key to the client or by setting\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"{exception_provider} AuthenticationError - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Connection error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise APIConnectionError(\n",
      "                            message=f\"{exception_provider} APIConnectionError - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        exception_mapping_worked = True\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AzureException - {message}\",\n",
      "                                llm_provider=\"azure\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                body=getattr(original_exception, \"body\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AzureException AuthenticationError - {message}\",\n",
      "                                llm_provider=\"azure\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AzureException Timeout - {message}\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                llm_provider=\"azure\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AzureException BadRequestError - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AzureException RateLimitError - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"AzureException ServiceUnavailableError - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AzureException Timeout - {message}\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                llm_provider=\"azure\",\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      ">                           raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"AzureException APIError - {message}\",\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                model=model,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\", url=\"https://openai.com/\"\n",
      "                                ),\n",
      "                            )\n",
      "\u001b[1m\u001b[31mE                           litellm.exceptions.APIError: litellm.APIError: AzureException APIError - Resource not found\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m:2077: APIError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\u001b[31m\u001b[1m_________________________________ test_wheel_4 _________________________________\u001b[0m\n",
      "\n",
      "self = <litellm.llms.azure.azure.AzureChatCompletion object at 0x10db8dd30>\n",
      "model = 'gpt-4-turbo'\n",
      "messages = [{'content': '<context>Putin started the war, Ukraine will not surrender and will finally win!</context>', 'role': 'us...on\\'t mention any special denotations such as \"T\", \"T1\", \"T+\", \"A-\", \"Ac\", \"Re\", etc.\\n</formatting>', 'role': 'user'}]\n",
      "model_response = ModelResponse(id='chatcmpl-3286120d-6a0f-486c-ab5c-79f261c09663', created=1748418122, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "api_key = '830bfa7d0fd34a5dae45e1a1d8f879de'\n",
      "api_base = 'https://openai-01-dialexityswedencentral.openai.azure.com/'\n",
      "api_version = '2024-10-21DIALEXITY_DEFAULT_MODEL=gpt-4', api_type = 'azure'\n",
      "azure_ad_token = None, azure_ad_token_provider = None, dynamic_params = False\n",
      "print_verbose = <function print_verbose at 0x10dbc3e20>, timeout = 600.0\n",
      "logging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ed4b890>\n",
      "optional_params = {'extra_body': {}, 'stream': False, 'tool_choice': 'required', 'tools': [{'function': {'description': 'Correctly forma..., 'properties': {'dialectical_components': {...}}, 'required': ['dialectical_components'], ...}}, 'type': 'function'}]}\n",
      "litellm_params = {'acompletion': False, 'aembedding': None, 'api_base': None, 'api_key': None, ...}\n",
      "logger_fn = None, acompletion = False, headers = None, client = None\n",
      "\n",
      "    def completion(  # noqa: PLR0915\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: list,\n",
      "        model_response: ModelResponse,\n",
      "        api_key: str,\n",
      "        api_base: str,\n",
      "        api_version: str,\n",
      "        api_type: str,\n",
      "        azure_ad_token: str,\n",
      "        azure_ad_token_provider: Callable,\n",
      "        dynamic_params: bool,\n",
      "        print_verbose: Callable,\n",
      "        timeout: Union[float, httpx.Timeout],\n",
      "        logging_obj: LiteLLMLoggingObj,\n",
      "        optional_params,\n",
      "        litellm_params,\n",
      "        logger_fn,\n",
      "        acompletion: bool = False,\n",
      "        headers: Optional[dict] = None,\n",
      "        client=None,\n",
      "    ):\n",
      "        if headers:\n",
      "            optional_params[\"extra_headers\"] = headers\n",
      "        try:\n",
      "            if model is None or messages is None:\n",
      "                raise AzureOpenAIError(\n",
      "                    status_code=422, message=\"Missing model or messages\"\n",
      "                )\n",
      "    \n",
      "            max_retries = optional_params.pop(\"max_retries\", None)\n",
      "            if max_retries is None:\n",
      "                max_retries = DEFAULT_MAX_RETRIES\n",
      "            json_mode: Optional[bool] = optional_params.pop(\"json_mode\", False)\n",
      "    \n",
      "            ### CHECK IF CLOUDFLARE AI GATEWAY ###\n",
      "            ### if so - set the model as part of the base url\n",
      "            if \"gateway.ai.cloudflare.com\" in api_base:\n",
      "                client = self._init_azure_client_for_cloudflare_ai_gateway(\n",
      "                    api_base=api_base,\n",
      "                    model=model,\n",
      "                    api_version=api_version,\n",
      "                    max_retries=max_retries,\n",
      "                    timeout=timeout,\n",
      "                    api_key=api_key,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    acompletion=acompletion,\n",
      "                    client=client,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "    \n",
      "                data = {\"model\": None, \"messages\": messages, **optional_params}\n",
      "            else:\n",
      "                data = litellm.AzureOpenAIConfig().transform_request(\n",
      "                    model=model,\n",
      "                    messages=messages,\n",
      "                    optional_params=optional_params,\n",
      "                    litellm_params=litellm_params,\n",
      "                    headers=headers or {},\n",
      "                )\n",
      "    \n",
      "            if acompletion is True:\n",
      "                if optional_params.get(\"stream\", False):\n",
      "                    return self.async_streaming(\n",
      "                        logging_obj=logging_obj,\n",
      "                        api_base=api_base,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        data=data,\n",
      "                        model=model,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        max_retries=max_retries,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "                else:\n",
      "                    return self.acompletion(\n",
      "                        api_base=api_base,\n",
      "                        data=data,\n",
      "                        model_response=model_response,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        model=model,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        logging_obj=logging_obj,\n",
      "                        max_retries=max_retries,\n",
      "                        convert_tool_call_to_json_mode=json_mode,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "            elif \"stream\" in optional_params and optional_params[\"stream\"] is True:\n",
      "                return self.streaming(\n",
      "                    logging_obj=logging_obj,\n",
      "                    api_base=api_base,\n",
      "                    dynamic_params=dynamic_params,\n",
      "                    data=data,\n",
      "                    model=model,\n",
      "                    api_key=api_key,\n",
      "                    api_version=api_version,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    timeout=timeout,\n",
      "                    client=client,\n",
      "                    max_retries=max_retries,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "            else:\n",
      "                ## LOGGING\n",
      "                logging_obj.pre_call(\n",
      "                    input=messages,\n",
      "                    api_key=api_key,\n",
      "                    additional_args={\n",
      "                        \"headers\": {\n",
      "                            \"api_key\": api_key,\n",
      "                            \"azure_ad_token\": azure_ad_token,\n",
      "                        },\n",
      "                        \"api_version\": api_version,\n",
      "                        \"api_base\": api_base,\n",
      "                        \"complete_input_dict\": data,\n",
      "                    },\n",
      "                )\n",
      "                if not isinstance(max_retries, int):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=422, message=\"max retries must be an int\"\n",
      "                    )\n",
      "                # init AzureOpenAI Client\n",
      "                azure_client = self.get_azure_openai_client(\n",
      "                    api_version=api_version,\n",
      "                    api_base=api_base,\n",
      "                    api_key=api_key,\n",
      "                    model=model,\n",
      "                    client=client,\n",
      "                    _is_async=False,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "                if not isinstance(azure_client, AzureOpenAI):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=500,\n",
      "                        message=\"azure_client is not an instance of AzureOpenAI\",\n",
      "                    )\n",
      "    \n",
      ">               headers, response = self.make_sync_azure_openai_chat_completion_request(\n",
      "                    azure_client=azure_client, data=data, timeout=timeout\n",
      "                )\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:328: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:148: in make_sync_azure_openai_chat_completion_request\n",
      "    raise e\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:140: in make_sync_azure_openai_chat_completion_request\n",
      "    raw_response = azure_client.chat.completions.with_raw_response.create(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_legacy_response.py\u001b[0m:364: in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_utils/_utils.py\u001b[0m:287: in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\u001b[0m:925: in create\n",
      "    return self._post(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_base_client.py\u001b[0m:1239: in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <openai.lib.azure.AzureOpenAI object at 0x10ede83e0>\n",
      "cast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\n",
      "options = FinalRequestOptions(method='post', url='/deployments/gpt-4-turbo/chat/completions', params={}, headers={'X-Stainless-R...ctical Components'}}, 'required': ['dialectical_components'], 'type': 'object'}}, 'type': 'function'}]}, extra_json={})\n",
      "\n",
      "    def request(\n",
      "        self,\n",
      "        cast_to: Type[ResponseT],\n",
      "        options: FinalRequestOptions,\n",
      "        *,\n",
      "        stream: bool = False,\n",
      "        stream_cls: type[_StreamT] | None = None,\n",
      "    ) -> ResponseT | _StreamT:\n",
      "        cast_to = self._maybe_override_cast_to(cast_to, options)\n",
      "    \n",
      "        # create a copy of the options we were given so that if the\n",
      "        # options are mutated later & we then retry, the retries are\n",
      "        # given the original options\n",
      "        input_options = model_copy(options)\n",
      "        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n",
      "            # ensure the idempotency key is reused between requests\n",
      "            input_options.idempotency_key = self._idempotency_key()\n",
      "    \n",
      "        response: httpx.Response | None = None\n",
      "        max_retries = input_options.get_max_retries(self.max_retries)\n",
      "    \n",
      "        retries_taken = 0\n",
      "        for retries_taken in range(max_retries + 1):\n",
      "            options = model_copy(input_options)\n",
      "            options = self._prepare_options(options)\n",
      "    \n",
      "            remaining_retries = max_retries - retries_taken\n",
      "            request = self._build_request(options, retries_taken=retries_taken)\n",
      "            self._prepare_request(request)\n",
      "    \n",
      "            kwargs: HttpxSendArgs = {}\n",
      "            if self.custom_auth is not None:\n",
      "                kwargs[\"auth\"] = self.custom_auth\n",
      "    \n",
      "            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n",
      "    \n",
      "            response = None\n",
      "            try:\n",
      "                response = self._client.send(\n",
      "                    request,\n",
      "                    stream=stream or self._should_stream_response_body(request=request),\n",
      "                    **kwargs,\n",
      "                )\n",
      "            except httpx.TimeoutException as err:\n",
      "                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n",
      "    \n",
      "                if remaining_retries > 0:\n",
      "                    self._sleep_for_retry(\n",
      "                        retries_taken=retries_taken,\n",
      "                        max_retries=max_retries,\n",
      "                        options=input_options,\n",
      "                        response=None,\n",
      "                    )\n",
      "                    continue\n",
      "    \n",
      "                log.debug(\"Raising timeout error\")\n",
      "                raise APITimeoutError(request=request) from err\n",
      "            except Exception as err:\n",
      "                log.debug(\"Encountered Exception\", exc_info=True)\n",
      "    \n",
      "                if remaining_retries > 0:\n",
      "                    self._sleep_for_retry(\n",
      "                        retries_taken=retries_taken,\n",
      "                        max_retries=max_retries,\n",
      "                        options=input_options,\n",
      "                        response=None,\n",
      "                    )\n",
      "                    continue\n",
      "    \n",
      "                log.debug(\"Raising connection error\")\n",
      "                raise APIConnectionError(request=request) from err\n",
      "    \n",
      "            log.debug(\n",
      "                'HTTP Response: %s %s \"%i %s\" %s',\n",
      "                request.method,\n",
      "                request.url,\n",
      "                response.status_code,\n",
      "                response.reason_phrase,\n",
      "                response.headers,\n",
      "            )\n",
      "            log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n",
      "    \n",
      "            try:\n",
      "                response.raise_for_status()\n",
      "            except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n",
      "                log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n",
      "    \n",
      "                if remaining_retries > 0 and self._should_retry(err.response):\n",
      "                    err.response.close()\n",
      "                    self._sleep_for_retry(\n",
      "                        retries_taken=retries_taken,\n",
      "                        max_retries=max_retries,\n",
      "                        options=input_options,\n",
      "                        response=response,\n",
      "                    )\n",
      "                    continue\n",
      "    \n",
      "                # If the response is streamed then we need to explicitly read the response\n",
      "                # to completion before attempting to access the response text.\n",
      "                if not err.response.is_closed:\n",
      "                    err.response.read()\n",
      "    \n",
      "                log.debug(\"Re-raising status error\")\n",
      ">               raise self._make_status_error_from_response(err.response) from None\n",
      "\u001b[1m\u001b[31mE               openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/openai/_base_client.py\u001b[0m:1034: NotFoundError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "model = 'gpt-4-turbo'\n",
      "messages = [{'content': '<context>Putin started the war, Ukraine will not surrender and will finally win!</context>', 'role': 'us...on\\'t mention any special denotations such as \"T\", \"T1\", \"T+\", \"A-\", \"Ac\", \"Re\", etc.\\n</formatting>', 'role': 'user'}]\n",
      "timeout = 600.0, temperature = None, top_p = None, n = None, stream = False\n",
      "stream_options = None, stop = None, max_completion_tokens = None\n",
      "max_tokens = None, modalities = None, prediction = None, audio = None\n",
      "presence_penalty = None, frequency_penalty = None, logit_bias = None\n",
      "user = None, reasoning_effort = None, response_format = None, seed = None\n",
      "tools = [{'function': {'description': 'Correctly formatted and typed parameters extracted from the completion. Must include re...itle': 'Dialectical Components', 'type': 'array'}}, 'required': ['dialectical_components'], ...}}, 'type': 'function'}]\n",
      "tool_choice = 'required', logprobs = None, top_logprobs = None\n",
      "parallel_tool_calls = None, web_search_options = None, deployment_id = None\n",
      "extra_headers = None, functions = None, function_call = None, base_url = None\n",
      "api_version = '2024-10-21DIALEXITY_DEFAULT_MODEL=gpt-4'\n",
      "api_key = '830bfa7d0fd34a5dae45e1a1d8f879de', model_list = None, thinking = None\n",
      "kwargs = {'litellm_call_id': '9c65165a-ff1d-4e1b-bd69-aed0ec2491ff', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ed4b890>}\n",
      "args = {'api_key': None, 'api_version': None, 'audio': None, 'base_url': None, ...}\n",
      "api_base = 'https://openai-01-dialexityswedencentral.openai.azure.com/'\n",
      "mock_response = None, mock_tool_calls = None, mock_timeout = None\n",
      "force_timeout = 600, logger_fn = None\n",
      "\n",
      "    @client\n",
      "    def completion(  # type: ignore # noqa: PLR0915\n",
      "        model: str,\n",
      "        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n",
      "        messages: List = [],\n",
      "        timeout: Optional[Union[float, str, httpx.Timeout]] = None,\n",
      "        temperature: Optional[float] = None,\n",
      "        top_p: Optional[float] = None,\n",
      "        n: Optional[int] = None,\n",
      "        stream: Optional[bool] = None,\n",
      "        stream_options: Optional[dict] = None,\n",
      "        stop=None,\n",
      "        max_completion_tokens: Optional[int] = None,\n",
      "        max_tokens: Optional[int] = None,\n",
      "        modalities: Optional[List[ChatCompletionModality]] = None,\n",
      "        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n",
      "        audio: Optional[ChatCompletionAudioParam] = None,\n",
      "        presence_penalty: Optional[float] = None,\n",
      "        frequency_penalty: Optional[float] = None,\n",
      "        logit_bias: Optional[dict] = None,\n",
      "        user: Optional[str] = None,\n",
      "        # openai v1.0+ new params\n",
      "        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n",
      "        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n",
      "        seed: Optional[int] = None,\n",
      "        tools: Optional[List] = None,\n",
      "        tool_choice: Optional[Union[str, dict]] = None,\n",
      "        logprobs: Optional[bool] = None,\n",
      "        top_logprobs: Optional[int] = None,\n",
      "        parallel_tool_calls: Optional[bool] = None,\n",
      "        web_search_options: Optional[OpenAIWebSearchOptions] = None,\n",
      "        deployment_id=None,\n",
      "        extra_headers: Optional[dict] = None,\n",
      "        # soon to be deprecated params by OpenAI\n",
      "        functions: Optional[List] = None,\n",
      "        function_call: Optional[str] = None,\n",
      "        # set api_base, api_version, api_key\n",
      "        base_url: Optional[str] = None,\n",
      "        api_version: Optional[str] = None,\n",
      "        api_key: Optional[str] = None,\n",
      "        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n",
      "        # Optional liteLLM function params\n",
      "        thinking: Optional[AnthropicThinkingParam] = None,\n",
      "        **kwargs,\n",
      "    ) -> Union[ModelResponse, CustomStreamWrapper]:\n",
      "        \"\"\"\n",
      "        Perform a completion() using any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n",
      "        Parameters:\n",
      "            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n",
      "            messages (List): A list of message objects representing the conversation context (default is an empty list).\n",
      "    \n",
      "            OPTIONAL PARAMS\n",
      "            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n",
      "            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n",
      "            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n",
      "            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n",
      "            n (int, optional): The number of completions to generate (default is 1).\n",
      "            stream (bool, optional): If True, return a streaming response (default is False).\n",
      "            stream_options (dict, optional): A dictionary containing options for the streaming response. Only set this when you set stream: true.\n",
      "            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n",
      "            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n",
      "            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n",
      "            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request.. You can use `[\"text\", \"audio\"]`\n",
      "            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n",
      "            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n",
      "            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n",
      "            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n",
      "            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n",
      "            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n",
      "            logprobs (bool, optional): Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message\n",
      "            top_logprobs (int, optional): An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.\n",
      "            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n",
      "            api_base (str, optional): Base URL for the API (default is None).\n",
      "            api_version (str, optional): API version (default is None).\n",
      "            api_key (str, optional): API key (default is None).\n",
      "            model_list (list, optional): List of api base, version, keys\n",
      "            extra_headers (dict, optional): Additional headers to include in the request.\n",
      "    \n",
      "            LITELLM Specific Params\n",
      "            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n",
      "            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n",
      "            max_retries (int, optional): The number of retries to attempt (default is 0).\n",
      "        Returns:\n",
      "            ModelResponse: A response object containing the generated completion and associated metadata.\n",
      "    \n",
      "        Note:\n",
      "            - This function is used to perform completions() using the specified language model.\n",
      "            - It supports various optional parameters for customizing the completion behavior.\n",
      "            - If 'mock_response' is provided, a mock completion response is returned for testing or debugging.\n",
      "        \"\"\"\n",
      "        ### VALIDATE Request ###\n",
      "        if model is None:\n",
      "            raise ValueError(\"model param not passed in.\")\n",
      "        # validate messages\n",
      "        messages = validate_and_fix_openai_messages(messages=messages)\n",
      "        # validate tool_choice\n",
      "        tool_choice = validate_chat_completion_tool_choice(tool_choice=tool_choice)\n",
      "        ######### unpacking kwargs #####################\n",
      "        args = locals()\n",
      "        api_base = kwargs.get(\"api_base\", None)\n",
      "        mock_response = kwargs.get(\"mock_response\", None)\n",
      "        mock_tool_calls = kwargs.get(\"mock_tool_calls\", None)\n",
      "        mock_timeout = cast(Optional[bool], kwargs.get(\"mock_timeout\", None))\n",
      "        force_timeout = kwargs.get(\"force_timeout\", 600)  ## deprecated\n",
      "        logger_fn = kwargs.get(\"logger_fn\", None)\n",
      "        verbose = kwargs.get(\"verbose\", False)\n",
      "        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n",
      "        litellm_logging_obj = kwargs.get(\"litellm_logging_obj\", None)\n",
      "        id = kwargs.get(\"id\", None)\n",
      "        metadata = kwargs.get(\"metadata\", None)\n",
      "        model_info = kwargs.get(\"model_info\", None)\n",
      "        proxy_server_request = kwargs.get(\"proxy_server_request\", None)\n",
      "        fallbacks = kwargs.get(\"fallbacks\", None)\n",
      "        provider_specific_header = cast(\n",
      "            Optional[ProviderSpecificHeader], kwargs.get(\"provider_specific_header\", None)\n",
      "        )\n",
      "        headers = kwargs.get(\"headers\", None) or extra_headers\n",
      "    \n",
      "        ensure_alternating_roles: Optional[bool] = kwargs.get(\n",
      "            \"ensure_alternating_roles\", None\n",
      "        )\n",
      "        user_continue_message: Optional[ChatCompletionUserMessage] = kwargs.get(\n",
      "            \"user_continue_message\", None\n",
      "        )\n",
      "        assistant_continue_message: Optional[ChatCompletionAssistantMessage] = kwargs.get(\n",
      "            \"assistant_continue_message\", None\n",
      "        )\n",
      "        if headers is None:\n",
      "            headers = {}\n",
      "        if extra_headers is not None:\n",
      "            headers.update(extra_headers)\n",
      "        num_retries = kwargs.get(\n",
      "            \"num_retries\", None\n",
      "        )  ## alt. param for 'max_retries'. Use this to pass retries w/ instructor.\n",
      "        max_retries = kwargs.get(\"max_retries\", None)\n",
      "        cooldown_time = kwargs.get(\"cooldown_time\", None)\n",
      "        context_window_fallback_dict = kwargs.get(\"context_window_fallback_dict\", None)\n",
      "        organization = kwargs.get(\"organization\", None)\n",
      "        ### VERIFY SSL ###\n",
      "        ssl_verify = kwargs.get(\"ssl_verify\", None)\n",
      "        ### CUSTOM MODEL COST ###\n",
      "        input_cost_per_token = kwargs.get(\"input_cost_per_token\", None)\n",
      "        output_cost_per_token = kwargs.get(\"output_cost_per_token\", None)\n",
      "        input_cost_per_second = kwargs.get(\"input_cost_per_second\", None)\n",
      "        output_cost_per_second = kwargs.get(\"output_cost_per_second\", None)\n",
      "        ### CUSTOM PROMPT TEMPLATE ###\n",
      "        initial_prompt_value = kwargs.get(\"initial_prompt_value\", None)\n",
      "        roles = kwargs.get(\"roles\", None)\n",
      "        final_prompt_value = kwargs.get(\"final_prompt_value\", None)\n",
      "        bos_token = kwargs.get(\"bos_token\", None)\n",
      "        eos_token = kwargs.get(\"eos_token\", None)\n",
      "        preset_cache_key = kwargs.get(\"preset_cache_key\", None)\n",
      "        hf_model_name = kwargs.get(\"hf_model_name\", None)\n",
      "        supports_system_message = kwargs.get(\"supports_system_message\", None)\n",
      "        base_model = kwargs.get(\"base_model\", None)\n",
      "        ### DISABLE FLAGS ###\n",
      "        disable_add_transform_inline_image_block = kwargs.get(\n",
      "            \"disable_add_transform_inline_image_block\", None\n",
      "        )\n",
      "        ### TEXT COMPLETION CALLS ###\n",
      "        text_completion = kwargs.get(\"text_completion\", False)\n",
      "        atext_completion = kwargs.get(\"atext_completion\", False)\n",
      "        ### ASYNC CALLS ###\n",
      "        acompletion = kwargs.get(\"acompletion\", False)\n",
      "        client = kwargs.get(\"client\", None)\n",
      "        ### Admin Controls ###\n",
      "        no_log = kwargs.get(\"no-log\", False)\n",
      "        ### PROMPT MANAGEMENT ###\n",
      "        prompt_id = cast(Optional[str], kwargs.get(\"prompt_id\", None))\n",
      "        prompt_variables = cast(Optional[dict], kwargs.get(\"prompt_variables\", None))\n",
      "        ### COPY MESSAGES ### - related issue https://github.com/BerriAI/litellm/discussions/4489\n",
      "        messages = get_completion_messages(\n",
      "            messages=messages,\n",
      "            ensure_alternating_roles=ensure_alternating_roles or False,\n",
      "            user_continue_message=user_continue_message,\n",
      "            assistant_continue_message=assistant_continue_message,\n",
      "        )\n",
      "        ######## end of unpacking kwargs ###########\n",
      "        standard_openai_params = get_standard_openai_params(params=args)\n",
      "        non_default_params = get_non_default_completion_params(kwargs=kwargs)\n",
      "        litellm_params = {}  # used to prevent unbound var errors\n",
      "        ## PROMPT MANAGEMENT HOOKS ##\n",
      "        if isinstance(litellm_logging_obj, LiteLLMLoggingObj) and (\n",
      "            litellm_logging_obj.should_run_prompt_management_hooks(\n",
      "                prompt_id=prompt_id, non_default_params=non_default_params\n",
      "            )\n",
      "        ):\n",
      "            (\n",
      "                model,\n",
      "                messages,\n",
      "                optional_params,\n",
      "            ) = litellm_logging_obj.get_chat_completion_prompt(\n",
      "                model=model,\n",
      "                messages=messages,\n",
      "                non_default_params=non_default_params,\n",
      "                prompt_id=prompt_id,\n",
      "                prompt_variables=prompt_variables,\n",
      "                prompt_label=kwargs.get(\"prompt_label\", None),\n",
      "            )\n",
      "    \n",
      "        try:\n",
      "            if base_url is not None:\n",
      "                api_base = base_url\n",
      "            if num_retries is not None:\n",
      "                max_retries = num_retries\n",
      "            logging = litellm_logging_obj\n",
      "            fallbacks = fallbacks or litellm.model_fallbacks\n",
      "            if fallbacks is not None:\n",
      "                return completion_with_fallbacks(**args)\n",
      "            if model_list is not None:\n",
      "                deployments = [\n",
      "                    m[\"litellm_params\"] for m in model_list if m[\"model_name\"] == model\n",
      "                ]\n",
      "                return litellm.batch_completion_models(deployments=deployments, **args)\n",
      "            if litellm.model_alias_map and model in litellm.model_alias_map:\n",
      "                model = litellm.model_alias_map[\n",
      "                    model\n",
      "                ]  # update the model to the actual value if an alias has been passed in\n",
      "            model_response = ModelResponse()\n",
      "            setattr(model_response, \"usage\", litellm.Usage())\n",
      "            if (\n",
      "                kwargs.get(\"azure\", False) is True\n",
      "            ):  # don't remove flag check, to remain backwards compatible for repos like Codium\n",
      "                custom_llm_provider = \"azure\"\n",
      "            if deployment_id is not None:  # azure llms\n",
      "                model = deployment_id\n",
      "                custom_llm_provider = \"azure\"\n",
      "            model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n",
      "                model=model,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "                api_base=api_base,\n",
      "                api_key=api_key,\n",
      "            )\n",
      "    \n",
      "            if (\n",
      "                provider_specific_header is not None\n",
      "                and provider_specific_header[\"custom_llm_provider\"] == custom_llm_provider\n",
      "            ):\n",
      "                headers.update(provider_specific_header[\"extra_headers\"])\n",
      "    \n",
      "            if model_response is not None and hasattr(model_response, \"_hidden_params\"):\n",
      "                model_response._hidden_params[\"custom_llm_provider\"] = custom_llm_provider\n",
      "                model_response._hidden_params[\"region_name\"] = kwargs.get(\n",
      "                    \"aws_region_name\", None\n",
      "                )  # support region-based pricing for bedrock\n",
      "    \n",
      "            ### TIMEOUT LOGIC ###\n",
      "            timeout = timeout or kwargs.get(\"request_timeout\", 600) or 600\n",
      "            # set timeout for 10 minutes by default\n",
      "            if isinstance(timeout, httpx.Timeout) and not supports_httpx_timeout(\n",
      "                custom_llm_provider\n",
      "            ):\n",
      "                timeout = timeout.read or 600  # default 10 min timeout\n",
      "            elif not isinstance(timeout, httpx.Timeout):\n",
      "                timeout = float(timeout)  # type: ignore\n",
      "    \n",
      "            ### REGISTER CUSTOM MODEL PRICING -- IF GIVEN ###\n",
      "            if input_cost_per_token is not None and output_cost_per_token is not None:\n",
      "                litellm.register_model(\n",
      "                    {\n",
      "                        f\"{custom_llm_provider}/{model}\": {\n",
      "                            \"input_cost_per_token\": input_cost_per_token,\n",
      "                            \"output_cost_per_token\": output_cost_per_token,\n",
      "                            \"litellm_provider\": custom_llm_provider,\n",
      "                        }\n",
      "                    }\n",
      "                )\n",
      "            elif (\n",
      "                input_cost_per_second is not None\n",
      "            ):  # time based pricing just needs cost in place\n",
      "                output_cost_per_second = output_cost_per_second\n",
      "                litellm.register_model(\n",
      "                    {\n",
      "                        f\"{custom_llm_provider}/{model}\": {\n",
      "                            \"input_cost_per_second\": input_cost_per_second,\n",
      "                            \"output_cost_per_second\": output_cost_per_second,\n",
      "                            \"litellm_provider\": custom_llm_provider,\n",
      "                        }\n",
      "                    }\n",
      "                )\n",
      "            ### BUILD CUSTOM PROMPT TEMPLATE -- IF GIVEN ###\n",
      "            custom_prompt_dict = {}  # type: ignore\n",
      "            if (\n",
      "                initial_prompt_value\n",
      "                or roles\n",
      "                or final_prompt_value\n",
      "                or bos_token\n",
      "                or eos_token\n",
      "            ):\n",
      "                custom_prompt_dict = {model: {}}\n",
      "                if initial_prompt_value:\n",
      "                    custom_prompt_dict[model][\"initial_prompt_value\"] = initial_prompt_value\n",
      "                if roles:\n",
      "                    custom_prompt_dict[model][\"roles\"] = roles\n",
      "                if final_prompt_value:\n",
      "                    custom_prompt_dict[model][\"final_prompt_value\"] = final_prompt_value\n",
      "                if bos_token:\n",
      "                    custom_prompt_dict[model][\"bos_token\"] = bos_token\n",
      "                if eos_token:\n",
      "                    custom_prompt_dict[model][\"eos_token\"] = eos_token\n",
      "    \n",
      "            if kwargs.get(\"model_file_id_mapping\"):\n",
      "                messages = update_messages_with_model_file_ids(\n",
      "                    messages=messages,\n",
      "                    model_id=kwargs.get(\"model_info\", {}).get(\"id\", None),\n",
      "                    model_file_id_mapping=cast(\n",
      "                        Dict[str, Dict[str, str]], kwargs.get(\"model_file_id_mapping\")\n",
      "                    ),\n",
      "                )\n",
      "    \n",
      "            provider_config: Optional[BaseConfig] = None\n",
      "            if custom_llm_provider is not None and custom_llm_provider in [\n",
      "                provider.value for provider in LlmProviders\n",
      "            ]:\n",
      "                provider_config = ProviderConfigManager.get_provider_chat_config(\n",
      "                    model=model, provider=LlmProviders(custom_llm_provider)\n",
      "                )\n",
      "    \n",
      "            if provider_config is not None:\n",
      "                messages = provider_config.translate_developer_role_to_system_role(\n",
      "                    messages=messages\n",
      "                )\n",
      "    \n",
      "            if (\n",
      "                supports_system_message is not None\n",
      "                and isinstance(supports_system_message, bool)\n",
      "                and supports_system_message is False\n",
      "            ):\n",
      "                messages = map_system_message_pt(messages=messages)\n",
      "    \n",
      "            if dynamic_api_key is not None:\n",
      "                api_key = dynamic_api_key\n",
      "            # check if user passed in any of the OpenAI optional params\n",
      "            optional_param_args = {\n",
      "                \"functions\": functions,\n",
      "                \"function_call\": function_call,\n",
      "                \"temperature\": temperature,\n",
      "                \"top_p\": top_p,\n",
      "                \"n\": n,\n",
      "                \"stream\": stream,\n",
      "                \"stream_options\": stream_options,\n",
      "                \"stop\": stop,\n",
      "                \"max_tokens\": max_tokens,\n",
      "                \"max_completion_tokens\": max_completion_tokens,\n",
      "                \"modalities\": modalities,\n",
      "                \"prediction\": prediction,\n",
      "                \"audio\": audio,\n",
      "                \"presence_penalty\": presence_penalty,\n",
      "                \"frequency_penalty\": frequency_penalty,\n",
      "                \"logit_bias\": logit_bias,\n",
      "                \"user\": user,\n",
      "                # params to identify the model\n",
      "                \"model\": model,\n",
      "                \"custom_llm_provider\": custom_llm_provider,\n",
      "                \"response_format\": response_format,\n",
      "                \"seed\": seed,\n",
      "                \"tools\": tools,\n",
      "                \"tool_choice\": tool_choice,\n",
      "                \"max_retries\": max_retries,\n",
      "                \"logprobs\": logprobs,\n",
      "                \"top_logprobs\": top_logprobs,\n",
      "                \"api_version\": api_version,\n",
      "                \"parallel_tool_calls\": parallel_tool_calls,\n",
      "                \"messages\": messages,\n",
      "                \"reasoning_effort\": reasoning_effort,\n",
      "                \"thinking\": thinking,\n",
      "                \"web_search_options\": web_search_options,\n",
      "                \"allowed_openai_params\": kwargs.get(\"allowed_openai_params\"),\n",
      "            }\n",
      "            optional_params = get_optional_params(\n",
      "                **optional_param_args, **non_default_params\n",
      "            )\n",
      "            processed_non_default_params = pre_process_non_default_params(\n",
      "                model=model,\n",
      "                passed_params=optional_param_args,\n",
      "                special_params=non_default_params,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "                additional_drop_params=kwargs.get(\"additional_drop_params\"),\n",
      "            )\n",
      "            processed_non_default_params = add_provider_specific_params_to_optional_params(\n",
      "                optional_params=processed_non_default_params,\n",
      "                passed_params=non_default_params,\n",
      "            )\n",
      "    \n",
      "            if litellm.add_function_to_prompt and optional_params.get(\n",
      "                \"functions_unsupported_model\", None\n",
      "            ):  # if user opts to add it to prompt, when API doesn't support function calling\n",
      "                functions_unsupported_model = optional_params.pop(\n",
      "                    \"functions_unsupported_model\"\n",
      "                )\n",
      "                messages = function_call_prompt(\n",
      "                    messages=messages, functions=functions_unsupported_model\n",
      "                )\n",
      "    \n",
      "            # For logging - save the values of the litellm-specific params passed in\n",
      "            litellm_params = get_litellm_params(\n",
      "                acompletion=acompletion,\n",
      "                api_key=api_key,\n",
      "                force_timeout=force_timeout,\n",
      "                logger_fn=logger_fn,\n",
      "                verbose=verbose,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "                api_base=api_base,\n",
      "                litellm_call_id=kwargs.get(\"litellm_call_id\", None),\n",
      "                model_alias_map=litellm.model_alias_map,\n",
      "                completion_call_id=id,\n",
      "                metadata=metadata,\n",
      "                model_info=model_info,\n",
      "                proxy_server_request=proxy_server_request,\n",
      "                preset_cache_key=preset_cache_key,\n",
      "                no_log=no_log,\n",
      "                input_cost_per_second=input_cost_per_second,\n",
      "                input_cost_per_token=input_cost_per_token,\n",
      "                output_cost_per_second=output_cost_per_second,\n",
      "                output_cost_per_token=output_cost_per_token,\n",
      "                cooldown_time=cooldown_time,\n",
      "                text_completion=kwargs.get(\"text_completion\"),\n",
      "                azure_ad_token_provider=kwargs.get(\"azure_ad_token_provider\"),\n",
      "                user_continue_message=kwargs.get(\"user_continue_message\"),\n",
      "                base_model=base_model,\n",
      "                litellm_trace_id=kwargs.get(\"litellm_trace_id\"),\n",
      "                litellm_session_id=kwargs.get(\"litellm_session_id\"),\n",
      "                hf_model_name=hf_model_name,\n",
      "                custom_prompt_dict=custom_prompt_dict,\n",
      "                litellm_metadata=kwargs.get(\"litellm_metadata\"),\n",
      "                disable_add_transform_inline_image_block=disable_add_transform_inline_image_block,\n",
      "                drop_params=kwargs.get(\"drop_params\"),\n",
      "                prompt_id=prompt_id,\n",
      "                prompt_variables=prompt_variables,\n",
      "                ssl_verify=ssl_verify,\n",
      "                merge_reasoning_content_in_choices=kwargs.get(\n",
      "                    \"merge_reasoning_content_in_choices\", None\n",
      "                ),\n",
      "                use_litellm_proxy=kwargs.get(\"use_litellm_proxy\", False),\n",
      "                api_version=api_version,\n",
      "                azure_ad_token=kwargs.get(\"azure_ad_token\"),\n",
      "                tenant_id=kwargs.get(\"tenant_id\"),\n",
      "                client_id=kwargs.get(\"client_id\"),\n",
      "                client_secret=kwargs.get(\"client_secret\"),\n",
      "                azure_username=kwargs.get(\"azure_username\"),\n",
      "                azure_password=kwargs.get(\"azure_password\"),\n",
      "                max_retries=max_retries,\n",
      "                timeout=timeout,\n",
      "            )\n",
      "            cast(LiteLLMLoggingObj, logging).update_environment_variables(\n",
      "                model=model,\n",
      "                user=user,\n",
      "                optional_params=processed_non_default_params,  # [IMPORTANT] - using processed_non_default_params ensures consistent params logged to langfuse for finetuning / eval datasets.\n",
      "                litellm_params=litellm_params,\n",
      "                custom_llm_provider=custom_llm_provider,\n",
      "            )\n",
      "            if mock_response or mock_tool_calls or mock_timeout:\n",
      "                kwargs.pop(\"mock_timeout\", None)  # remove for any fallbacks triggered\n",
      "                return mock_completion(\n",
      "                    model,\n",
      "                    messages,\n",
      "                    stream=stream,\n",
      "                    n=n,\n",
      "                    mock_response=mock_response,\n",
      "                    mock_tool_calls=mock_tool_calls,\n",
      "                    logging=logging,\n",
      "                    acompletion=acompletion,\n",
      "                    mock_delay=kwargs.get(\"mock_delay\", None),\n",
      "                    custom_llm_provider=custom_llm_provider,\n",
      "                    mock_timeout=mock_timeout,\n",
      "                    timeout=timeout,\n",
      "                )\n",
      "    \n",
      "            if custom_llm_provider == \"azure\":\n",
      "                # azure configs\n",
      "                ## check dynamic params ##\n",
      "                dynamic_params = False\n",
      "                if client is not None and (\n",
      "                    isinstance(client, openai.AzureOpenAI)\n",
      "                    or isinstance(client, openai.AsyncAzureOpenAI)\n",
      "                ):\n",
      "                    dynamic_params = _check_dynamic_azure_params(\n",
      "                        azure_client_params={\"api_version\": api_version},\n",
      "                        azure_client=client,\n",
      "                    )\n",
      "    \n",
      "                api_type = get_secret(\"AZURE_API_TYPE\") or \"azure\"\n",
      "    \n",
      "                api_base = api_base or litellm.api_base or get_secret(\"AZURE_API_BASE\")\n",
      "    \n",
      "                api_version = (\n",
      "                    api_version\n",
      "                    or litellm.api_version\n",
      "                    or get_secret(\"AZURE_API_VERSION\")\n",
      "                    or litellm.AZURE_DEFAULT_API_VERSION\n",
      "                )\n",
      "    \n",
      "                api_key = (\n",
      "                    api_key\n",
      "                    or litellm.api_key\n",
      "                    or litellm.azure_key\n",
      "                    or get_secret(\"AZURE_OPENAI_API_KEY\")\n",
      "                    or get_secret(\"AZURE_API_KEY\")\n",
      "                )\n",
      "    \n",
      "                azure_ad_token = optional_params.get(\"extra_body\", {}).pop(\n",
      "                    \"azure_ad_token\", None\n",
      "                ) or get_secret(\"AZURE_AD_TOKEN\")\n",
      "    \n",
      "                azure_ad_token_provider = litellm_params.get(\n",
      "                    \"azure_ad_token_provider\", None\n",
      "                )\n",
      "    \n",
      "                headers = headers or litellm.headers\n",
      "    \n",
      "                if extra_headers is not None:\n",
      "                    optional_params[\"extra_headers\"] = extra_headers\n",
      "                if max_retries is not None:\n",
      "                    optional_params[\"max_retries\"] = max_retries\n",
      "    \n",
      "                if litellm.AzureOpenAIO1Config().is_o_series_model(model=model):\n",
      "                    ## LOAD CONFIG - if set\n",
      "                    config = litellm.AzureOpenAIO1Config.get_config()\n",
      "                    for k, v in config.items():\n",
      "                        if (\n",
      "                            k not in optional_params\n",
      "                        ):  # completion(top_k=3) > azure_config(top_k=3) <- allows for dynamic variables to be passed in\n",
      "                            optional_params[k] = v\n",
      "    \n",
      "                    response = azure_o1_chat_completions.completion(\n",
      "                        model=model,\n",
      "                        messages=messages,\n",
      "                        headers=headers,\n",
      "                        api_key=api_key,\n",
      "                        api_base=api_base,\n",
      "                        api_version=api_version,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        model_response=model_response,\n",
      "                        print_verbose=print_verbose,\n",
      "                        optional_params=optional_params,\n",
      "                        litellm_params=litellm_params,\n",
      "                        logger_fn=logger_fn,\n",
      "                        logging_obj=logging,\n",
      "                        acompletion=acompletion,\n",
      "                        timeout=timeout,  # type: ignore\n",
      "                        client=client,  # pass AsyncAzureOpenAI, AzureOpenAI client\n",
      "                        custom_llm_provider=custom_llm_provider,\n",
      "                    )\n",
      "                else:\n",
      "                    ## LOAD CONFIG - if set\n",
      "                    config = litellm.AzureOpenAIConfig.get_config()\n",
      "                    for k, v in config.items():\n",
      "                        if (\n",
      "                            k not in optional_params\n",
      "                        ):  # completion(top_k=3) > azure_config(top_k=3) <- allows for dynamic variables to be passed in\n",
      "                            optional_params[k] = v\n",
      "    \n",
      "                    ## COMPLETION CALL\n",
      ">                   response = azure_chat_completions.completion(\n",
      "                        model=model,\n",
      "                        messages=messages,\n",
      "                        headers=headers,\n",
      "                        api_key=api_key,\n",
      "                        api_base=api_base,\n",
      "                        api_version=api_version,\n",
      "                        api_type=api_type,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        model_response=model_response,\n",
      "                        print_verbose=print_verbose,\n",
      "                        optional_params=optional_params,\n",
      "                        litellm_params=litellm_params,\n",
      "                        logger_fn=logger_fn,\n",
      "                        logging_obj=logging,\n",
      "                        acompletion=acompletion,\n",
      "                        timeout=timeout,  # type: ignore\n",
      "                        client=client,  # pass AsyncAzureOpenAI, AzureOpenAI client\n",
      "                    )\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/main.py\u001b[0m:1366: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <litellm.llms.azure.azure.AzureChatCompletion object at 0x10db8dd30>\n",
      "model = 'gpt-4-turbo'\n",
      "messages = [{'content': '<context>Putin started the war, Ukraine will not surrender and will finally win!</context>', 'role': 'us...on\\'t mention any special denotations such as \"T\", \"T1\", \"T+\", \"A-\", \"Ac\", \"Re\", etc.\\n</formatting>', 'role': 'user'}]\n",
      "model_response = ModelResponse(id='chatcmpl-3286120d-6a0f-486c-ab5c-79f261c09663', created=1748418122, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "api_key = '830bfa7d0fd34a5dae45e1a1d8f879de'\n",
      "api_base = 'https://openai-01-dialexityswedencentral.openai.azure.com/'\n",
      "api_version = '2024-10-21DIALEXITY_DEFAULT_MODEL=gpt-4', api_type = 'azure'\n",
      "azure_ad_token = None, azure_ad_token_provider = None, dynamic_params = False\n",
      "print_verbose = <function print_verbose at 0x10dbc3e20>, timeout = 600.0\n",
      "logging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ed4b890>\n",
      "optional_params = {'extra_body': {}, 'stream': False, 'tool_choice': 'required', 'tools': [{'function': {'description': 'Correctly forma..., 'properties': {'dialectical_components': {...}}, 'required': ['dialectical_components'], ...}}, 'type': 'function'}]}\n",
      "litellm_params = {'acompletion': False, 'aembedding': None, 'api_base': None, 'api_key': None, ...}\n",
      "logger_fn = None, acompletion = False, headers = None, client = None\n",
      "\n",
      "    def completion(  # noqa: PLR0915\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: list,\n",
      "        model_response: ModelResponse,\n",
      "        api_key: str,\n",
      "        api_base: str,\n",
      "        api_version: str,\n",
      "        api_type: str,\n",
      "        azure_ad_token: str,\n",
      "        azure_ad_token_provider: Callable,\n",
      "        dynamic_params: bool,\n",
      "        print_verbose: Callable,\n",
      "        timeout: Union[float, httpx.Timeout],\n",
      "        logging_obj: LiteLLMLoggingObj,\n",
      "        optional_params,\n",
      "        litellm_params,\n",
      "        logger_fn,\n",
      "        acompletion: bool = False,\n",
      "        headers: Optional[dict] = None,\n",
      "        client=None,\n",
      "    ):\n",
      "        if headers:\n",
      "            optional_params[\"extra_headers\"] = headers\n",
      "        try:\n",
      "            if model is None or messages is None:\n",
      "                raise AzureOpenAIError(\n",
      "                    status_code=422, message=\"Missing model or messages\"\n",
      "                )\n",
      "    \n",
      "            max_retries = optional_params.pop(\"max_retries\", None)\n",
      "            if max_retries is None:\n",
      "                max_retries = DEFAULT_MAX_RETRIES\n",
      "            json_mode: Optional[bool] = optional_params.pop(\"json_mode\", False)\n",
      "    \n",
      "            ### CHECK IF CLOUDFLARE AI GATEWAY ###\n",
      "            ### if so - set the model as part of the base url\n",
      "            if \"gateway.ai.cloudflare.com\" in api_base:\n",
      "                client = self._init_azure_client_for_cloudflare_ai_gateway(\n",
      "                    api_base=api_base,\n",
      "                    model=model,\n",
      "                    api_version=api_version,\n",
      "                    max_retries=max_retries,\n",
      "                    timeout=timeout,\n",
      "                    api_key=api_key,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    acompletion=acompletion,\n",
      "                    client=client,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "    \n",
      "                data = {\"model\": None, \"messages\": messages, **optional_params}\n",
      "            else:\n",
      "                data = litellm.AzureOpenAIConfig().transform_request(\n",
      "                    model=model,\n",
      "                    messages=messages,\n",
      "                    optional_params=optional_params,\n",
      "                    litellm_params=litellm_params,\n",
      "                    headers=headers or {},\n",
      "                )\n",
      "    \n",
      "            if acompletion is True:\n",
      "                if optional_params.get(\"stream\", False):\n",
      "                    return self.async_streaming(\n",
      "                        logging_obj=logging_obj,\n",
      "                        api_base=api_base,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        data=data,\n",
      "                        model=model,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        max_retries=max_retries,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "                else:\n",
      "                    return self.acompletion(\n",
      "                        api_base=api_base,\n",
      "                        data=data,\n",
      "                        model_response=model_response,\n",
      "                        api_key=api_key,\n",
      "                        api_version=api_version,\n",
      "                        model=model,\n",
      "                        azure_ad_token=azure_ad_token,\n",
      "                        azure_ad_token_provider=azure_ad_token_provider,\n",
      "                        dynamic_params=dynamic_params,\n",
      "                        timeout=timeout,\n",
      "                        client=client,\n",
      "                        logging_obj=logging_obj,\n",
      "                        max_retries=max_retries,\n",
      "                        convert_tool_call_to_json_mode=json_mode,\n",
      "                        litellm_params=litellm_params,\n",
      "                    )\n",
      "            elif \"stream\" in optional_params and optional_params[\"stream\"] is True:\n",
      "                return self.streaming(\n",
      "                    logging_obj=logging_obj,\n",
      "                    api_base=api_base,\n",
      "                    dynamic_params=dynamic_params,\n",
      "                    data=data,\n",
      "                    model=model,\n",
      "                    api_key=api_key,\n",
      "                    api_version=api_version,\n",
      "                    azure_ad_token=azure_ad_token,\n",
      "                    azure_ad_token_provider=azure_ad_token_provider,\n",
      "                    timeout=timeout,\n",
      "                    client=client,\n",
      "                    max_retries=max_retries,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "            else:\n",
      "                ## LOGGING\n",
      "                logging_obj.pre_call(\n",
      "                    input=messages,\n",
      "                    api_key=api_key,\n",
      "                    additional_args={\n",
      "                        \"headers\": {\n",
      "                            \"api_key\": api_key,\n",
      "                            \"azure_ad_token\": azure_ad_token,\n",
      "                        },\n",
      "                        \"api_version\": api_version,\n",
      "                        \"api_base\": api_base,\n",
      "                        \"complete_input_dict\": data,\n",
      "                    },\n",
      "                )\n",
      "                if not isinstance(max_retries, int):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=422, message=\"max retries must be an int\"\n",
      "                    )\n",
      "                # init AzureOpenAI Client\n",
      "                azure_client = self.get_azure_openai_client(\n",
      "                    api_version=api_version,\n",
      "                    api_base=api_base,\n",
      "                    api_key=api_key,\n",
      "                    model=model,\n",
      "                    client=client,\n",
      "                    _is_async=False,\n",
      "                    litellm_params=litellm_params,\n",
      "                )\n",
      "                if not isinstance(azure_client, AzureOpenAI):\n",
      "                    raise AzureOpenAIError(\n",
      "                        status_code=500,\n",
      "                        message=\"azure_client is not an instance of AzureOpenAI\",\n",
      "                    )\n",
      "    \n",
      "                headers, response = self.make_sync_azure_openai_chat_completion_request(\n",
      "                    azure_client=azure_client, data=data, timeout=timeout\n",
      "                )\n",
      "                stringified_response = response.model_dump()\n",
      "                ## LOGGING\n",
      "                logging_obj.post_call(\n",
      "                    input=messages,\n",
      "                    api_key=api_key,\n",
      "                    original_response=stringified_response,\n",
      "                    additional_args={\n",
      "                        \"headers\": headers,\n",
      "                        \"api_version\": api_version,\n",
      "                        \"api_base\": api_base,\n",
      "                    },\n",
      "                )\n",
      "                return convert_to_model_response_object(\n",
      "                    response_object=stringified_response,\n",
      "                    model_response_object=model_response,\n",
      "                    convert_tool_call_to_json_mode=json_mode,\n",
      "                    _response_headers=headers,\n",
      "                )\n",
      "        except AzureOpenAIError as e:\n",
      "            raise e\n",
      "        except Exception as e:\n",
      "            status_code = getattr(e, \"status_code\", 500)\n",
      "            error_headers = getattr(e, \"headers\", None)\n",
      "            error_response = getattr(e, \"response\", None)\n",
      "            error_body = getattr(e, \"body\", None)\n",
      "            if error_headers is None and error_response:\n",
      "                error_headers = getattr(error_response, \"headers\", None)\n",
      ">           raise AzureOpenAIError(\n",
      "                status_code=status_code,\n",
      "                message=str(e),\n",
      "                headers=error_headers,\n",
      "                body=error_body,\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           litellm.llms.azure.common_utils.AzureOpenAIError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/llms/azure/azure.py\u001b[0m:358: AzureOpenAIError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "    @observe()\n",
      "    def test_wheel_4():\n",
      "        number_of_thoughts = 4\n",
      ">       wheels = asyncio.run(factory.build(theses=[None] * number_of_thoughts))\n",
      "\n",
      "\u001b[1m\u001b[31mtest_analyst.py\u001b[0m:118: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py\u001b[0m:195: in run\n",
      "    return runner.run(main)\n",
      "\u001b[1m\u001b[31m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py\u001b[0m:118: in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "\u001b[1m\u001b[31m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py\u001b[0m:719: in run_until_complete\n",
      "    return future.result()\n",
      "\u001b[1m\u001b[31m../src/dialectical_framework/synthesist/factories/wheel_builder.py\u001b[0m:59: in build\n",
      "    cycles: List[Cycle] = await analyst.extract(wu_count)\n",
      "\u001b[1m\u001b[31m../src/dialectical_framework/analyst/thought_mapping.py\u001b[0m:299: in extract\n",
      "    box = await self.find_multiple(prompt_stuff=prompt_stuff)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/langfuse/decorators/langfuse_decorator.py\u001b[0m:219: in async_wrapper\n",
      "    self._handle_exception(observation, e)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/langfuse/decorators/langfuse_decorator.py\u001b[0m:520: in _handle_exception\n",
      "    raise e\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/langfuse/decorators/langfuse_decorator.py\u001b[0m:217: in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/integrations/_middleware_factory.py\u001b[0m:146: in wrapper_async\n",
      "    result = await fn(*args, **kwargs)  # pyright: ignore [reportGeneralTypeIssues]\n",
      "\u001b[1m\u001b[31m../src/dialectical_framework/analyst/thought_mapping.py\u001b[0m:141: in find_multiple\n",
      "    return _find_multiple_call()\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/llm/_call.py\u001b[0m:346: in inner\n",
      "    result = decorated(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/core/base/_extract.py\u001b[0m:158: in inner\n",
      "    call_response = create_decorator(fn=fn, **create_decorator_kwargs)(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/mirascope/core/base/_create.py\u001b[0m:232: in inner\n",
      "    response = create(stream=False, **call_kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/utils.py\u001b[0m:1283: in wrapper\n",
      "    raise e\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/utils.py\u001b[0m:1161: in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/main.py\u001b[0m:3241: in completion\n",
      "    raise exception_type(\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m:2239: in exception_type\n",
      "    raise e\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "model = 'gpt-4-turbo'\n",
      "original_exception = AzureOpenAIError(\"Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\")\n",
      "custom_llm_provider = 'azure'\n",
      "completion_kwargs = {'api_key': None, 'api_version': None, 'audio': None, 'base_url': None, ...}\n",
      "extra_kwargs = {'litellm_call_id': '9c65165a-ff1d-4e1b-bd69-aed0ec2491ff', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10ed4b890>}\n",
      "\n",
      "    def exception_type(  # type: ignore  # noqa: PLR0915\n",
      "        model,\n",
      "        original_exception,\n",
      "        custom_llm_provider,\n",
      "        completion_kwargs={},\n",
      "        extra_kwargs={},\n",
      "    ):\n",
      "        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n",
      "        if any(\n",
      "            isinstance(original_exception, exc_type)\n",
      "            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n",
      "        ):\n",
      "            return original_exception\n",
      "        exception_mapping_worked = False\n",
      "        exception_provider = custom_llm_provider\n",
      "        if litellm.suppress_debug_info is False:\n",
      "            print()  # noqa\n",
      "            print(  # noqa\n",
      "                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n",
      "            )  # noqa\n",
      "            print(  # noqa\n",
      "                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n",
      "            )  # noqa\n",
      "            print()  # noqa\n",
      "    \n",
      "        litellm_response_headers = _get_response_headers(\n",
      "            original_exception=original_exception\n",
      "        )\n",
      "        try:\n",
      "            error_str = str(original_exception)\n",
      "            if model:\n",
      "                if hasattr(original_exception, \"message\"):\n",
      "                    error_str = str(original_exception.message)\n",
      "                if isinstance(original_exception, BaseException):\n",
      "                    exception_type = type(original_exception).__name__\n",
      "                else:\n",
      "                    exception_type = \"\"\n",
      "    \n",
      "                ################################################################################\n",
      "                # Common Extra information needed for all providers\n",
      "                # We pass num retries, api_base, vertex_deployment etc to the exception here\n",
      "                ################################################################################\n",
      "                extra_information = \"\"\n",
      "                try:\n",
      "                    _api_base = litellm.get_api_base(\n",
      "                        model=model, optional_params=extra_kwargs\n",
      "                    )\n",
      "                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n",
      "                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n",
      "                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n",
      "                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n",
      "                    _model_group = _metadata.get(\"model_group\")\n",
      "                    _deployment = _metadata.get(\"deployment\")\n",
      "                    extra_information = f\"\\nModel: {model}\"\n",
      "    \n",
      "                    if (\n",
      "                        isinstance(custom_llm_provider, str)\n",
      "                        and len(custom_llm_provider) > 0\n",
      "                    ):\n",
      "                        exception_provider = (\n",
      "                            custom_llm_provider[0].upper()\n",
      "                            + custom_llm_provider[1:]\n",
      "                            + \"Exception\"\n",
      "                        )\n",
      "    \n",
      "                    if _api_base:\n",
      "                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n",
      "                    if (\n",
      "                        messages\n",
      "                        and len(messages) > 0\n",
      "                        and litellm.redact_messages_in_exceptions is False\n",
      "                    ):\n",
      "                        extra_information += f\"\\nMessages: `{messages}`\"\n",
      "    \n",
      "                    if _model_group is not None:\n",
      "                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n",
      "                    if _deployment is not None:\n",
      "                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n",
      "                    if _vertex_project is not None:\n",
      "                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n",
      "                    if _vertex_location is not None:\n",
      "                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n",
      "    \n",
      "                    # on litellm proxy add key name + team to exceptions\n",
      "                    extra_information = _add_key_name_and_team_to_alert(\n",
      "                        request_info=extra_information, metadata=_metadata\n",
      "                    )\n",
      "                except Exception:\n",
      "                    # DO NOT LET this Block raising the original exception\n",
      "                    pass\n",
      "    \n",
      "                ################################################################################\n",
      "                # End of Common Extra information Needed for all providers\n",
      "                ################################################################################\n",
      "    \n",
      "                ################################################################################\n",
      "                #################### Start of Provider Exception mapping ####################\n",
      "                ################################################################################\n",
      "    \n",
      "                if (\n",
      "                    \"Request Timeout Error\" in error_str\n",
      "                    or \"Request timed out\" in error_str\n",
      "                    or \"Timed out generating response\" in error_str\n",
      "                    or \"The read operation timed out\" in error_str\n",
      "                ):\n",
      "                    exception_mapping_worked = True\n",
      "    \n",
      "                    raise Timeout(\n",
      "                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n",
      "                        model=model,\n",
      "                        llm_provider=custom_llm_provider,\n",
      "                        litellm_debug_info=extra_information,\n",
      "                    )\n",
      "    \n",
      "                if (\n",
      "                    custom_llm_provider == \"litellm_proxy\"\n",
      "                ):  # handle special case where calling litellm proxy + exception str contains error message\n",
      "                    extract_and_raise_litellm_exception(\n",
      "                        response=getattr(original_exception, \"response\", None),\n",
      "                        error_str=error_str,\n",
      "                        model=model,\n",
      "                        custom_llm_provider=custom_llm_provider,\n",
      "                    )\n",
      "                if (\n",
      "                    custom_llm_provider == \"openai\"\n",
      "                    or custom_llm_provider == \"text-completion-openai\"\n",
      "                    or custom_llm_provider == \"custom_openai\"\n",
      "                    or custom_llm_provider in litellm.openai_compatible_providers\n",
      "                ):\n",
      "                    # custom_llm_provider is openai, make it OpenAI\n",
      "                    message = get_error_message(error_obj=original_exception)\n",
      "                    if message is None:\n",
      "                        if hasattr(original_exception, \"message\"):\n",
      "                            message = original_exception.message\n",
      "                        else:\n",
      "                            message = str(original_exception)\n",
      "    \n",
      "                    if message is not None and isinstance(\n",
      "                        message, str\n",
      "                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n",
      "                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n",
      "                        message = message.replace(\n",
      "                            \"openai.OpenAIError\",\n",
      "                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n",
      "                        )\n",
      "                    if custom_llm_provider == \"openai\":\n",
      "                        exception_provider = \"OpenAI\" + \"Exception\"\n",
      "                    else:\n",
      "                        exception_provider = (\n",
      "                            custom_llm_provider[0].upper()\n",
      "                            + custom_llm_provider[1:]\n",
      "                            + \"Exception\"\n",
      "                        )\n",
      "    \n",
      "                    if \"429\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"RateLimitError: {exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"This model's maximum context length is\" in error_str\n",
      "                        or \"string too long. Expected a string with maximum length\"\n",
      "                        in error_str\n",
      "                        or \"model's maximum context limit\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"invalid_request_error\" in error_str\n",
      "                        and \"model_not_found\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise NotFoundError(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"A timeout occurred\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        (\n",
      "                            \"invalid_request_error\" in error_str\n",
      "                            and \"content_policy_violation\" in error_str\n",
      "                        )\n",
      "                        or (\n",
      "                            \"Invalid prompt\" in error_str\n",
      "                            and \"violating our usage policy\" in error_str\n",
      "                        )\n",
      "                        or (\n",
      "                            \"request was rejected as a result of the safety system\"\n",
      "                            in error_str.lower()\n",
      "                        )\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"invalid_request_error\" in error_str\n",
      "                        and \"Incorrect API key provided\" not in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            body=getattr(original_exception, \"body\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Web server is returning an unknown error\" in error_str\n",
      "                        or \"The server had an error processing your request.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                        )\n",
      "                    elif \"Request too large\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"RateLimitError: {exception_provider} - {message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"Mistral API raised a streaming error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        _request = httpx.Request(\n",
      "                            method=\"POST\", url=\"https://api.openai.com/v1\"\n",
      "                        )\n",
      "                        raise APIError(\n",
      "                            status_code=500,\n",
      "                            message=f\"{exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            request=_request,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        exception_mapping_worked = True\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{exception_provider} - {message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"NotFoundError: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"Timeout Error: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                body=getattr(original_exception, \"body\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"RateLimitError: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"Timeout Error: {exception_provider} - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"APIError: {exception_provider} - {message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                request=getattr(original_exception, \"request\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                    else:\n",
      "                        # if no status code then it is an APIConnectionError: https://github.com/openai/openai-python#handling-errors\n",
      "                        # exception_mapping_worked = True\n",
      "                        raise APIConnectionError(\n",
      "                            message=f\"APIConnectionError: {exception_provider} - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            request=httpx.Request(\n",
      "                                method=\"POST\", url=\"https://api.openai.com/v1/\"\n",
      "                            ),\n",
      "                        )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"anthropic\"\n",
      "                    or custom_llm_provider == \"anthropic_text\"\n",
      "                ):  # one of the anthropics\n",
      "                    if \"prompt is too long\" in error_str or \"prompt: length\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    elif \"overloaded_error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise InternalServerError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if \"Invalid API Key\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if \"content filtering policy\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if \"Client error '400 Bad Request'\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=\"AnthropicError - {}\".format(error_str),\n",
      "                            model=model,\n",
      "                            llm_provider=\"anthropic\",\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        verbose_logger.debug(\n",
      "                            f\"status_code: {original_exception.status_code}\"\n",
      "                        )\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 413\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"anthropic\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"anthropic\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"anthropic\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AnthropicException - {error_str}\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 500\n",
      "                            or original_exception.status_code == 529\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.InternalServerError(\n",
      "                                message=f\"AnthropicException - {error_str}. Handle with `litellm.InternalServerError`.\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.ServiceUnavailableError(\n",
      "                                message=f\"AnthropicException - {error_str}. Handle with `litellm.ServiceUnavailableError`.\",\n",
      "                                llm_provider=\"anthropic\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"replicate\":\n",
      "                    if \"Incorrect authentication token\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            llm_provider=\"replicate\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"input is too long\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"replicate\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif exception_type == \"ModelError\":\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"replicate\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Request was throttled\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"ReplicateException - {error_str}\",\n",
      "                            llm_provider=\"replicate\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 413\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"replicate\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise UnprocessableEntityError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"replicate\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"replicate\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise UnprocessableEntityError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"ReplicateException - {original_exception.message}\",\n",
      "                                llm_provider=\"replicate\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    exception_mapping_worked = True\n",
      "                    raise APIError(\n",
      "                        status_code=500,\n",
      "                        message=f\"ReplicateException - {str(original_exception)}\",\n",
      "                        llm_provider=\"replicate\",\n",
      "                        model=model,\n",
      "                        request=httpx.Request(\n",
      "                            method=\"POST\",\n",
      "                            url=\"https://api.replicate.com/v1/deployments\",\n",
      "                        ),\n",
      "                    )\n",
      "                elif custom_llm_provider in litellm._openai_like_providers:\n",
      "                    if \"authorization denied for\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "    \n",
      "                        # Predibase returns the raw API Key in the response - this block ensures it's not returned in the exception\n",
      "                        if (\n",
      "                            error_str is not None\n",
      "                            and isinstance(error_str, str)\n",
      "                            and \"bearer\" in error_str.lower()\n",
      "                        ):\n",
      "                            # only keep the first 10 chars after the occurnence of \"bearer\"\n",
      "                            _bearer_token_start_index = error_str.lower().find(\"bearer\")\n",
      "                            error_str = error_str[: _bearer_token_start_index + 14]\n",
      "                            error_str += \"XXXXXXX\" + '\"'\n",
      "    \n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"{custom_llm_provider}Exception: Authentication Error - {error_str}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"model's maximum context limit\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"{custom_llm_provider}Exception: Context Window Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                        )\n",
      "                    elif \"token_quota_reached\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"{custom_llm_provider}Exception: Rate Limit Errror - {error_str}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The server received an invalid response from an upstream server.\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                        )\n",
      "                    elif \"model_no_support_for_function\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"{custom_llm_provider}Exception - Use 'watsonx_text' route instead. IBM WatsonX does not support `/text/chat` endpoint. - {error_str}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.InternalServerError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 401\n",
      "                            or original_exception.status_code == 403\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 422\n",
      "                            or original_exception.status_code == 424\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"{custom_llm_provider}Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"bedrock\":\n",
      "                    if (\n",
      "                        \"too many tokens\" in error_str\n",
      "                        or \"expected maxLength:\" in error_str\n",
      "                        or \"Input is too long\" in error_str\n",
      "                        or \"prompt is too long\" in error_str\n",
      "                        or \"prompt: length: 1..\" in error_str\n",
      "                        or \"Too many input tokens\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"BedrockException: Context Window Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Conversation blocks and tool result blocks cannot be provided in the same turn.\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"BedrockException - {error_str}\\n. Enable 'litellm.modify_params=True' (for PROXY do: `litellm_settings::modify_params: True`) to insert a dummy assistant message and fix this error.\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Malformed input request\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"BedrockException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"A conversation must start with a user message.\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"BedrockException - {error_str}\\n. Pass in default user message via `completion(..,user_continue_message=)` or enable `litellm.modify_params=True`.\\nFor Proxy: do via `litellm_settings::modify_params: True` or user_continue_message under `litellm_params`\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Unable to locate credentials\" in error_str\n",
      "                        or \"The security token included in the request is invalid\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"BedrockException Invalid Authentication - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"AccessDeniedException\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise PermissionDeniedError(\n",
      "                            message=f\"BedrockException PermissionDeniedError - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"throttlingException\" in error_str\n",
      "                        or \"ThrottlingException\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"BedrockException: Rate Limit Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Connect timeout on endpoint URL\" in error_str\n",
      "                        or \"timed out\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"BedrockException: Timeout Error - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                        )\n",
      "                    elif \"Could not process image\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"BedrockException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"bedrock\",\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=500,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\", url=\"https://api.openai.com/v1/\"\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                llm_provider=\"bedrock\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"BedrockException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"sagemaker\"\n",
      "                    or custom_llm_provider == \"sagemaker_chat\"\n",
      "                ):\n",
      "                    if \"Unable to locate credentials\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"litellm.BadRequestError: SagemakerException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"sagemaker\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"Input validation error: `best_of` must be > 0 and <= 2\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=\"SagemakerException - the value of 'n' must be > 0 and <= 2 for sagemaker endpoints\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"sagemaker\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"`inputs` tokens + `max_new_tokens` must be <=\" in error_str\n",
      "                        or \"instance type with more CPU capacity or memory\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"SagemakerException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"sagemaker\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=500,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\", url=\"https://api.openai.com/v1/\"\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 422\n",
      "                            or original_exception.status_code == 424\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                litellm_debug_info=extra_information,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"SagemakerException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"vertex_ai\"\n",
      "                    or custom_llm_provider == \"vertex_ai_beta\"\n",
      "                    or custom_llm_provider == \"gemini\"\n",
      "                ):\n",
      "                    if (\n",
      "                        \"Vertex AI API has not been used in project\" in error_str\n",
      "                        or \"Unable to find your project\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"litellm.BadRequestError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            response=httpx.Response(\n",
      "                                status_code=400,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    if \"400 Request payload size exceeds\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"VertexException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"None Unknown Error.\" in error_str\n",
      "                        or \"Content has no parts.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"litellm.InternalServerError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            response=httpx.Response(\n",
      "                                status_code=500,\n",
      "                                content=str(original_exception),\n",
      "                                request=httpx.Request(method=\"completion\", url=\"https://github.com/BerriAI/litellm\"),  # type: ignore\n",
      "                            ),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"API key not valid.\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"{custom_llm_provider}Exception - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif \"403\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"VertexAIException BadRequestError - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            response=httpx.Response(\n",
      "                                status_code=403,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The response was blocked.\" in error_str\n",
      "                        or \"Output blocked by content filtering policy\"\n",
      "                        in error_str  # anthropic on vertex ai\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=f\"VertexAIException ContentPolicyViolationError - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=httpx.Response(\n",
      "                                status_code=400,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"429 Quota exceeded\" in error_str\n",
      "                        or \"Quota exceeded for\" in error_str\n",
      "                        or \"IndexError: list index out of range\" in error_str\n",
      "                        or \"429 Unable to submit request because the service is temporarily out of capacity.\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"litellm.RateLimitError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=httpx.Response(\n",
      "                                status_code=429,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"500 Internal Server Error\" in error_str\n",
      "                        or \"The model is overloaded.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"litellm.InternalServerError: VertexAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"vertex_ai\",\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"VertexAIException BadRequestError - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"vertex_ai\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=400,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\",\n",
      "                                        url=\"https://cloud.google.com/vertex-ai/\",\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        if original_exception.status_code == 404:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise NotFoundError(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                        if original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "    \n",
      "                        if original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"litellm.RateLimitError: VertexAIException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"vertex_ai\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=429,\n",
      "                                    request=httpx.Request(\n",
      "                                        method=\"POST\",\n",
      "                                        url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                    ),\n",
      "                                ),\n",
      "                            )\n",
      "                        if original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise litellm.InternalServerError(\n",
      "                                message=f\"VertexAIException InternalServerError - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"vertex_ai\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=httpx.Response(\n",
      "                                    status_code=500,\n",
      "                                    content=str(original_exception),\n",
      "                                    request=httpx.Request(method=\"completion\", url=\"https://github.com/BerriAI/litellm\"),  # type: ignore\n",
      "                                ),\n",
      "                            )\n",
      "                        if original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"VertexAIException - {original_exception.message}\",\n",
      "                                llm_provider=custom_llm_provider,\n",
      "                                model=model,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"palm\" or custom_llm_provider == \"gemini\":\n",
      "                    if \"503 Getting metadata\" in error_str:\n",
      "                        # auth errors look like this\n",
      "                        # 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=\"GeminiException - Invalid api key\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"palm\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if (\n",
      "                        \"504 Deadline expired before operation could complete.\" in error_str\n",
      "                        or \"504 Deadline Exceeded\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"GeminiException - {original_exception.message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"palm\",\n",
      "                            exception_status_code=original_exception.status_code,\n",
      "                        )\n",
      "                    if \"400 Request payload size exceeds\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"GeminiException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"palm\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if (\n",
      "                        \"500 An internal error has occurred.\" in error_str\n",
      "                        or \"list index out of range\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise APIError(\n",
      "                            status_code=getattr(original_exception, \"status_code\", 500),\n",
      "                            message=f\"GeminiException - {original_exception.message}\",\n",
      "                            llm_provider=\"palm\",\n",
      "                            model=model,\n",
      "                            request=httpx.Response(\n",
      "                                status_code=429,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\",\n",
      "                                    url=\" https://cloud.google.com/vertex-ai/\",\n",
      "                                ),\n",
      "                            ),\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"GeminiException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"palm\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    # Dailed: Error occurred: 400 Request payload size exceeds the limit: 20000 bytes\n",
      "                elif custom_llm_provider == \"cloudflare\":\n",
      "                    if \"Authentication error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"Cloudflare Exception - {original_exception.message}\",\n",
      "                            llm_provider=\"cloudflare\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if \"must have required property\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"Cloudflare Exception - {original_exception.message}\",\n",
      "                            llm_provider=\"cloudflare\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                elif (\n",
      "                    custom_llm_provider == \"cohere\" or custom_llm_provider == \"cohere_chat\"\n",
      "                ):  # Cohere\n",
      "                    if (\n",
      "                        \"invalid api token\" in error_str\n",
      "                        or \"No API key provided.\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"too many tokens\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"cohere\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        if (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 498\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    elif (\n",
      "                        \"CohereConnectionError\" in exception_type\n",
      "                    ):  # cohere seems to fire these errors when we load test it (1k+ messages / min)\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"invalid type:\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Unexpected server error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ServiceUnavailableError(\n",
      "                            message=f\"CohereException - {original_exception.message}\",\n",
      "                            llm_provider=\"cohere\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    else:\n",
      "                        if hasattr(original_exception, \"status_code\"):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"CohereException - {original_exception.message}\",\n",
      "                                llm_provider=\"cohere\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                        raise original_exception\n",
      "                elif custom_llm_provider == \"huggingface\":\n",
      "                    if \"length limit exceeded\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=error_str,\n",
      "                            model=model,\n",
      "                            llm_provider=\"huggingface\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"A valid user token is required\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=error_str,\n",
      "                            llm_provider=\"huggingface\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Rate limit reached\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise RateLimitError(\n",
      "                            message=error_str,\n",
      "                            llm_provider=\"huggingface\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"huggingface\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"HuggingfaceException - {original_exception.message}\",\n",
      "                                llm_provider=\"huggingface\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"ai21\":\n",
      "                    if hasattr(original_exception, \"message\"):\n",
      "                        if \"Prompt has too many tokens\" in original_exception.message:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ContextWindowExceededError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        if \"Bad or missing API token.\" in original_exception.message:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                llm_provider=\"ai21\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                            )\n",
      "                        if original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"ai21\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                llm_provider=\"ai21\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"AI21Exception - {original_exception.message}\",\n",
      "                                llm_provider=\"ai21\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"nlp_cloud\":\n",
      "                    if \"detail\" in error_str:\n",
      "                        if \"Input text length should not exceed\" in error_str:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ContextWindowExceededError(\n",
      "                                message=f\"NLPCloudException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif \"value is not a valid\" in error_str:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"NLPCloudException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=500,\n",
      "                                message=f\"NLPCloudException - {error_str}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                    if hasattr(\n",
      "                        original_exception, \"status_code\"\n",
      "                    ):  # https://docs.nlpcloud.com/?shell#errors\n",
      "                        if (\n",
      "                            original_exception.status_code == 400\n",
      "                            or original_exception.status_code == 406\n",
      "                            or original_exception.status_code == 413\n",
      "                            or original_exception.status_code == 422\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 401\n",
      "                            or original_exception.status_code == 403\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 522\n",
      "                            or original_exception.status_code == 524\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 429\n",
      "                            or original_exception.status_code == 402\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 500\n",
      "                            or original_exception.status_code == 503\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                        elif (\n",
      "                            original_exception.status_code == 504\n",
      "                            or original_exception.status_code == 520\n",
      "                        ):\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"NLPCloudException - {original_exception.message}\",\n",
      "                                llm_provider=\"nlp_cloud\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"together_ai\":\n",
      "                    try:\n",
      "                        error_response = json.loads(error_str)\n",
      "                    except Exception:\n",
      "                        error_response = {\"error\": error_str}\n",
      "                    if (\n",
      "                        \"error\" in error_response\n",
      "                        and \"`inputs` tokens + `max_new_tokens` must be <=\"\n",
      "                        in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error\" in error_response\n",
      "                        and \"invalid private key\" in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error\" in error_response\n",
      "                        and \"INVALID_ARGUMENT\" in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"A timeout occurred\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"TogetherAIException - {error_str}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error\" in error_response\n",
      "                        and \"API key doesn't match expected format.\"\n",
      "                        in error_response[\"error\"]\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"error_type\" in error_response\n",
      "                        and error_response[\"error_type\"] == \"validation\"\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"together_ai\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"TogetherAIException - {error_response['error']}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"together_ai\",\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                                llm_provider=\"together_ai\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 524:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                                llm_provider=\"together_ai\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                    else:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise APIError(\n",
      "                            status_code=original_exception.status_code,\n",
      "                            message=f\"TogetherAIException - {original_exception.message}\",\n",
      "                            llm_provider=\"together_ai\",\n",
      "                            model=model,\n",
      "                            request=original_exception.request,\n",
      "                        )\n",
      "                elif custom_llm_provider == \"aleph_alpha\":\n",
      "                    if (\n",
      "                        \"This is longer than the model's maximum context length\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                            llm_provider=\"aleph_alpha\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"InvalidToken\" in error_str or \"No token provided\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                            llm_provider=\"aleph_alpha\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        verbose_logger.debug(\n",
      "                            f\"status code: {original_exception.status_code}\"\n",
      "                        )\n",
      "                        if original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                            )\n",
      "                        elif original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 500:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"AlephAlphaException - {original_exception.message}\",\n",
      "                                llm_provider=\"aleph_alpha\",\n",
      "                                model=model,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        raise original_exception\n",
      "                    raise original_exception\n",
      "                elif (\n",
      "                    custom_llm_provider == \"ollama\" or custom_llm_provider == \"ollama_chat\"\n",
      "                ):\n",
      "                    if isinstance(original_exception, dict):\n",
      "                        error_str = original_exception.get(\"error\", \"\")\n",
      "                    else:\n",
      "                        error_str = str(original_exception)\n",
      "                    if \"no such file or directory\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"OllamaException: Invalid Model/Model not loaded - {original_exception}\",\n",
      "                            model=model,\n",
      "                            llm_provider=\"ollama\",\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Failed to establish a new connection\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ServiceUnavailableError(\n",
      "                            message=f\"OllamaException: {original_exception}\",\n",
      "                            llm_provider=\"ollama\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Invalid response object from API\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"OllamaException: {original_exception}\",\n",
      "                            llm_provider=\"ollama\",\n",
      "                            model=model,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Read timed out\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise Timeout(\n",
      "                            message=f\"OllamaException: {original_exception}\",\n",
      "                            llm_provider=\"ollama\",\n",
      "                            model=model,\n",
      "                        )\n",
      "                elif custom_llm_provider == \"vllm\":\n",
      "                    if hasattr(original_exception, \"status_code\"):\n",
      "                        if original_exception.status_code == 0:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise APIConnectionError(\n",
      "                                message=f\"VLLMException - {original_exception.message}\",\n",
      "                                llm_provider=\"vllm\",\n",
      "                                model=model,\n",
      "                                request=original_exception.request,\n",
      "                            )\n",
      "                elif custom_llm_provider == \"azure\" or custom_llm_provider == \"azure_text\":\n",
      "                    message = get_error_message(error_obj=original_exception)\n",
      "                    if message is None:\n",
      "                        if hasattr(original_exception, \"message\"):\n",
      "                            message = original_exception.message\n",
      "                        else:\n",
      "                            message = str(original_exception)\n",
      "    \n",
      "                    if \"Internal server error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise litellm.InternalServerError(\n",
      "                            message=f\"AzureException Internal server error - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"This model's maximum context length is\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContextWindowExceededError(\n",
      "                            message=f\"AzureException ContextWindowExceededError - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"DeploymentNotFound\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise NotFoundError(\n",
      "                            message=f\"AzureException NotFoundError - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        (\n",
      "                            \"invalid_request_error\" in error_str\n",
      "                            and \"content_policy_violation\" in error_str\n",
      "                        )\n",
      "                        or (\n",
      "                            \"The response was filtered due to the prompt triggering Azure OpenAI's content management\"\n",
      "                            in error_str\n",
      "                        )\n",
      "                        or \"Your task failed as a result of our safety system\" in error_str\n",
      "                        or \"The model produced invalid content\" in error_str\n",
      "                        or \"content_filter_policy\" in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise ContentPolicyViolationError(\n",
      "                            message=f\"litellm.ContentPolicyViolationError: AzureException - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"invalid_request_error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise BadRequestError(\n",
      "                            message=f\"AzureException BadRequestError - {message}\",\n",
      "                            llm_provider=\"azure\",\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                            body=getattr(original_exception, \"body\", None),\n",
      "                        )\n",
      "                    elif (\n",
      "                        \"The api_key client option must be set either by passing api_key to the client or by setting\"\n",
      "                        in error_str\n",
      "                    ):\n",
      "                        exception_mapping_worked = True\n",
      "                        raise AuthenticationError(\n",
      "                            message=f\"{exception_provider} AuthenticationError - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                            response=getattr(original_exception, \"response\", None),\n",
      "                        )\n",
      "                    elif \"Connection error\" in error_str:\n",
      "                        exception_mapping_worked = True\n",
      "                        raise APIConnectionError(\n",
      "                            message=f\"{exception_provider} APIConnectionError - {message}\",\n",
      "                            llm_provider=custom_llm_provider,\n",
      "                            model=model,\n",
      "                            litellm_debug_info=extra_information,\n",
      "                        )\n",
      "                    elif hasattr(original_exception, \"status_code\"):\n",
      "                        exception_mapping_worked = True\n",
      "                        if original_exception.status_code == 400:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AzureException - {message}\",\n",
      "                                llm_provider=\"azure\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                                body=getattr(original_exception, \"body\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 401:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise AuthenticationError(\n",
      "                                message=f\"AzureException AuthenticationError - {message}\",\n",
      "                                llm_provider=\"azure\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 408:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AzureException Timeout - {message}\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                llm_provider=\"azure\",\n",
      "                            )\n",
      "                        elif original_exception.status_code == 422:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise BadRequestError(\n",
      "                                message=f\"AzureException BadRequestError - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 429:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise RateLimitError(\n",
      "                                message=f\"AzureException RateLimitError - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 503:\n",
      "                            exception_mapping_worked = True\n",
      "                            raise ServiceUnavailableError(\n",
      "                                message=f\"AzureException ServiceUnavailableError - {message}\",\n",
      "                                model=model,\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                response=getattr(original_exception, \"response\", None),\n",
      "                            )\n",
      "                        elif original_exception.status_code == 504:  # gateway timeout error\n",
      "                            exception_mapping_worked = True\n",
      "                            raise Timeout(\n",
      "                                message=f\"AzureException Timeout - {message}\",\n",
      "                                model=model,\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                llm_provider=\"azure\",\n",
      "                                exception_status_code=original_exception.status_code,\n",
      "                            )\n",
      "                        else:\n",
      "                            exception_mapping_worked = True\n",
      ">                           raise APIError(\n",
      "                                status_code=original_exception.status_code,\n",
      "                                message=f\"AzureException APIError - {message}\",\n",
      "                                llm_provider=\"azure\",\n",
      "                                litellm_debug_info=extra_information,\n",
      "                                model=model,\n",
      "                                request=httpx.Request(\n",
      "                                    method=\"POST\", url=\"https://openai.com/\"\n",
      "                                ),\n",
      "                            )\n",
      "\u001b[1m\u001b[31mE                           litellm.exceptions.APIError: litellm.APIError: AzureException APIError - Resource not found\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m:2077: APIError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/_pytest/config/__init__.py:1441\n",
      "  /Users/justintan/Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/_pytest/config/__init__.py:1441: PytestConfigWarning: Unknown config option: asyncio_default_fixture_loop_scope\n",
      "  \n",
      "    self._warn_or_fail_if_strict(f\"Unknown config option: {key}\\n\")\n",
      "\n",
      "../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/_pytest/config/__init__.py:1441\n",
      "  /Users/justintan/Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/_pytest/config/__init__.py:1441: PytestConfigWarning: Unknown config option: asyncio_mode\n",
      "  \n",
      "    self._warn_or_fail_if_strict(f\"Unknown config option: {key}\\n\")\n",
      "\n",
      "../../Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/pydantic/_internal/_config.py:323\n",
      "  /Users/justintan/Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/pydantic/_internal/_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "    warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n",
      "\n",
      "test_analyst.py:70\n",
      "  /Users/justintan/dialectical-framework/tests/test_analyst.py:70: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.asyncio\n",
      "\n",
      "tests/test_analyst.py::test_wheel_redefine\n",
      "  /Users/justintan/Library/Caches/pypoetry/virtualenvs/dialectical-framework-6UUiJe8X-py3.13/lib/python3.13/site-packages/_pytest/python.py:148: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.\n",
      "  You need to install a suitable plugin for your async framework, for example:\n",
      "    - anyio\n",
      "    - pytest-asyncio\n",
      "    - pytest-tornasync\n",
      "    - pytest-trio\n",
      "    - pytest-twisted\n",
      "    warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_analyst.py::\u001b[1mtest_thought_mapping\u001b[0m - litellm.exceptions.APIError: litellm.APIError: AzureException APIError - Re...\n",
      "\u001b[31mFAILED\u001b[0m test_analyst.py::\u001b[1mtest_wheel_2\u001b[0m - litellm.exceptions.APIError: litellm.APIError: AzureException APIError - Re...\n",
      "\u001b[31mFAILED\u001b[0m test_analyst.py::\u001b[1mtest_wheel_3\u001b[0m - litellm.exceptions.APIError: litellm.APIError: AzureException APIError - Re...\n",
      "\u001b[31mFAILED\u001b[0m test_analyst.py::\u001b[1mtest_wheel_4\u001b[0m - litellm.exceptions.APIError: litellm.APIError: AzureException APIError - Re...\n",
      "\u001b[31m============== \u001b[31m\u001b[1m4 failed\u001b[0m, \u001b[32m1 passed\u001b[0m, \u001b[33m1 skipped\u001b[0m, \u001b[33m5 warnings\u001b[0m\u001b[31m in 7.14s\u001b[0m\u001b[31m ==============\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!poetry run pytest test_analyst.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
